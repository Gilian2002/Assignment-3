# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zh3PUyHRUqCZBuNIlaXLGYlFAAgPjevT

#Loading the standard parts
"""

pip install torch torchvision torchaudio

# For reading data
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
# For visualizing
import plotly.express as px
# For model building
import torch
import torch.nn as nn
import torch.nn.functional as F

#Import numpy
#import matplotlib.pyplot as plt
import numpy as np

class CustomMNIST(Dataset):
    def __init__(self, url):
        # read in our raw data from the hosting URL
        self.raw_data = pd.read_csv(url)

    # return the length of the complete data set
    #   to be used in internal calculations for pytorch
    def __len__(self):
        return self.raw_data.shape[0]

    # retrieve a single record based on index position `idx`
    def __getitem__(self, idx):
        # extract the image separate from the label
        image = self.raw_data.iloc[idx, 1:].values.reshape(1, 28, 28)
        # Specify dtype to align with default dtype used by weight matrices
        image = torch.tensor(image, dtype=torch.float32)
        # extract the label
        label = self.raw_data.iloc[idx, 0]

        # return the image and its corresponding label
        return image, label

"""# **Data Loading**

For the dataloading I could not make use of the simply downloading it or linking as my macbook didn't allowed my to download the files. There was a too big change of malware detected. So hereby a little different route probably.
"""

import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.datasets import FashionMNIST

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(7), #10 -> 78%
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
# Load Fashion MNIST dataset
train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)
# Create data loaders

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define label names for better understanding
label_names = {
    0: "T-shirt/top",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle boot"
}
# Check that our data look right when we sample
idx=1
image, label = train_dataset.__getitem__(idx)

# Rescale pixel values to 0-255 and convert to NumPy array
image_np = image.squeeze().numpy()  # Remove channel dimension if present
image_np = (image_np * 0.5 + 0.5) * 255  # Rescale to 0-255 range
image_np = image_np.astype(np.uint8)  # Ensure data type is uint8

print(f"This image is labeled a {label_names[label]}")
px.imshow(image_np, color_continuous_scale="gray")

"""#Now building the network
More setting the parameters
"""

class FirstNet(nn.Module):
    def __init__(self):
      # We define the components of our model here
      super(FirstNet, self).__init__()
      # Function to flatten our image
      self.flatten = nn.Flatten()
      # Create the sequence of our network
      self.linear_relu_model = nn.Sequential(
            # Add a linear output layer w/ 10 perceptrons
            nn.LazyLinear(10),
        )

    def forward(self, x):
      # We construct the sequencing of our model here
      x = self.flatten(x)
      # Pass flattened images through our sequence
      output = self.linear_relu_model(x)

      # Return the evaluations of our ten
      #   classes as a 10-dimensional vector
      return output

# Create an instance of our model
model = FirstNet()
print(list(model.parameters()))

# Define some training parameters
learning_rate = 1e-2
batch_size = 64
epochs = 3

# Define our loss function
#   This one works for multiclass problems
loss_fn = nn.CrossEntropyLoss()

"""#Prep to train"""

#
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    # Set the model to training mode
    # important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.train()
    # Loop over batches via the dataloader
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation and looking for improved gradients
        loss.backward()
        optimizer.step()
        # Zeroing out the gradient (otherwise they are summed)
        #   in preparation for next round
        optimizer.zero_grad()

        # Print progress update every few loops
        if batch % 10 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test_loop(dataloader, model, loss_fn):
    # Set the model to evaluation mode
    # important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    # Evaluating the model with torch.no_grad() ensures
    # that no gradients are computed during test mode
    # also serves to reduce unnecessary gradient computations
    # and memory usage for tensors with requires_grad=True
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    # Printing some output after a testing round
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

# Need to repeat the training process for each epoch.
#   In each epoch, the model will eventually see EVERY
#   observations in the data
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loop(test_loader, model, loss_fn)
print("Done!")

# Decide if we are loading for predictions or more training
model.eval()
# - or -
#model.train()

# Make predictions
pred = model(test_dataset.__getitem__(1)[0]).argmax()
truth = test_dataset.__getitem__(1)[1]
print(f"This image is predicted to be a {pred}, and is labeled as {truth}")

# Loading for predictions
model.eval()

# Initialize counters for correct predictions and total predictions
correct = 0
total = 0

# Iterate over the test dataset
for data, target in test_dataset:
    # Make predictions
    output = model(data)
    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability

    # Update counters
    # Convert target to a PyTorch tensor and unsqueeze it before using eq
    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()
    total += 1  # Increment total by 1 for each data point

# Calculate accuracy
accuracy = 100. * correct / total

print(f"Accuracy on test data: {accuracy:.2f}%")

"""Initially I had a score of 7.92%, but for my machine learning research I found someone that said that angles and especially rotations matter. This will let it 'learn' better to make better predictions for new data it has never seen before. So that was what I did, with my transformers.

#Saving the model
"""

# Specify ou
filepath= "model.pt"
state = {
    'epoch': epochs,
    'state_dict': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'validation_accuracy': {accuracy},
}
torch.save(state, filepath)

# Load the saved state
state = torch.load(filepath, weights_only=False)

# Load model, optimizer, and epoch information
model.load_state_dict(state['state_dict'])
optimizer.load_state_dict(state['optimizer'])
start_epoch = state['epoch']  # Load the epoch count

last_val_accuracy = state.get('validation_accuracy', None)

print(f"Resuming from epoch {start_epoch}.")
print(f"Last saved epoch validation accuracy: {last_val_accuracy}")

# Set the model to training mode if resuming training
model.train()

# Resume training from the last saved epoch
for epoch in range(start_epoch, epochs):
    # Your training code here
    print(f"Epoch {epoch+1}/{total_epochs}")
    # Train the model for this epoch
    # ...
    # At the end of the loop, save model again if desired
print(f"Model and optimizer states loaded. Resuming from epoch {start_epoch}.")

# Set model to evaluation mode
model.eval()

# Initialize counters for correct predictions and total predictions
correct = 0
total = 0

# Disable gradient calculations for prediction
with torch.no_grad():
    # Iterate over the test dataloader
    for data, target in test_loader:  # Replace `test_dataloader` with your DataLoader for the test data

        # Make predictions
        output = model(data)
        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability

        # Update counters for accuracy
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)  # Increment total by the batch size

# Calculate and print accuracy
accuracy = 100. * correct / total
print(f"Accuracy on test data: {accuracy:.2f}%")

# Define your model and optimizer before training
model = FirstNet()  # Make sure this matches your model class
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Replace with your learning rate and optimizer

# Train your model here...
# Assuming you have a loop for training

# After training completes, save model and optimizer states
EPOCH = epochs  # Set this to the number of epochs completed
PATH = "/content/data/FashionMNIST/Train/state_dict_model.pt"

# Save the state dict for the model and optimizer, along with the epoch number
torch.save({
    'epoch': EPOCH,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, PATH)

print(f"Model and optimizer states saved successfully at {PATH}")

# Save our model for later, so we can train more or make predictions
EPOCH = epochs
# We use the .pt file extension by convention for saving
# pytorch models
PATH = "/content/data/FashionMNIST/Train/state_dict_model.pt"

model = FirstNet()
# The save function creates a binary storing all our data for
torch.save({
    'epoch': EPOCH,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, PATH)

"""# **Loading the model**"""

PATH = "/content/data/FashionMNIST/Train/state_dict_model.pt"
#Load the model back in
model = FirstNet()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
EPOCH = checkpoint['epoch']

# Set the model to evaluation mode
model.eval()

# If you need to set the model to training mode
# model.train()
test_dataset = torch.utils.data.TensorDataset(torch.randn(100, 1, 28, 28), torch.randint(0, 10, (100,)))

# Evaluate the model on the test dataset
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the test images: {100 * correct / total}%')

# Specify our path
PATH = "/content/data/FashionMNIST/Train/state_dict_model.pt"

# Create a new "blank" model to load our information into
model = FirstNet()

# Recreate our optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Load back all of our data from the file
checkpoint = torch.load(PATH, weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
EPOCH = checkpoint['epoch']

import torch

# Load the file
pt_file = torch.load("/content/data/FashionMNIST/Train/state_dict_model.pt")

# Print the keys of the loaded file to see what's available
print(pt_file.keys())

# Access the desired information using the correct keys
# For example, to access the model's state dictionary:
print(pt_file['model_state_dict'])
# or to get the epoch number:
print(pt_file['epoch'])

# Decide if we are loading for predictions or more training
model.eval()
# - or -
#model.train()

# Make predictions
pred = model(test_dataset.__getitem__(1)[0]).argmax()
truth = test_dataset.__getitem__(1)[1]
print(f"This image is predicted to be a {pred}, and is labeled as {truth}")

"""Accurancy"""

import torch

# Decide if we are loading for predictions or more training
model.eval()

# Initialize counters for correct predictions and total predictions
correct = 0
total = 0

# Iterate over the test dataset
for data, target in test_dataset:
    # Make predictions
    output = model(data)
    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability

    # Update counters
    # Convert target to a PyTorch tensor and unsqueeze it before using eq
    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()
    total += 1  # Increment total by 1 for each data point

# Calculate accuracy
accuracy = 100. * correct / total

print(f"Accuracy on test data: {accuracy:.2f}%")