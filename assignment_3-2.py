# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zh3PUyHRUqCZBuNIlaXLGYlFAAgPjevT

# **Homework assignment 3**


Course:  Business Forecasting

Prof:    Dr. White

<br>

Student: Gilian Koenders

Number:  59046858

Date:    11-13-2024



---

## Seting up the installation
In the code chunks below, the installation is prepared with installing Torchvision and the necessary libraries.
"""

pip install torch torchvision torchaudio

# For reading data
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
# For visualizing
import plotly.express as px
# For model building
import torch
import torch.nn as nn
import torch.nn.functional as F

#Import numpy
import numpy as np

"""## Data Loading

For the dataloading I could innitialy not make use of the file becuase I got a whole load of warnings, it said that there was a  chance of malware detected. Worked around it with the school computer, but don't have a fancy way of loading it, I had to place it into Google Colab temporary drive.


To make the model more robust, I added some image transformations, such as the random flips and slight rotations, and normalized the data. For the importing I used the IDX fiel format.

After the data is loaded I checked it based on a example. Herefore I had to rewrite as it is no longer searching for a 0, but 0=T-shirt. That gave me the confirmation that the data is in the correct shape.
"""

#!pip install python-mnist
import struct
import gzip
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from mnist import MNIST
import torchvision.transforms as transforms
from PIL import Image

class FashionMNIST(Dataset):
    def __init__(self, images_path, labels_path, transform=None):
        self.images_path = images_path
        self.labels_path = labels_path
        self.transform = transform
        self.images, self.labels = self.load_idx_files()

    def load_idx_files(self):
        with open(self.labels_path, 'rb') as lbpath:
            magic, num = struct.unpack(">II", lbpath.read(8))
            labels = torch.tensor(list(lbpath.read()), dtype=torch.long)

        with open(self.images_path, 'rb') as imgpath:
            magic, num, rows, cols = struct.unpack(">IIII", imgpath.read(16))
            images = torch.tensor(list(imgpath.read()), dtype=torch.uint8).view(num, rows, cols)

        return images, labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        image = Image.fromarray(image.numpy(), mode='L')

        if self.transform:
            image = self.transform(image)

        return image, label

# Transforms parameters
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(7),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load the FashionMNIST dataset
train_data = FashionMNIST('/content/train-images-idx3-ubyte.idx', '/content/train-labels-idx1-ubyte.idx', transform=transform)
test_data = FashionMNIST('/content/t10k-images-idx3-ubyte.idx', '/content/t10k-labels-idx1-ubyte.idx', transform=transform)

# Create data loaders
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

# Define label names for better understanding
label_names = {
    0: "T-shirt/top",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle boot"
}
# Check that our data look right when we sample
idx=1
image, label = train_dataset.__getitem__(idx)

# Rescale pixel values to 0-255 and convert to NumPy array
image_np = image.squeeze().numpy()  # Remove channel dimension if present
image_np = (image_np * 0.5 + 0.5) * 255  # Rescale to 0-255 range
image_np = image_np.astype(np.uint8)  # Ensure data type is uint8

print(f"This image is labeled a {label_names[label]}")
px.imshow(image_np, color_continuous_scale="gray")

"""##Now building the network
For the network , the parameters are set below. I added some extra layers, after some online search. This will give the neural network a better understanding, and can then better predict on unseen data.

To see if there are parametes print, I added a little line of code for it. This is used as check up if the model works fine. I left it inserted. After this code are the training parameters notated.
"""

class FirstNet(nn.Module):
    def __init__(self):

      super(FirstNet, self).__init__()

      self.flatten = nn.Flatten()

      self.linear_relu_model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        output = self.linear_relu_model(x)
        return output

# To see what the model contains I print the parameters
model = FirstNet()
print(list(model.parameters()))

# Define some training parameters
learning_rate = 1e-2
batch_size = 64
epochs = 20

# Define our loss function
loss_fn = nn.CrossEntropyLoss()

"""## Training of Data

For the training part, only minor changes have been made because I had an error once. For the rest I left it as default. Only for the training part I eventually made some changes, to let it continue after 20 epochs
"""

#Implementing the optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    # Loop over batches via the dataloader
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Print progress update every few loops
        if batch % 10 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test_loop(dataloader, model, loss_fn):
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    # Printing some output after a testing round
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

# Need to repeat the training process for each epoch.
#   In each epoch, the model will eventually see EVERY
#   observations in the data
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loop(test_loader, model, loss_fn)
print("Done!")

# Decide if we are loading for predictions or more training
model.eval()
# - or -
#model.train()

# Make predictions
pred = model(test_dataset.__getitem__(1)[0]).argmax()
truth = test_dataset.__getitem__(1)[1]
print(f"This image is predicted to be a {pred}, and is labeled as {truth}")

"""For the evaluation only giving me the predicted image was not enough, I had now clue how good my training data was on the testdata. So I trained it once, which gave me a poor outcome. Than I added the transformers and this improved my prediction on the test data drastically."""

# Loading for predictions
model.eval()

# Initialize counters for correct predictions and total predictions
correct = 0
total = 0

# Iterate over the test dataset
for data, target in test_dataset:

    output = model(data)
    pred = output.argmax(dim=1, keepdim=True)

    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()
    total += 1

# Calculate accuracy
accuracy = 100. * correct / total

print(f"Accuracy on test data: {accuracy:.2f}%")

"""Initially I had a score of 7.92%, but for my machine learning research I found someone that said that angles and especially rotations matter. This will let it 'learn' better to make better predictions for new data it has never seen before. So that was what I did, with my transformers.

##Saving the model

For saving the model I made some chances after I did not manage to save and export my file. Ultimately I think that it was not really necessary but I am able to see more about my data than only some basic information. With the export of the additional data I was better able to tell of my import was correct.
"""

# Save our model for later, so we can train more or make predictions
EPOCH = epochs

PATH = "model_BF.pt"

state = {
    'epoch': EPOCH,
    'state_dict': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'avarage_loss': {loss_fn},
    'validation_accuracy': {accuracy}, #added to see at import if it imports everything
}
torch.save(state, PATH)
from google.colab import files
files.download('model_BF.pt')

"""## Loading the model

For loading the model I made use of the github repository. In my Github the model.pt is placed and in here imported. When using the code of the lecture I had now clue of what was saved. So I changed to see which epoch it uses and what it latest accuracy was. With that I was able to understand what was going on in the file and which information it contained.
"""

filepath= "model_BF.pt"
# Load the saved state
state = torch.load(filepath, weights_only=False)

# Load model, optimizer, and epoch information
model.load_state_dict(state['state_dict'])
optimizer.load_state_dict(state['optimizer'])
start_epoch = state['epoch']
last_val_accuracy = state.get('validation_accuracy', None)
last_avg_loss = state.get('avarage_loss', None)

print(f"Resuming from epoch {start_epoch}.")
print(f"Last saved epoch validation accuracy: {last_val_accuracy}")

"""## Testing the imported model
With the code below, which is the same as a little higher. A test is performed on how the model accuracy is on the testdata.

I have done this, because I had troubles on my testdata and kept losing my model somehow. So instead of only seeing what my image does I wanted to now what the % was on the test data. That was the reason why I left the code in so that you could see how my model performs on the test data.
"""

model.eval()

# These are the counters for correct predictions and total predictions
correct = 0
total = 0

# Iterate over the test dataset
for data, target in test_dataset:
    output = model(data)
    pred = output.argmax(dim=1, keepdim=True)
    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()
    total += 1

# Calculate accuracy
accuracy = 100. * correct / total

print(f"Accuracy on test data: {accuracy:.2f}%")