{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework assignment 3**\n",
        "\n",
        "\n",
        "Course:  Business Forecasting\n",
        "\n",
        "Prof:    Dr. White\n",
        "\n",
        "<br>\n",
        "\n",
        "Student: Gilian Koenders\n",
        "\n",
        "Number:  59046858\n",
        "\n",
        "Date:    11-13-2024\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jt2ljHV1JVJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seting up the installation\n",
        "In the code chunks below, the installation is prepared with installing Torchvision and the necessary libraries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E9SdGdIJe9e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dlSf_0PmdnN7",
        "outputId": "a5185c27-5358-4090-a102-a49a15bb1c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For reading data\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# For visualizing\n",
        "import plotly.express as px\n",
        "# For model building\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Import numpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ilWWSMbEd06q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "LfczNPzH_O1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the dataloading I could innitialy not make use of the file becuase I got a whole load of warnings, it said that there was a  chance of malware detected. Worked around it with the school computer, but don't have a fancy way of loading it, I had to place it into Google Colab temporary drive.\n",
        "\n",
        "\n",
        "To make the model more robust, I added some image transformations, such as the random flips and slight rotations, and normalized the data. For the importing I used the IDX fiel format.\n",
        "\n",
        "After the data is loaded I checked it based on a example. Herefore I had to rewrite as it is no longer searching for a 0, but 0=T-shirt. That gave me the confirmation that the data is in the correct shape."
      ],
      "metadata": {
        "id": "P7G6LpDm_Q11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-mnist\n",
        "import struct\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from mnist import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, images_path, labels_path, transform=None):\n",
        "        self.images_path = images_path\n",
        "        self.labels_path = labels_path\n",
        "        self.transform = transform\n",
        "        self.images, self.labels = self.load_idx_files()\n",
        "\n",
        "    def load_idx_files(self):\n",
        "        with open(self.labels_path, 'rb') as lbpath:\n",
        "            magic, num = struct.unpack(\">II\", lbpath.read(8))\n",
        "            labels = torch.tensor(list(lbpath.read()), dtype=torch.long)\n",
        "\n",
        "        with open(self.images_path, 'rb') as imgpath:\n",
        "            magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
        "            images = torch.tensor(list(imgpath.read()), dtype=torch.uint8).view(num, rows, cols)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.fromarray(image.numpy(), mode='L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transforms parameters\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Og4lleKBbhWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296ae5f3-90a0-4939-cb6f-11915f928564"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-mnist in /usr/local/lib/python3.10/dist-packages (0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data manually in the Google Colab drive, the file is too big for me to store on the drive or on github."
      ],
      "metadata": {
        "id": "qWxRih77KQ6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FashionMNIST dataset\n",
        "train_data = FashionMNIST('/content/train-images-idx3-ubyte.idx', '/content/train-labels-idx1-ubyte.idx', transform=transform)\n",
        "test_data = FashionMNIST('/content/t10k-images-idx3-ubyte.idx', '/content/t10k-labels-idx1-ubyte.idx', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "C4Y_CXPsFErw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label names for better understanding\n",
        "label_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "# Check that our data look right when we sample\n",
        "idx=1\n",
        "image, label = train_data.__getitem__(idx)\n",
        "\n",
        "# Rescale pixel values to 0-255 and convert to NumPy array\n",
        "image_np = image.squeeze().numpy()  # Remove channel dimension if present\n",
        "image_np = (image_np * 0.5 + 0.5) * 255  # Rescale to 0-255 range\n",
        "image_np = image_np.astype(np.uint8)  # Ensure data type is uint8\n",
        "\n",
        "print(f\"This image is labeled a {label_names[label.item()]}\")\n",
        "px.imshow(image_np, color_continuous_scale=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "iC6CersHpR0d",
        "outputId": "ecb9380f-ab29-4e79-cfdd-cacae26e8289"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is labeled a T-shirt/top\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"198d88be-fb34-4da2-a971-93525c20d475\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"198d88be-fb34-4da2-a971-93525c20d475\")) {                    Plotly.newPlot(                        \"198d88be-fb34-4da2-a971-93525c20d475\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0,0,0,0,0,0,0,0,0,168,87,42,47,53,103,188,40,0,136,48,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,15,133,217,255,255,255,255,236,228,216,219,224,222,176,13,0,0,0,0,0,0],[0,0,0,0,44,160,231,254,215,201,202,204,215,200,196,198,203,212,198,200,219,188,0,0,0,0,0,0],[0,0,0,164,224,218,209,201,201,200,198,198,196,201,199,199,198,202,203,203,199,219,50,0,0,0,0,0],[0,0,40,225,200,201,200,200,200,252,247,246,249,245,250,248,238,212,207,204,206,226,116,0,0,0,0,0],[0,0,140,222,203,203,207,235,248,50,44,49,47,73,46,75,101,204,215,203,209,222,200,0,0,0,0,0],[0,0,224,220,206,202,222,113,62,71,68,58,103,0,98,70,0,200,215,210,212,218,247,0,0,0,0,0],[0,37,250,210,214,206,219,0,48,255,251,174,255,139,243,254,0,214,224,215,214,214,226,44,0,0,0,0],[0,95,220,208,214,217,215,0,205,12,11,16,15,59,34,41,0,205,216,220,211,214,235,164,0,0,0,0],[0,156,212,206,212,216,189,0,70,76,70,70,78,83,94,89,71,51,198,211,248,237,223,187,106,0,0,0],[0,0,208,219,213,222,207,206,87,252,250,253,252,248,245,248,250,252,193,222,159,53,16,0,0,0,0,0],[0,0,113,193,215,225,212,201,239,203,201,200,200,200,200,197,192,208,193,225,46,0,0,0,0,0,0,0],[0,0,0,0,0,0,165,210,195,212,204,206,205,204,204,206,203,214,192,212,45,0,5,0,0,0,0,0],[0,0,0,0,0,0,107,218,197,212,204,206,205,204,205,206,202,212,195,197,10,0,0,0,0,0,0,0],[0,0,0,0,2,0,91,218,200,205,204,207,205,204,206,205,202,205,199,191,1,0,2,0,0,0,0,0],[0,0,0,0,4,0,77,218,205,199,206,209,206,205,205,206,205,201,198,188,0,0,1,0,0,0,0,0],[0,0,0,0,4,0,74,219,209,198,208,210,207,207,204,207,207,200,197,215,0,0,1,0,0,0,0,0],[0,0,0,0,3,0,72,221,207,198,208,210,207,207,205,208,206,203,198,212,0,0,0,0,0,0,0,0],[0,0,0,0,3,0,75,222,202,200,206,211,207,208,206,209,206,206,209,201,204,0,0,0,0,0,0,0],[0,0,0,0,0,2,0,80,221,198,209,210,210,205,211,205,207,208,205,201,202,0,0,0,0,0,0,0],[0,0,0,0,0,2,0,96,221,195,210,209,210,206,213,205,207,209,205,204,204,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,105,217,194,211,208,210,207,215,205,207,208,205,207,204,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,115,213,193,212,207,210,210,215,206,206,208,207,208,198,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,118,210,195,211,207,210,212,212,207,204,208,208,210,198,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,121,207,196,210,207,211,212,213,209,206,208,207,210,172,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,124,207,197,205,201,204,205,207,204,199,201,203,221,188,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,127,206,197,243,240,244,244,244,238,236,234,214,146,139,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,162,224,214,119,121,125,124,125,137,135,135,130,0,0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(0, 0, 0)\"],[0.09090909090909091,\"rgb(16, 16, 16)\"],[0.18181818181818182,\"rgb(38, 38, 38)\"],[0.2727272727272727,\"rgb(59, 59, 59)\"],[0.36363636363636365,\"rgb(81, 80, 80)\"],[0.45454545454545453,\"rgb(102, 101, 101)\"],[0.5454545454545454,\"rgb(124, 123, 122)\"],[0.6363636363636364,\"rgb(146, 146, 145)\"],[0.7272727272727273,\"rgb(171, 171, 170)\"],[0.8181818181818182,\"rgb(197, 197, 195)\"],[0.9090909090909091,\"rgb(224, 224, 223)\"],[1.0,\"rgb(254, 254, 253)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('198d88be-fb34-4da2-a971-93525c20d475');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now building the network\n",
        "For the network , the parameters are set below. I added some extra layers, after some online search. This will give the neural network a better understanding, and can then better predict on unseen data.\n",
        "\n",
        "To see if there are parametes print, I added a little line of code for it. This is used as check up if the model works fine. I left it inserted. After this code are the training parameters notated."
      ],
      "metadata": {
        "id": "ghU7f8TRf8eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstNet(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "      super(FirstNet, self).__init__()\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      self.linear_relu_model = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        output = self.linear_relu_model(x)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "1wMfQLuKgTTz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see what the model contains I print the parameters\n",
        "model = FirstNet()\n",
        "print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoSVa-mmgUg4",
        "outputId": "a4bfa9a2-f98b-4c30-df65-055c3befa9c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.0105,  0.0086, -0.0125,  ...,  0.0197,  0.0272,  0.0292],\n",
            "        [-0.0061, -0.0258, -0.0278,  ...,  0.0311, -0.0098,  0.0290],\n",
            "        [-0.0085, -0.0122,  0.0207,  ..., -0.0141,  0.0026, -0.0210],\n",
            "        ...,\n",
            "        [-0.0161,  0.0235, -0.0179,  ...,  0.0138,  0.0290, -0.0038],\n",
            "        [-0.0171, -0.0173,  0.0140,  ...,  0.0104,  0.0260,  0.0125],\n",
            "        [-0.0151, -0.0243,  0.0156,  ..., -0.0341, -0.0016, -0.0309]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 3.0019e-02, -1.8864e-02, -2.5100e-02,  1.9676e-03,  3.4244e-02,\n",
            "        -3.0937e-02, -6.5419e-03,  2.2159e-02, -2.4858e-02, -2.1569e-02,\n",
            "         2.1972e-02, -2.6622e-02, -4.9197e-03,  2.4765e-02, -1.3105e-02,\n",
            "        -2.3399e-02, -1.3346e-03, -3.0538e-02,  2.6711e-02, -4.9150e-03,\n",
            "        -1.4489e-02, -1.6925e-02,  2.6323e-02, -2.4172e-02, -9.3698e-03,\n",
            "        -2.8979e-02, -2.6857e-04, -2.1753e-02, -1.1536e-02,  1.1657e-02,\n",
            "         2.3239e-02, -6.3269e-03, -2.5179e-03,  1.3316e-02, -2.1707e-02,\n",
            "        -2.6054e-02, -8.9825e-03, -1.2983e-03, -8.9948e-03,  3.5006e-02,\n",
            "         3.1453e-02,  4.6998e-03,  4.8964e-03,  2.1804e-02, -3.3603e-02,\n",
            "         6.5123e-03,  2.4547e-02, -1.6864e-02, -7.9335e-03,  8.7715e-03,\n",
            "        -3.3714e-03,  1.1679e-02,  2.4691e-02, -2.7438e-02,  2.4276e-02,\n",
            "        -1.8499e-02, -2.4234e-03, -1.1209e-02, -2.7180e-02, -1.8526e-02,\n",
            "        -2.5131e-02, -3.0469e-02, -1.0836e-02,  1.4598e-02, -2.1914e-02,\n",
            "        -3.4560e-02, -3.2708e-02,  2.0110e-02, -2.6128e-02,  2.7130e-02,\n",
            "         2.4647e-04, -6.3030e-03, -2.1487e-02,  1.7101e-02, -2.0780e-02,\n",
            "        -1.6508e-03, -3.2486e-02,  9.7143e-05,  2.4651e-02,  1.2382e-02,\n",
            "         2.6988e-02, -3.5061e-02, -1.4457e-02, -2.7204e-02, -2.1437e-02,\n",
            "         1.6664e-02,  2.2274e-02, -2.7231e-02,  1.9099e-02, -3.5656e-02,\n",
            "        -3.4746e-02, -1.4771e-02,  1.3303e-02, -3.2496e-02, -3.4261e-02,\n",
            "         3.1107e-02,  5.4678e-04, -1.8600e-02, -1.8646e-02, -1.1114e-02,\n",
            "        -4.4362e-03,  2.9153e-02, -8.3797e-03, -1.3310e-02,  1.5179e-02,\n",
            "         1.3505e-02, -1.8254e-02,  2.6742e-02,  2.3432e-02, -2.7457e-02,\n",
            "         3.4702e-02,  1.2737e-02,  1.8189e-02,  2.8671e-02, -6.0377e-04,\n",
            "        -1.0523e-02,  1.0790e-02, -2.2686e-02, -9.5713e-03, -2.4071e-02,\n",
            "         1.2229e-02,  1.1698e-03, -2.6506e-02, -1.7838e-02,  1.9170e-02,\n",
            "         1.8955e-02,  2.9462e-02, -2.9288e-02,  3.0104e-02, -1.3235e-02,\n",
            "         3.0929e-02,  7.3485e-03, -2.8112e-02,  1.1625e-02,  3.3895e-02,\n",
            "         2.9586e-02,  2.1972e-02, -1.0380e-03, -2.0399e-02,  1.2765e-02,\n",
            "         6.7295e-03, -1.2193e-02, -3.4234e-03, -6.3034e-04, -1.3656e-02,\n",
            "        -2.5111e-02,  9.4672e-03, -2.2903e-02, -1.3724e-02,  2.1796e-03,\n",
            "         2.6417e-02, -3.5301e-03, -3.3648e-02, -3.5091e-02,  2.0735e-02,\n",
            "        -2.6048e-02, -9.1312e-03, -1.1714e-02,  1.4559e-02,  2.5012e-02,\n",
            "         8.4277e-03, -3.2842e-02,  1.8710e-02, -2.3012e-02, -1.3256e-02,\n",
            "         1.9294e-02,  3.1963e-03,  8.7397e-03,  2.8461e-03, -2.5827e-02,\n",
            "         5.0217e-03, -1.9032e-02,  3.2023e-02,  2.0373e-02,  1.6752e-02,\n",
            "         3.4331e-02,  2.2712e-02, -5.3717e-03,  3.3054e-02,  2.4567e-02,\n",
            "         1.2416e-02, -1.8337e-02,  5.1121e-03,  2.9613e-02,  1.3024e-02,\n",
            "         2.6226e-02,  3.4393e-03,  3.4877e-03,  2.6727e-02,  1.9710e-02,\n",
            "        -1.6357e-03, -2.5674e-02,  2.4333e-02, -2.5402e-02,  1.3581e-02,\n",
            "        -2.9187e-02, -2.5636e-02, -3.1444e-02,  3.0144e-02, -9.3861e-03,\n",
            "         2.4677e-02,  2.0790e-02, -2.7597e-02,  6.6097e-03, -2.4719e-02,\n",
            "         9.9945e-03,  3.4987e-02, -2.0728e-02,  3.0410e-02,  1.1456e-02,\n",
            "         1.1703e-02, -8.6867e-03, -3.1841e-02, -1.1255e-02,  3.5690e-02,\n",
            "         2.9851e-02, -2.3993e-02,  2.7622e-02,  9.4172e-03,  3.0993e-02,\n",
            "        -3.4518e-02, -1.3923e-02, -1.4683e-02, -2.1620e-02, -2.0764e-02,\n",
            "         1.2488e-02,  4.8170e-03, -4.3279e-03,  1.1307e-02, -1.6783e-02,\n",
            "        -5.7096e-03, -2.5550e-02, -1.2183e-02,  1.7911e-02,  2.1904e-03,\n",
            "         1.4904e-02, -1.3805e-02,  2.9066e-02, -1.6464e-03, -1.8698e-02,\n",
            "        -3.5313e-02, -3.2966e-02, -1.4630e-02, -3.5531e-02, -2.7156e-02,\n",
            "        -2.0218e-02, -1.0709e-03,  2.2383e-02, -2.6648e-02, -2.5962e-02,\n",
            "         1.8470e-02, -2.9482e-02, -9.9517e-03,  1.3644e-02,  1.9633e-02,\n",
            "        -1.5022e-02,  1.7113e-02, -1.1310e-02, -1.1834e-02, -3.2272e-02,\n",
            "        -1.3595e-02, -9.1380e-04, -1.0180e-02, -6.3301e-03,  3.5525e-02,\n",
            "         3.4458e-02, -2.0461e-04, -3.3830e-02, -1.0709e-02,  4.0976e-03,\n",
            "         2.3393e-02,  1.3360e-03, -8.3819e-03,  5.9747e-03,  1.3435e-02,\n",
            "         3.2962e-02, -3.0833e-02,  3.1666e-02, -3.9952e-03, -3.0111e-02,\n",
            "         1.2040e-02,  1.0258e-02, -1.8650e-02, -2.8782e-02, -1.1283e-03,\n",
            "         2.4910e-02, -9.7101e-03,  2.7648e-03, -2.2198e-02, -2.0205e-02,\n",
            "        -3.0913e-02, -2.2632e-02, -2.2508e-02, -1.9029e-02, -2.6255e-02,\n",
            "        -3.0627e-02, -4.2101e-03, -3.3292e-02,  8.9838e-03, -2.6170e-02,\n",
            "         1.1146e-02, -3.2160e-02,  1.6851e-02,  9.0703e-03, -3.6065e-03,\n",
            "         3.4082e-02,  3.0439e-02,  1.4451e-02, -2.2167e-03, -2.0030e-02,\n",
            "         3.2856e-02,  8.0080e-03, -7.8940e-03,  1.1084e-03,  7.8927e-03,\n",
            "         1.0581e-02, -3.3269e-03,  3.2738e-02, -1.1166e-02, -3.0171e-02,\n",
            "        -4.5185e-03,  9.2683e-03, -4.1614e-03, -2.6609e-02,  1.5229e-02,\n",
            "         2.0647e-03,  1.3233e-03, -2.1035e-02,  3.2329e-02,  9.5401e-03,\n",
            "         1.9945e-02, -5.6615e-04,  2.6117e-03, -2.6681e-02, -3.4022e-03,\n",
            "         5.2784e-03, -2.5350e-02,  1.2080e-02,  3.0727e-02, -1.0767e-03,\n",
            "        -1.9343e-02, -2.9298e-02,  1.4594e-02, -4.3972e-03, -3.4030e-02,\n",
            "         2.3405e-02, -1.6773e-02,  2.6166e-02, -2.0647e-02, -1.6338e-03,\n",
            "         1.1676e-02,  3.6082e-03,  3.2057e-02, -1.5134e-02, -4.5456e-03,\n",
            "         2.6666e-02,  2.4617e-03,  1.4372e-02, -8.8244e-03,  2.9160e-02,\n",
            "        -2.5155e-02,  2.0206e-02,  2.9475e-02, -2.8614e-02,  2.2367e-02,\n",
            "        -2.0450e-02, -1.1840e-02,  1.7322e-02,  9.7953e-04,  3.5128e-02,\n",
            "        -1.9488e-02,  6.1254e-03, -1.9816e-02,  6.4360e-03,  6.2515e-03,\n",
            "         3.0299e-02,  1.6042e-02,  4.4945e-03, -1.1174e-02, -1.5607e-02,\n",
            "        -1.6095e-02, -1.6020e-02,  2.0793e-02, -1.3293e-02,  2.7620e-02,\n",
            "        -3.5580e-02, -2.2006e-02, -1.5219e-02,  1.2437e-02, -7.8951e-04,\n",
            "         1.4527e-02, -2.0468e-02,  3.2829e-02,  2.4048e-02,  2.2787e-02,\n",
            "         3.5424e-02, -1.0923e-02, -2.8560e-02, -1.2579e-02, -2.0813e-02,\n",
            "        -3.2500e-02, -3.3015e-02, -2.4917e-02, -1.8731e-02,  2.7529e-02,\n",
            "         4.2670e-04,  2.0331e-02,  1.0720e-02, -1.8353e-02, -1.2577e-02,\n",
            "         1.5095e-02, -1.1232e-02,  5.5394e-03, -3.5385e-02,  4.7774e-03,\n",
            "        -1.6324e-02, -1.6810e-02, -1.2189e-02, -2.2010e-03, -1.5506e-02,\n",
            "        -2.2550e-02, -2.4434e-02,  2.8893e-02, -2.1557e-02,  3.4089e-02,\n",
            "         2.0423e-02, -2.6561e-02,  1.7968e-02, -2.4732e-03, -2.4572e-02,\n",
            "         2.9823e-02,  9.7723e-03, -3.4766e-02,  9.2253e-03,  6.5063e-03,\n",
            "        -1.2482e-02, -1.1594e-02,  3.0727e-02,  1.7446e-02, -2.3489e-02,\n",
            "        -2.6119e-02,  3.0799e-02,  3.1471e-02, -1.8242e-02, -1.8851e-02,\n",
            "        -1.5353e-02,  2.6089e-02,  3.0723e-02,  1.3530e-02, -1.3988e-02,\n",
            "         2.0060e-02, -1.0760e-02,  9.9192e-03, -7.4511e-03,  3.0988e-02,\n",
            "        -2.2086e-02, -6.4188e-03, -1.7940e-02, -2.0288e-02,  2.3375e-03,\n",
            "        -3.4894e-02,  1.8265e-02, -4.4696e-03, -3.5505e-02,  2.9731e-02,\n",
            "        -2.9937e-02, -1.9132e-02,  2.9537e-02, -2.4241e-02, -2.0047e-02,\n",
            "        -2.9948e-02,  2.6326e-02, -7.3821e-03,  1.3956e-02,  5.9305e-04,\n",
            "         3.1852e-02, -3.4653e-02,  7.7812e-03, -4.6313e-03, -2.7798e-02,\n",
            "        -2.5538e-02,  1.4494e-02,  1.2191e-02,  7.5162e-03,  7.4573e-03,\n",
            "         1.9387e-02,  1.7999e-02,  3.2928e-02, -1.3793e-03, -3.0329e-02,\n",
            "        -2.5484e-02,  2.8515e-03, -1.6892e-02,  9.2227e-04,  1.5129e-02,\n",
            "        -1.7323e-02, -2.0776e-02, -2.7395e-02, -3.0105e-02, -1.7327e-02,\n",
            "         2.1297e-02, -1.2194e-02, -3.5388e-02, -3.8781e-03,  9.1573e-03,\n",
            "        -2.4413e-02,  2.3954e-02,  1.9340e-02, -2.4253e-02, -3.3061e-02,\n",
            "        -1.6854e-02, -2.0643e-02], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0265, -0.0055,  0.0176,  ..., -0.0030, -0.0155, -0.0356],\n",
            "        [ 0.0143,  0.0315, -0.0139,  ..., -0.0108, -0.0405,  0.0150],\n",
            "        [-0.0262, -0.0353,  0.0057,  ...,  0.0390, -0.0169,  0.0438],\n",
            "        ...,\n",
            "        [ 0.0226,  0.0176, -0.0235,  ..., -0.0307,  0.0200, -0.0333],\n",
            "        [-0.0116,  0.0031, -0.0310,  ..., -0.0105, -0.0048, -0.0272],\n",
            "        [ 0.0121, -0.0105, -0.0238,  ...,  0.0409, -0.0262,  0.0267]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-2.0224e-02,  1.7930e-02,  3.9494e-03,  2.1699e-02, -1.2709e-02,\n",
            "        -3.6198e-02,  1.6137e-03,  1.6842e-02,  4.3924e-02,  3.8722e-02,\n",
            "         8.6797e-04, -5.3017e-03,  7.1877e-04, -5.9595e-03,  4.1059e-02,\n",
            "         1.9690e-02, -2.8438e-02,  4.0911e-02, -2.2598e-02,  1.6791e-02,\n",
            "        -3.7689e-02,  1.7211e-02,  1.2223e-03, -3.7983e-02,  9.1571e-03,\n",
            "        -4.2542e-02, -1.2012e-02, -3.7332e-03,  4.8236e-03, -4.1423e-02,\n",
            "         9.7167e-03,  1.5538e-02, -3.0554e-02, -2.0716e-02,  2.0554e-02,\n",
            "        -2.2237e-02,  1.9547e-02,  1.9623e-02,  5.4384e-03,  4.7519e-03,\n",
            "         7.7620e-03,  2.7788e-02,  1.0715e-02,  2.9837e-02, -4.0729e-02,\n",
            "         3.2290e-02,  8.9276e-03,  1.9334e-02, -1.3454e-02, -3.5104e-04,\n",
            "         2.3127e-02, -2.8801e-02,  1.9178e-02, -3.8470e-02, -2.9133e-02,\n",
            "        -3.6195e-02, -1.9331e-02, -3.0152e-02, -1.4697e-02,  3.4181e-02,\n",
            "        -1.9369e-02, -2.5954e-02,  1.4797e-02, -3.4771e-02,  3.7867e-02,\n",
            "         1.9947e-02, -3.1836e-02,  1.0874e-02,  6.6989e-03, -1.2684e-03,\n",
            "         3.4830e-03, -3.5953e-02,  1.7042e-03, -3.1406e-02,  7.3108e-03,\n",
            "         4.0475e-03,  1.4938e-02, -3.9719e-02,  2.5452e-02, -1.9096e-02,\n",
            "         4.2727e-02, -3.1536e-02, -2.0905e-02,  2.9507e-02, -4.3565e-02,\n",
            "        -1.3646e-02,  3.3910e-02,  2.4832e-02, -2.2355e-02,  5.6202e-03,\n",
            "        -3.6443e-02,  2.4523e-02,  4.4143e-02,  8.6293e-03, -8.2079e-04,\n",
            "        -8.0257e-03, -5.9611e-03,  2.1857e-02, -2.0768e-02, -1.5471e-02,\n",
            "        -3.0931e-05,  1.0409e-02,  4.0081e-02,  9.3972e-04, -3.3174e-02,\n",
            "        -9.7505e-03, -3.3886e-02, -1.1992e-03,  1.9766e-02, -3.7972e-02,\n",
            "        -2.4027e-02,  3.1314e-02,  3.1432e-02,  4.3736e-02, -3.2710e-02,\n",
            "         3.2457e-03,  1.4293e-02,  3.5528e-02, -3.6718e-02,  6.4862e-03,\n",
            "        -4.2630e-02,  2.6105e-02,  2.4078e-02, -2.3694e-02, -2.6707e-02,\n",
            "        -1.0245e-02, -3.1853e-02,  2.2610e-02, -1.4832e-02, -4.0840e-02,\n",
            "        -5.2136e-03,  9.3182e-03,  2.6069e-02,  4.2525e-02, -3.8884e-02,\n",
            "         3.2175e-02,  3.5586e-02, -4.3196e-02,  6.6343e-03, -1.1996e-02,\n",
            "        -9.6395e-03,  2.2709e-02,  1.6377e-02, -1.9261e-02,  3.8016e-04,\n",
            "         1.7410e-02,  3.0780e-02, -6.0696e-03,  7.8273e-03,  7.9366e-03,\n",
            "        -1.5047e-02, -2.2576e-02, -1.5190e-02,  8.3509e-03, -2.4010e-02,\n",
            "         2.0257e-02, -2.3323e-02,  1.5359e-02, -1.2462e-02, -2.9037e-02,\n",
            "         2.2666e-02,  2.9765e-02,  1.8855e-02, -2.9613e-02, -4.3586e-02,\n",
            "         1.3057e-02, -4.3767e-02,  3.2036e-02, -2.2535e-02, -2.9065e-02,\n",
            "        -8.5990e-03,  2.7117e-02, -4.1226e-02,  3.7159e-02,  1.5916e-03,\n",
            "         2.1705e-02,  4.1732e-02,  1.1268e-02, -3.3719e-02,  8.9581e-03,\n",
            "         1.0345e-02, -8.9669e-03,  2.2426e-02,  3.4985e-02,  2.4813e-02,\n",
            "        -3.7423e-02, -2.0416e-02, -4.2620e-02, -2.8382e-02,  3.7829e-02,\n",
            "        -5.6955e-03, -2.7715e-02, -2.5299e-02, -2.5597e-02, -3.8747e-02,\n",
            "         3.3181e-02, -3.8148e-02, -3.8144e-02,  8.0478e-03, -4.3867e-02,\n",
            "         2.4477e-02, -3.9414e-02,  2.4411e-02, -4.0550e-02, -4.4050e-02,\n",
            "         1.5589e-02,  2.4634e-02, -1.4577e-02,  2.1762e-02,  1.0621e-02,\n",
            "         1.2222e-02, -1.8127e-02, -2.8991e-03, -5.7324e-03, -3.5443e-02,\n",
            "        -1.5182e-02, -4.0738e-02,  4.3375e-02, -3.0495e-02,  3.1185e-02,\n",
            "         1.3586e-02,  8.8529e-03,  1.5832e-02, -1.8555e-02, -3.5443e-02,\n",
            "         4.3151e-02, -1.5483e-02,  1.2171e-02,  6.9713e-03, -3.0089e-02,\n",
            "        -3.5298e-02, -1.6845e-02, -2.4901e-02,  4.3293e-02, -1.6097e-02,\n",
            "        -1.2644e-02, -3.3635e-02, -2.4081e-02,  3.3153e-02,  3.2834e-02,\n",
            "        -4.2169e-02,  4.3061e-02,  1.9346e-02,  1.2787e-03, -3.4451e-02,\n",
            "        -1.9156e-02, -3.8745e-02,  4.1726e-02,  3.1691e-02,  4.1628e-02,\n",
            "        -2.3946e-02,  1.9675e-02,  3.1308e-02,  2.9511e-03,  2.5032e-02,\n",
            "        -5.1855e-03], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0478, -0.0506,  0.0413,  ..., -0.0218,  0.0249,  0.0146],\n",
            "        [ 0.0047, -0.0331, -0.0116,  ..., -0.0189,  0.0481, -0.0410],\n",
            "        [-0.0329,  0.0043,  0.0018,  ..., -0.0328, -0.0101,  0.0528],\n",
            "        ...,\n",
            "        [ 0.0396,  0.0292, -0.0480,  ...,  0.0072,  0.0055, -0.0391],\n",
            "        [-0.0583, -0.0606,  0.0066,  ...,  0.0369, -0.0189,  0.0435],\n",
            "        [ 0.0197,  0.0343, -0.0273,  ..., -0.0326, -0.0115,  0.0121]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0137,  0.0320, -0.0018, -0.0251, -0.0127, -0.0546,  0.0144,  0.0602,\n",
            "         0.0316,  0.0370, -0.0009, -0.0033, -0.0101, -0.0035,  0.0157, -0.0149,\n",
            "        -0.0159,  0.0213,  0.0316,  0.0621, -0.0601,  0.0194,  0.0153, -0.0579,\n",
            "         0.0259, -0.0175, -0.0534, -0.0575, -0.0487,  0.0101,  0.0547, -0.0530,\n",
            "        -0.0591,  0.0334,  0.0431, -0.0514,  0.0430, -0.0099,  0.0482,  0.0027,\n",
            "         0.0235, -0.0622,  0.0139, -0.0324, -0.0591, -0.0205,  0.0576, -0.0095,\n",
            "         0.0266, -0.0038, -0.0208,  0.0138, -0.0575, -0.0203,  0.0117, -0.0303,\n",
            "         0.0161,  0.0289,  0.0202, -0.0163, -0.0355, -0.0090,  0.0358, -0.0191,\n",
            "         0.0108, -0.0584,  0.0546,  0.0360,  0.0427, -0.0005,  0.0249,  0.0521,\n",
            "         0.0083, -0.0076, -0.0468,  0.0491,  0.0512, -0.0084, -0.0174,  0.0601,\n",
            "         0.0426, -0.0266, -0.0130, -0.0005, -0.0594,  0.0404, -0.0120, -0.0556,\n",
            "        -0.0229, -0.0151, -0.0003, -0.0039,  0.0578,  0.0525, -0.0065, -0.0155,\n",
            "         0.0358,  0.0614,  0.0344, -0.0502, -0.0133, -0.0612, -0.0298,  0.0481,\n",
            "         0.0296,  0.0480, -0.0514,  0.0083,  0.0546, -0.0191,  0.0498, -0.0141,\n",
            "        -0.0299,  0.0470,  0.0196, -0.0089,  0.0430,  0.0212, -0.0326, -0.0563,\n",
            "        -0.0465, -0.0393, -0.0024,  0.0295,  0.0192, -0.0599,  0.0148,  0.0411],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0778,  0.0197,  0.0786,  ...,  0.0452,  0.0344, -0.0363],\n",
            "        [-0.0399,  0.0320, -0.0702,  ...,  0.0108, -0.0406,  0.0632],\n",
            "        [ 0.0400,  0.0312, -0.0145,  ...,  0.0323, -0.0579,  0.0520],\n",
            "        ...,\n",
            "        [ 0.0046, -0.0203,  0.0333,  ..., -0.0436, -0.0058,  0.0488],\n",
            "        [-0.0227, -0.0874,  0.0273,  ..., -0.0167, -0.0731,  0.0324],\n",
            "        [ 0.0205,  0.0482, -0.0151,  ...,  0.0637, -0.0857, -0.0760]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0318,  0.0882,  0.0706, -0.0799, -0.0076, -0.0614,  0.0114,  0.0553,\n",
            "         0.0342, -0.0594, -0.0072,  0.0777, -0.0287, -0.0209,  0.0636,  0.0202,\n",
            "        -0.0834,  0.0528, -0.0803, -0.0675, -0.0098,  0.0735, -0.0596, -0.0792,\n",
            "        -0.0779,  0.0688,  0.0747,  0.0474, -0.0843, -0.0541,  0.0617, -0.0358,\n",
            "        -0.0813,  0.0610,  0.0673,  0.0554,  0.0526, -0.0430,  0.0272, -0.0373,\n",
            "        -0.0758,  0.0866,  0.0205,  0.0353, -0.0231, -0.0724, -0.0735, -0.0517,\n",
            "        -0.0082,  0.0425, -0.0536, -0.0582, -0.0350, -0.0092,  0.0824,  0.0701,\n",
            "         0.0400,  0.0294,  0.0231,  0.0289, -0.0231,  0.0269,  0.0348, -0.0741],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0627, -0.1242, -0.0311,  ...,  0.0611,  0.0315,  0.0513],\n",
            "        [-0.1169,  0.0766, -0.1072,  ..., -0.0385,  0.0883,  0.0300],\n",
            "        [-0.0178, -0.1045, -0.1044,  ...,  0.0998,  0.0557,  0.0767],\n",
            "        ...,\n",
            "        [ 0.0598,  0.0561, -0.0645,  ...,  0.1065, -0.0786,  0.0674],\n",
            "        [-0.0746, -0.0503,  0.0919,  ...,  0.1116, -0.1227, -0.0956],\n",
            "        [ 0.0633,  0.1114, -0.0161,  ...,  0.0989,  0.1061,  0.0071]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0092, -0.0391,  0.0174,  0.0627,  0.0415, -0.0327,  0.0504,  0.0549,\n",
            "        -0.0022, -0.0817, -0.0883, -0.0609,  0.0759, -0.1000, -0.0391, -0.1236,\n",
            "        -0.1210,  0.0690, -0.0110,  0.1190,  0.0476, -0.0509, -0.0741,  0.0548,\n",
            "        -0.1196,  0.1157,  0.0742, -0.0460, -0.0766,  0.0603,  0.1044, -0.1128],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-5.2483e-02,  5.0459e-02, -1.7061e-01, -3.6427e-02, -8.4040e-03,\n",
            "          1.0406e-01,  1.4841e-02,  6.4224e-02, -7.6760e-02,  8.1995e-02,\n",
            "         -7.9679e-02,  6.1068e-03,  1.6618e-01, -3.3891e-02,  5.1307e-02,\n",
            "         -6.1053e-02,  1.8720e-02,  1.6610e-01,  4.3426e-02, -1.6633e-01,\n",
            "          1.1881e-01,  4.8434e-02,  1.0079e-01, -1.0569e-01, -1.2374e-01,\n",
            "          4.5161e-02,  3.9264e-03,  1.7346e-02,  1.1525e-01, -2.6251e-02,\n",
            "          7.6704e-03, -7.3531e-02],\n",
            "        [ 4.6973e-02, -9.5541e-02, -1.3196e-01, -1.3751e-01, -7.4808e-02,\n",
            "         -4.6493e-02, -5.0261e-02, -5.3447e-02, -1.4112e-01,  1.1737e-01,\n",
            "          6.1466e-02,  1.6950e-02, -1.3795e-01,  1.3861e-01, -1.4038e-01,\n",
            "          3.7970e-02,  1.7199e-01, -3.6258e-02,  7.6103e-02, -1.2705e-02,\n",
            "          7.9752e-02, -8.7843e-02, -1.1825e-01,  1.2046e-01, -5.6476e-02,\n",
            "         -1.1647e-01, -1.2372e-01, -1.7469e-01,  1.2147e-01, -2.1037e-02,\n",
            "         -1.1925e-01,  9.5274e-02],\n",
            "        [ 1.5443e-01,  1.0899e-01,  3.0143e-02,  3.8387e-03,  1.1800e-01,\n",
            "          6.4094e-02,  4.7297e-02, -6.1670e-02, -5.1116e-02,  1.0065e-01,\n",
            "          1.7095e-01,  8.3386e-02,  2.8133e-02,  4.2669e-02,  3.4173e-02,\n",
            "          1.0186e-01,  1.2883e-01,  1.7424e-02, -7.2112e-02,  8.6357e-02,\n",
            "          1.1131e-01,  5.2754e-02,  1.4499e-01, -8.5694e-02,  1.6688e-01,\n",
            "         -1.0161e-01, -2.5485e-02,  5.1628e-02, -1.4580e-01, -9.6005e-02,\n",
            "         -4.1549e-02,  1.1383e-01],\n",
            "        [-5.6616e-02,  2.0605e-02, -3.1682e-02,  4.4749e-02, -4.4506e-02,\n",
            "          7.5754e-02, -1.4712e-01,  1.0772e-01, -1.3553e-01, -3.5020e-03,\n",
            "          3.5863e-02,  1.4637e-01, -6.7086e-02, -1.2882e-01,  1.2116e-02,\n",
            "         -1.5386e-02, -8.2668e-02,  5.2585e-02,  1.0446e-01, -1.9473e-02,\n",
            "         -6.9223e-02,  5.8283e-02,  2.1604e-02, -8.2004e-02,  1.2958e-01,\n",
            "         -1.3248e-01, -2.2033e-02, -7.2746e-02, -1.2296e-01,  4.1121e-02,\n",
            "          1.4878e-01, -3.6633e-02],\n",
            "        [ 2.6541e-03,  4.6482e-02, -8.8047e-04, -1.1205e-01,  9.8286e-02,\n",
            "          1.2510e-01,  1.2286e-01, -6.7278e-02, -6.5778e-02, -8.6980e-02,\n",
            "          1.3689e-01,  6.1373e-02,  4.2757e-02, -5.2235e-02, -9.6033e-02,\n",
            "          1.2989e-01, -1.0493e-01, -1.3209e-01, -3.8595e-02,  3.7550e-02,\n",
            "          1.3210e-01,  1.3388e-01,  1.1222e-01,  1.3707e-02, -9.8105e-02,\n",
            "         -1.2692e-01,  9.9991e-03,  5.4083e-02, -3.2078e-02,  1.2246e-01,\n",
            "         -1.6656e-01, -2.5488e-02],\n",
            "        [-1.2027e-01,  1.2298e-01,  8.4226e-04,  1.2283e-01, -1.5447e-01,\n",
            "          6.2181e-02,  2.9212e-02,  1.6237e-01,  6.8519e-02,  1.2844e-01,\n",
            "          3.8225e-02, -1.4542e-01, -3.5261e-02,  4.4761e-02,  1.3510e-01,\n",
            "         -4.5304e-02, -1.2155e-01, -1.1273e-01, -1.3303e-01, -8.4048e-02,\n",
            "          8.0178e-02,  4.5848e-02,  3.8761e-02, -1.2430e-02, -2.1080e-02,\n",
            "          1.4441e-01,  1.4582e-01,  1.3472e-01, -1.1969e-01, -3.1051e-02,\n",
            "         -5.8248e-02,  1.5077e-01],\n",
            "        [ 6.5171e-02, -1.4133e-01, -4.9406e-02, -2.2822e-02, -1.5222e-01,\n",
            "         -6.8341e-02, -3.5820e-02,  1.2502e-01, -1.6179e-01, -1.3438e-01,\n",
            "          9.8223e-03,  1.1217e-01,  2.9256e-02, -1.2387e-01, -6.5741e-02,\n",
            "         -3.7477e-02, -9.7526e-02,  1.4088e-02,  3.4432e-02,  9.1040e-02,\n",
            "         -9.4425e-03, -2.1755e-02,  8.7034e-02, -9.2337e-02,  9.3996e-02,\n",
            "         -9.6657e-02, -1.7618e-01, -1.0225e-01,  6.4604e-02,  1.6029e-01,\n",
            "          1.4650e-01,  2.5536e-02],\n",
            "        [-9.4326e-02,  1.7333e-01,  6.1518e-02, -1.7128e-01,  1.0187e-01,\n",
            "          1.2122e-01, -5.0972e-02, -1.3233e-01,  9.0982e-03,  1.0217e-01,\n",
            "          5.3059e-02,  1.0903e-01, -2.3560e-02,  6.1247e-02, -1.4932e-01,\n",
            "          7.1950e-02,  3.1728e-02, -5.1315e-02,  1.3234e-01,  9.2530e-02,\n",
            "         -2.9714e-02,  1.5074e-01,  6.6405e-02,  8.8641e-02, -1.4508e-01,\n",
            "          1.2182e-01, -1.3659e-01, -1.0843e-01,  7.5308e-02, -3.4239e-02,\n",
            "          1.3835e-01, -5.7799e-02],\n",
            "        [ 8.7214e-02,  6.3419e-02,  8.8997e-03, -9.6196e-02,  3.8270e-02,\n",
            "         -7.4520e-02, -9.2038e-02,  2.2594e-02, -1.6495e-02,  3.9273e-02,\n",
            "          3.5019e-02,  1.7665e-01, -5.6859e-02, -3.3680e-02,  7.4223e-02,\n",
            "         -4.5361e-02,  1.7576e-01,  3.3324e-03,  1.5396e-01, -1.0239e-01,\n",
            "          5.9622e-02, -1.7556e-01, -1.1514e-01,  5.4074e-02,  8.2959e-02,\n",
            "         -1.0382e-01,  8.8800e-03, -1.4016e-01,  2.9504e-02, -1.4061e-01,\n",
            "         -1.4739e-02, -8.3410e-02],\n",
            "        [ 1.2913e-01,  7.1265e-02, -1.7190e-01, -1.6457e-01, -1.5003e-01,\n",
            "         -6.5758e-02,  5.3562e-02,  2.3644e-02,  6.3273e-02,  8.7193e-02,\n",
            "          1.5013e-01, -1.1326e-01, -8.1334e-02, -5.2910e-02, -5.2207e-02,\n",
            "         -4.6307e-02,  1.0644e-01, -1.1005e-01, -1.5326e-01,  6.9705e-02,\n",
            "         -5.5417e-03, -1.0409e-01,  8.8430e-02, -1.0323e-01,  4.9390e-02,\n",
            "         -6.7178e-02,  1.1769e-01,  6.3012e-02, -1.2442e-01,  4.4776e-02,\n",
            "          1.2443e-01,  1.4470e-01],\n",
            "        [ 6.1082e-02,  7.1084e-02, -8.2951e-02, -1.0018e-01,  7.7870e-02,\n",
            "          1.3585e-01,  1.4149e-01,  4.6265e-02, -5.9985e-02, -1.5782e-02,\n",
            "         -7.7766e-02, -1.5440e-01,  3.9055e-02, -1.7449e-02,  8.1437e-02,\n",
            "          4.3567e-02, -4.2194e-02, -5.4017e-03,  1.1701e-01, -7.8506e-03,\n",
            "          1.3811e-01,  9.4690e-02,  2.7345e-02,  1.6487e-01, -1.3557e-01,\n",
            "         -1.0322e-01,  6.6692e-02,  1.5663e-01,  6.1677e-02, -1.3717e-01,\n",
            "         -3.0944e-03, -1.4473e-01],\n",
            "        [-4.2143e-02,  1.0500e-01, -2.3043e-03,  3.9797e-02, -1.3362e-01,\n",
            "          1.0560e-01, -2.8678e-02,  5.2558e-02,  7.6644e-02, -1.3719e-01,\n",
            "          1.1611e-01,  1.2883e-01,  2.9988e-02, -3.1771e-02, -1.6348e-01,\n",
            "          6.5999e-02, -1.3663e-01,  1.4485e-01, -6.7663e-02, -9.7254e-02,\n",
            "         -3.6631e-02, -9.2010e-02, -1.5259e-01,  1.4778e-01, -7.9069e-02,\n",
            "         -1.5107e-01,  1.6198e-01, -8.5009e-02,  1.0247e-01,  2.3234e-02,\n",
            "         -1.3949e-01,  1.1473e-01],\n",
            "        [-7.7577e-02,  1.3281e-01,  1.4803e-01, -1.2384e-01,  1.0200e-01,\n",
            "         -1.0682e-01,  4.4453e-02,  8.2589e-02, -1.2836e-01,  4.8490e-02,\n",
            "          9.5772e-02, -1.2163e-01, -1.3688e-01,  1.7109e-01, -1.2610e-01,\n",
            "         -1.5021e-01,  1.7306e-01, -1.1021e-02, -1.1564e-01, -9.1133e-02,\n",
            "         -6.8191e-03, -8.1235e-02, -3.1761e-02,  1.1118e-03, -1.7192e-01,\n",
            "          1.9558e-02,  1.2143e-01, -9.1386e-02, -7.8595e-02,  6.7593e-02,\n",
            "          9.6710e-02, -1.6051e-01],\n",
            "        [ 1.4084e-01, -1.2682e-01, -1.3007e-01,  1.3007e-01, -1.5259e-01,\n",
            "         -3.2327e-02,  1.6711e-01, -1.5023e-01, -1.2059e-01, -1.6549e-01,\n",
            "          1.2152e-01, -8.2600e-02,  5.2595e-02,  7.5762e-02, -5.6454e-02,\n",
            "          6.9112e-02, -1.3344e-04, -7.6873e-02,  5.7786e-02, -1.6762e-01,\n",
            "          1.5931e-01,  7.6406e-02,  8.2772e-02, -5.4374e-02, -1.2185e-01,\n",
            "          1.4089e-01, -1.0627e-01, -1.0175e-01, -1.7365e-01, -1.0828e-01,\n",
            "         -1.6013e-01,  3.5357e-02],\n",
            "        [-8.6091e-02, -1.0900e-01,  8.1029e-03,  4.4700e-02,  1.5464e-01,\n",
            "         -1.2060e-01, -6.8259e-02, -6.9279e-02, -1.3319e-01,  5.3317e-02,\n",
            "          2.4999e-02, -6.5437e-02, -6.6006e-02,  1.1270e-01,  1.5152e-01,\n",
            "          4.9773e-02, -1.8545e-02, -1.1671e-01,  1.0960e-01, -6.1033e-02,\n",
            "         -5.5349e-02, -9.7658e-02, -3.7442e-02, -1.4805e-01,  1.3188e-03,\n",
            "         -1.6059e-01,  1.5467e-01, -1.6215e-01,  1.6559e-01, -1.7204e-02,\n",
            "          5.6477e-03, -1.3408e-02],\n",
            "        [-1.6871e-01,  8.2389e-02, -6.7693e-02, -1.5506e-01,  8.7521e-02,\n",
            "         -6.5445e-02, -1.5901e-01,  4.5679e-02, -5.3005e-02, -1.1357e-01,\n",
            "          9.1889e-02,  5.3124e-03, -7.3948e-02,  7.1908e-02, -1.6376e-01,\n",
            "          2.4767e-02, -1.5092e-01,  5.6059e-02,  1.5010e-01,  7.9858e-02,\n",
            "         -7.3902e-02,  4.8336e-02, -2.2535e-03, -2.1455e-04,  1.0719e-01,\n",
            "         -1.5227e-01,  1.5864e-01,  1.0459e-01, -1.7238e-01, -1.5809e-01,\n",
            "         -1.3906e-01, -1.1006e-01]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0896,  0.0485,  0.0806, -0.0901,  0.0217, -0.0836,  0.0586, -0.0311,\n",
            "        -0.1294,  0.0744, -0.1140, -0.0028,  0.0418,  0.0991, -0.0067,  0.0988],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0352,  0.1454,  0.0861,  0.1182,  0.0213,  0.1453,  0.2253,  0.1253,\n",
            "          0.0936, -0.1958, -0.0081, -0.1889,  0.1656, -0.0732,  0.0038,  0.1127],\n",
            "        [-0.1930, -0.0350,  0.2154,  0.0677,  0.0445, -0.2152, -0.2043,  0.2028,\n",
            "          0.1966,  0.1483,  0.0016,  0.0807,  0.0529, -0.1009,  0.1574,  0.2091],\n",
            "        [ 0.1908, -0.0646,  0.2270,  0.2362, -0.0065, -0.0601,  0.1513, -0.0149,\n",
            "          0.1213,  0.1941,  0.0236,  0.0541, -0.0834, -0.2112, -0.0825,  0.2065],\n",
            "        [-0.0124,  0.2349, -0.1676, -0.2099, -0.0935, -0.2477,  0.1036,  0.2470,\n",
            "          0.2100, -0.2063,  0.1903, -0.2216, -0.2341, -0.0794, -0.0024,  0.0842],\n",
            "        [ 0.2258, -0.1081,  0.1243,  0.2222,  0.1781, -0.2219, -0.1096, -0.0770,\n",
            "          0.1188, -0.0020, -0.2296,  0.0413, -0.0622, -0.1060, -0.1785,  0.0777],\n",
            "        [ 0.2485,  0.1740, -0.1241, -0.2387,  0.2451, -0.2218, -0.1309, -0.1768,\n",
            "          0.1161, -0.1898,  0.1663,  0.0440, -0.0274, -0.0633,  0.2433,  0.1617],\n",
            "        [ 0.2079,  0.0203, -0.1969,  0.0742,  0.2254,  0.2136, -0.2319, -0.1453,\n",
            "          0.1151, -0.0784, -0.2059, -0.1125, -0.1633,  0.0059, -0.1924,  0.1552],\n",
            "        [-0.1833, -0.0186,  0.1491,  0.2140,  0.1137, -0.1082, -0.2297,  0.0299,\n",
            "         -0.0067, -0.2035, -0.0676,  0.1626,  0.2380,  0.1501,  0.0229, -0.2419],\n",
            "        [-0.0242,  0.2060, -0.0215, -0.0253, -0.0180,  0.0275,  0.1643,  0.0455,\n",
            "         -0.1303, -0.2435, -0.2413, -0.1302,  0.1792, -0.1252, -0.0251,  0.0486],\n",
            "        [ 0.1049, -0.2433,  0.1675, -0.0079, -0.0709,  0.0995,  0.0110, -0.1276,\n",
            "         -0.0839, -0.2250, -0.0383,  0.1771,  0.0294, -0.0898, -0.1318,  0.0151]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.1556,  0.0967,  0.2272,  0.1361,  0.0347,  0.1980, -0.1450, -0.1125,\n",
            "         0.2272, -0.1625], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some training parameters\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Define our loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "GmzTy1fDgX4N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "qR4D98nyYWRB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training of Data"
      ],
      "metadata": {
        "id": "_VkKq4ALggFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training part, only minor changes have been made because I had an error once. For the rest I left it as default. Only for the training part I eventually made some changes, to let it continue after 20 epochs"
      ],
      "metadata": {
        "id": "uo7hUKjpm0rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    # Loop over batches via the dataloader\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print progress update every few loops\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "LDWHoJcugtiL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Printing some output after a testing round\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "sqvqI0I_gv1z"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to repeat the training process for each epoch.\n",
        "#   In each epoch, the model will eventually see EVERY\n",
        "#   observations in the data\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_loader, model, loss_fn, optimizer)\n",
        "    test_loop(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmqYzymag9UA",
        "outputId": "18e0619a-7ecb-4d0e-cd44-6e6aca3ffd85",
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.287567  [   64/60000]\n",
            "loss: 0.346154  [  704/60000]\n",
            "loss: 0.184471  [ 1344/60000]\n",
            "loss: 0.307112  [ 1984/60000]\n",
            "loss: 0.334379  [ 2624/60000]\n",
            "loss: 0.359173  [ 3264/60000]\n",
            "loss: 0.266468  [ 3904/60000]\n",
            "loss: 0.162830  [ 4544/60000]\n",
            "loss: 0.362062  [ 5184/60000]\n",
            "loss: 0.358872  [ 5824/60000]\n",
            "loss: 0.259077  [ 6464/60000]\n",
            "loss: 0.166177  [ 7104/60000]\n",
            "loss: 0.227919  [ 7744/60000]\n",
            "loss: 0.397168  [ 8384/60000]\n",
            "loss: 0.252455  [ 9024/60000]\n",
            "loss: 0.271041  [ 9664/60000]\n",
            "loss: 0.154117  [10304/60000]\n",
            "loss: 0.312020  [10944/60000]\n",
            "loss: 0.212168  [11584/60000]\n",
            "loss: 0.276612  [12224/60000]\n",
            "loss: 0.305235  [12864/60000]\n",
            "loss: 0.254099  [13504/60000]\n",
            "loss: 0.247920  [14144/60000]\n",
            "loss: 0.246068  [14784/60000]\n",
            "loss: 0.215796  [15424/60000]\n",
            "loss: 0.341520  [16064/60000]\n",
            "loss: 0.428889  [16704/60000]\n",
            "loss: 0.379980  [17344/60000]\n",
            "loss: 0.271408  [17984/60000]\n",
            "loss: 0.287836  [18624/60000]\n",
            "loss: 0.337902  [19264/60000]\n",
            "loss: 0.194428  [19904/60000]\n",
            "loss: 0.324155  [20544/60000]\n",
            "loss: 0.245499  [21184/60000]\n",
            "loss: 0.255769  [21824/60000]\n",
            "loss: 0.234215  [22464/60000]\n",
            "loss: 0.347861  [23104/60000]\n",
            "loss: 0.301191  [23744/60000]\n",
            "loss: 0.302901  [24384/60000]\n",
            "loss: 0.268400  [25024/60000]\n",
            "loss: 0.163083  [25664/60000]\n",
            "loss: 0.179933  [26304/60000]\n",
            "loss: 0.129168  [26944/60000]\n",
            "loss: 0.314890  [27584/60000]\n",
            "loss: 0.206383  [28224/60000]\n",
            "loss: 0.198043  [28864/60000]\n",
            "loss: 0.259595  [29504/60000]\n",
            "loss: 0.349562  [30144/60000]\n",
            "loss: 0.214767  [30784/60000]\n",
            "loss: 0.222745  [31424/60000]\n",
            "loss: 0.355622  [32064/60000]\n",
            "loss: 0.284464  [32704/60000]\n",
            "loss: 0.265760  [33344/60000]\n",
            "loss: 0.400862  [33984/60000]\n",
            "loss: 0.276735  [34624/60000]\n",
            "loss: 0.508723  [35264/60000]\n",
            "loss: 0.265243  [35904/60000]\n",
            "loss: 0.243331  [36544/60000]\n",
            "loss: 0.279793  [37184/60000]\n",
            "loss: 0.348125  [37824/60000]\n",
            "loss: 0.378924  [38464/60000]\n",
            "loss: 0.218111  [39104/60000]\n",
            "loss: 0.276488  [39744/60000]\n",
            "loss: 0.175895  [40384/60000]\n",
            "loss: 0.271403  [41024/60000]\n",
            "loss: 0.206589  [41664/60000]\n",
            "loss: 0.310231  [42304/60000]\n",
            "loss: 0.225115  [42944/60000]\n",
            "loss: 0.310579  [43584/60000]\n",
            "loss: 0.102466  [44224/60000]\n",
            "loss: 0.244412  [44864/60000]\n",
            "loss: 0.261850  [45504/60000]\n",
            "loss: 0.278616  [46144/60000]\n",
            "loss: 0.398388  [46784/60000]\n",
            "loss: 0.329924  [47424/60000]\n",
            "loss: 0.184486  [48064/60000]\n",
            "loss: 0.315135  [48704/60000]\n",
            "loss: 0.227177  [49344/60000]\n",
            "loss: 0.192519  [49984/60000]\n",
            "loss: 0.319209  [50624/60000]\n",
            "loss: 0.253594  [51264/60000]\n",
            "loss: 0.177635  [51904/60000]\n",
            "loss: 0.235700  [52544/60000]\n",
            "loss: 0.181098  [53184/60000]\n",
            "loss: 0.285061  [53824/60000]\n",
            "loss: 0.431248  [54464/60000]\n",
            "loss: 0.420752  [55104/60000]\n",
            "loss: 0.261562  [55744/60000]\n",
            "loss: 0.268923  [56384/60000]\n",
            "loss: 0.171049  [57024/60000]\n",
            "loss: 0.281965  [57664/60000]\n",
            "loss: 0.167645  [58304/60000]\n",
            "loss: 0.126941  [58944/60000]\n",
            "loss: 0.239504  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.401880 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.289426  [   64/60000]\n",
            "loss: 0.370019  [  704/60000]\n",
            "loss: 0.207115  [ 1344/60000]\n",
            "loss: 0.199963  [ 1984/60000]\n",
            "loss: 0.267675  [ 2624/60000]\n",
            "loss: 0.279897  [ 3264/60000]\n",
            "loss: 0.241879  [ 3904/60000]\n",
            "loss: 0.301906  [ 4544/60000]\n",
            "loss: 0.291969  [ 5184/60000]\n",
            "loss: 0.283359  [ 5824/60000]\n",
            "loss: 0.282256  [ 6464/60000]\n",
            "loss: 0.233950  [ 7104/60000]\n",
            "loss: 0.352211  [ 7744/60000]\n",
            "loss: 0.189864  [ 8384/60000]\n",
            "loss: 0.319514  [ 9024/60000]\n",
            "loss: 0.220280  [ 9664/60000]\n",
            "loss: 0.389312  [10304/60000]\n",
            "loss: 0.371015  [10944/60000]\n",
            "loss: 0.179546  [11584/60000]\n",
            "loss: 0.373167  [12224/60000]\n",
            "loss: 0.408585  [12864/60000]\n",
            "loss: 0.254610  [13504/60000]\n",
            "loss: 0.215218  [14144/60000]\n",
            "loss: 0.392168  [14784/60000]\n",
            "loss: 0.341237  [15424/60000]\n",
            "loss: 0.184449  [16064/60000]\n",
            "loss: 0.309040  [16704/60000]\n",
            "loss: 0.284585  [17344/60000]\n",
            "loss: 0.263263  [17984/60000]\n",
            "loss: 0.448307  [18624/60000]\n",
            "loss: 0.277148  [19264/60000]\n",
            "loss: 0.266856  [19904/60000]\n",
            "loss: 0.266367  [20544/60000]\n",
            "loss: 0.323869  [21184/60000]\n",
            "loss: 0.547329  [21824/60000]\n",
            "loss: 0.201529  [22464/60000]\n",
            "loss: 0.259956  [23104/60000]\n",
            "loss: 0.264742  [23744/60000]\n",
            "loss: 0.247075  [24384/60000]\n",
            "loss: 0.224973  [25024/60000]\n",
            "loss: 0.201731  [25664/60000]\n",
            "loss: 0.140304  [26304/60000]\n",
            "loss: 0.215715  [26944/60000]\n",
            "loss: 0.197763  [27584/60000]\n",
            "loss: 0.294173  [28224/60000]\n",
            "loss: 0.288452  [28864/60000]\n",
            "loss: 0.167582  [29504/60000]\n",
            "loss: 0.192490  [30144/60000]\n",
            "loss: 0.263832  [30784/60000]\n",
            "loss: 0.321649  [31424/60000]\n",
            "loss: 0.251452  [32064/60000]\n",
            "loss: 0.271927  [32704/60000]\n",
            "loss: 0.221614  [33344/60000]\n",
            "loss: 0.229134  [33984/60000]\n",
            "loss: 0.147713  [34624/60000]\n",
            "loss: 0.232244  [35264/60000]\n",
            "loss: 0.377538  [35904/60000]\n",
            "loss: 0.266694  [36544/60000]\n",
            "loss: 0.323193  [37184/60000]\n",
            "loss: 0.211561  [37824/60000]\n",
            "loss: 0.301838  [38464/60000]\n",
            "loss: 0.398568  [39104/60000]\n",
            "loss: 0.235004  [39744/60000]\n",
            "loss: 0.262258  [40384/60000]\n",
            "loss: 0.279321  [41024/60000]\n",
            "loss: 0.270855  [41664/60000]\n",
            "loss: 0.237076  [42304/60000]\n",
            "loss: 0.129627  [42944/60000]\n",
            "loss: 0.307544  [43584/60000]\n",
            "loss: 0.240600  [44224/60000]\n",
            "loss: 0.264804  [44864/60000]\n",
            "loss: 0.299226  [45504/60000]\n",
            "loss: 0.223553  [46144/60000]\n",
            "loss: 0.284042  [46784/60000]\n",
            "loss: 0.362364  [47424/60000]\n",
            "loss: 0.277411  [48064/60000]\n",
            "loss: 0.314525  [48704/60000]\n",
            "loss: 0.209418  [49344/60000]\n",
            "loss: 0.101337  [49984/60000]\n",
            "loss: 0.228841  [50624/60000]\n",
            "loss: 0.233634  [51264/60000]\n",
            "loss: 0.325346  [51904/60000]\n",
            "loss: 0.353858  [52544/60000]\n",
            "loss: 0.155172  [53184/60000]\n",
            "loss: 0.278053  [53824/60000]\n",
            "loss: 0.186905  [54464/60000]\n",
            "loss: 0.180890  [55104/60000]\n",
            "loss: 0.314619  [55744/60000]\n",
            "loss: 0.183357  [56384/60000]\n",
            "loss: 0.364783  [57024/60000]\n",
            "loss: 0.233126  [57664/60000]\n",
            "loss: 0.177450  [58304/60000]\n",
            "loss: 0.375336  [58944/60000]\n",
            "loss: 0.330427  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.403045 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.288840  [   64/60000]\n",
            "loss: 0.297297  [  704/60000]\n",
            "loss: 0.181260  [ 1344/60000]\n",
            "loss: 0.194871  [ 1984/60000]\n",
            "loss: 0.259662  [ 2624/60000]\n",
            "loss: 0.165877  [ 3264/60000]\n",
            "loss: 0.205013  [ 3904/60000]\n",
            "loss: 0.382320  [ 4544/60000]\n",
            "loss: 0.311462  [ 5184/60000]\n",
            "loss: 0.259963  [ 5824/60000]\n",
            "loss: 0.285226  [ 6464/60000]\n",
            "loss: 0.154372  [ 7104/60000]\n",
            "loss: 0.214408  [ 7744/60000]\n",
            "loss: 0.363537  [ 8384/60000]\n",
            "loss: 0.331394  [ 9024/60000]\n",
            "loss: 0.304642  [ 9664/60000]\n",
            "loss: 0.158944  [10304/60000]\n",
            "loss: 0.168519  [10944/60000]\n",
            "loss: 0.228416  [11584/60000]\n",
            "loss: 0.309817  [12224/60000]\n",
            "loss: 0.115224  [12864/60000]\n",
            "loss: 0.338122  [13504/60000]\n",
            "loss: 0.309504  [14144/60000]\n",
            "loss: 0.332984  [14784/60000]\n",
            "loss: 0.290020  [15424/60000]\n",
            "loss: 0.281492  [16064/60000]\n",
            "loss: 0.243779  [16704/60000]\n",
            "loss: 0.212911  [17344/60000]\n",
            "loss: 0.345589  [17984/60000]\n",
            "loss: 0.170868  [18624/60000]\n",
            "loss: 0.364451  [19264/60000]\n",
            "loss: 0.306667  [19904/60000]\n",
            "loss: 0.124572  [20544/60000]\n",
            "loss: 0.385823  [21184/60000]\n",
            "loss: 0.335376  [21824/60000]\n",
            "loss: 0.233913  [22464/60000]\n",
            "loss: 0.238802  [23104/60000]\n",
            "loss: 0.322641  [23744/60000]\n",
            "loss: 0.354236  [24384/60000]\n",
            "loss: 0.243553  [25024/60000]\n",
            "loss: 0.405463  [25664/60000]\n",
            "loss: 0.232753  [26304/60000]\n",
            "loss: 0.337029  [26944/60000]\n",
            "loss: 0.281466  [27584/60000]\n",
            "loss: 0.238560  [28224/60000]\n",
            "loss: 0.374966  [28864/60000]\n",
            "loss: 0.387968  [29504/60000]\n",
            "loss: 0.153624  [30144/60000]\n",
            "loss: 0.316817  [30784/60000]\n",
            "loss: 0.294299  [31424/60000]\n",
            "loss: 0.192054  [32064/60000]\n",
            "loss: 0.325130  [32704/60000]\n",
            "loss: 0.246025  [33344/60000]\n",
            "loss: 0.251082  [33984/60000]\n",
            "loss: 0.352731  [34624/60000]\n",
            "loss: 0.249702  [35264/60000]\n",
            "loss: 0.269714  [35904/60000]\n",
            "loss: 0.218842  [36544/60000]\n",
            "loss: 0.208826  [37184/60000]\n",
            "loss: 0.214138  [37824/60000]\n",
            "loss: 0.284096  [38464/60000]\n",
            "loss: 0.216606  [39104/60000]\n",
            "loss: 0.314153  [39744/60000]\n",
            "loss: 0.194771  [40384/60000]\n",
            "loss: 0.135952  [41024/60000]\n",
            "loss: 0.232608  [41664/60000]\n",
            "loss: 0.293767  [42304/60000]\n",
            "loss: 0.110090  [42944/60000]\n",
            "loss: 0.448354  [43584/60000]\n",
            "loss: 0.338326  [44224/60000]\n",
            "loss: 0.301050  [44864/60000]\n",
            "loss: 0.197806  [45504/60000]\n",
            "loss: 0.205078  [46144/60000]\n",
            "loss: 0.247758  [46784/60000]\n",
            "loss: 0.194785  [47424/60000]\n",
            "loss: 0.375535  [48064/60000]\n",
            "loss: 0.357729  [48704/60000]\n",
            "loss: 0.155519  [49344/60000]\n",
            "loss: 0.268993  [49984/60000]\n",
            "loss: 0.250571  [50624/60000]\n",
            "loss: 0.351124  [51264/60000]\n",
            "loss: 0.225125  [51904/60000]\n",
            "loss: 0.193060  [52544/60000]\n",
            "loss: 0.112285  [53184/60000]\n",
            "loss: 0.231660  [53824/60000]\n",
            "loss: 0.292222  [54464/60000]\n",
            "loss: 0.205129  [55104/60000]\n",
            "loss: 0.201868  [55744/60000]\n",
            "loss: 0.258458  [56384/60000]\n",
            "loss: 0.406871  [57024/60000]\n",
            "loss: 0.442674  [57664/60000]\n",
            "loss: 0.391656  [58304/60000]\n",
            "loss: 0.292299  [58944/60000]\n",
            "loss: 0.155559  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.388357 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.253686  [   64/60000]\n",
            "loss: 0.198568  [  704/60000]\n",
            "loss: 0.261342  [ 1344/60000]\n",
            "loss: 0.243816  [ 1984/60000]\n",
            "loss: 0.236902  [ 2624/60000]\n",
            "loss: 0.274485  [ 3264/60000]\n",
            "loss: 0.338154  [ 3904/60000]\n",
            "loss: 0.143395  [ 4544/60000]\n",
            "loss: 0.291793  [ 5184/60000]\n",
            "loss: 0.175207  [ 5824/60000]\n",
            "loss: 0.196142  [ 6464/60000]\n",
            "loss: 0.334097  [ 7104/60000]\n",
            "loss: 0.273075  [ 7744/60000]\n",
            "loss: 0.343273  [ 8384/60000]\n",
            "loss: 0.228148  [ 9024/60000]\n",
            "loss: 0.336640  [ 9664/60000]\n",
            "loss: 0.081008  [10304/60000]\n",
            "loss: 0.327295  [10944/60000]\n",
            "loss: 0.305551  [11584/60000]\n",
            "loss: 0.280835  [12224/60000]\n",
            "loss: 0.260692  [12864/60000]\n",
            "loss: 0.205841  [13504/60000]\n",
            "loss: 0.210670  [14144/60000]\n",
            "loss: 0.351599  [14784/60000]\n",
            "loss: 0.258195  [15424/60000]\n",
            "loss: 0.206351  [16064/60000]\n",
            "loss: 0.210744  [16704/60000]\n",
            "loss: 0.333113  [17344/60000]\n",
            "loss: 0.232990  [17984/60000]\n",
            "loss: 0.318259  [18624/60000]\n",
            "loss: 0.314189  [19264/60000]\n",
            "loss: 0.250633  [19904/60000]\n",
            "loss: 0.285698  [20544/60000]\n",
            "loss: 0.216940  [21184/60000]\n",
            "loss: 0.219137  [21824/60000]\n",
            "loss: 0.091043  [22464/60000]\n",
            "loss: 0.185818  [23104/60000]\n",
            "loss: 0.218619  [23744/60000]\n",
            "loss: 0.299772  [24384/60000]\n",
            "loss: 0.180807  [25024/60000]\n",
            "loss: 0.240018  [25664/60000]\n",
            "loss: 0.168395  [26304/60000]\n",
            "loss: 0.133318  [26944/60000]\n",
            "loss: 0.398605  [27584/60000]\n",
            "loss: 0.169066  [28224/60000]\n",
            "loss: 0.259028  [28864/60000]\n",
            "loss: 0.233269  [29504/60000]\n",
            "loss: 0.279365  [30144/60000]\n",
            "loss: 0.193701  [30784/60000]\n",
            "loss: 0.224513  [31424/60000]\n",
            "loss: 0.322593  [32064/60000]\n",
            "loss: 0.334284  [32704/60000]\n",
            "loss: 0.272275  [33344/60000]\n",
            "loss: 0.225339  [33984/60000]\n",
            "loss: 0.279504  [34624/60000]\n",
            "loss: 0.238180  [35264/60000]\n",
            "loss: 0.329506  [35904/60000]\n",
            "loss: 0.256761  [36544/60000]\n",
            "loss: 0.238937  [37184/60000]\n",
            "loss: 0.300218  [37824/60000]\n",
            "loss: 0.290501  [38464/60000]\n",
            "loss: 0.136835  [39104/60000]\n",
            "loss: 0.178515  [39744/60000]\n",
            "loss: 0.314835  [40384/60000]\n",
            "loss: 0.266894  [41024/60000]\n",
            "loss: 0.270889  [41664/60000]\n",
            "loss: 0.242016  [42304/60000]\n",
            "loss: 0.335597  [42944/60000]\n",
            "loss: 0.192958  [43584/60000]\n",
            "loss: 0.276890  [44224/60000]\n",
            "loss: 0.272583  [44864/60000]\n",
            "loss: 0.236468  [45504/60000]\n",
            "loss: 0.278941  [46144/60000]\n",
            "loss: 0.163531  [46784/60000]\n",
            "loss: 0.255634  [47424/60000]\n",
            "loss: 0.177011  [48064/60000]\n",
            "loss: 0.476423  [48704/60000]\n",
            "loss: 0.246861  [49344/60000]\n",
            "loss: 0.315298  [49984/60000]\n",
            "loss: 0.194563  [50624/60000]\n",
            "loss: 0.286787  [51264/60000]\n",
            "loss: 0.272167  [51904/60000]\n",
            "loss: 0.284278  [52544/60000]\n",
            "loss: 0.356719  [53184/60000]\n",
            "loss: 0.274419  [53824/60000]\n",
            "loss: 0.348047  [54464/60000]\n",
            "loss: 0.152713  [55104/60000]\n",
            "loss: 0.486966  [55744/60000]\n",
            "loss: 0.230679  [56384/60000]\n",
            "loss: 0.321592  [57024/60000]\n",
            "loss: 0.319247  [57664/60000]\n",
            "loss: 0.346625  [58304/60000]\n",
            "loss: 0.293889  [58944/60000]\n",
            "loss: 0.232757  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.378422 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.247346  [   64/60000]\n",
            "loss: 0.251897  [  704/60000]\n",
            "loss: 0.250226  [ 1344/60000]\n",
            "loss: 0.261800  [ 1984/60000]\n",
            "loss: 0.176243  [ 2624/60000]\n",
            "loss: 0.221121  [ 3264/60000]\n",
            "loss: 0.176200  [ 3904/60000]\n",
            "loss: 0.209500  [ 4544/60000]\n",
            "loss: 0.226045  [ 5184/60000]\n",
            "loss: 0.145047  [ 5824/60000]\n",
            "loss: 0.176049  [ 6464/60000]\n",
            "loss: 0.165269  [ 7104/60000]\n",
            "loss: 0.260800  [ 7744/60000]\n",
            "loss: 0.295155  [ 8384/60000]\n",
            "loss: 0.224261  [ 9024/60000]\n",
            "loss: 0.226240  [ 9664/60000]\n",
            "loss: 0.260537  [10304/60000]\n",
            "loss: 0.182567  [10944/60000]\n",
            "loss: 0.295390  [11584/60000]\n",
            "loss: 0.337294  [12224/60000]\n",
            "loss: 0.323762  [12864/60000]\n",
            "loss: 0.258515  [13504/60000]\n",
            "loss: 0.321901  [14144/60000]\n",
            "loss: 0.158185  [14784/60000]\n",
            "loss: 0.214495  [15424/60000]\n",
            "loss: 0.339808  [16064/60000]\n",
            "loss: 0.371135  [16704/60000]\n",
            "loss: 0.120511  [17344/60000]\n",
            "loss: 0.655541  [17984/60000]\n",
            "loss: 0.151420  [18624/60000]\n",
            "loss: 0.268978  [19264/60000]\n",
            "loss: 0.440821  [19904/60000]\n",
            "loss: 0.143168  [20544/60000]\n",
            "loss: 0.284727  [21184/60000]\n",
            "loss: 0.186127  [21824/60000]\n",
            "loss: 0.184580  [22464/60000]\n",
            "loss: 0.379583  [23104/60000]\n",
            "loss: 0.203012  [23744/60000]\n",
            "loss: 0.280947  [24384/60000]\n",
            "loss: 0.294915  [25024/60000]\n",
            "loss: 0.138210  [25664/60000]\n",
            "loss: 0.310894  [26304/60000]\n",
            "loss: 0.294387  [26944/60000]\n",
            "loss: 0.174181  [27584/60000]\n",
            "loss: 0.160439  [28224/60000]\n",
            "loss: 0.328407  [28864/60000]\n",
            "loss: 0.232077  [29504/60000]\n",
            "loss: 0.262491  [30144/60000]\n",
            "loss: 0.272240  [30784/60000]\n",
            "loss: 0.323696  [31424/60000]\n",
            "loss: 0.248779  [32064/60000]\n",
            "loss: 0.151617  [32704/60000]\n",
            "loss: 0.167869  [33344/60000]\n",
            "loss: 0.250939  [33984/60000]\n",
            "loss: 0.259992  [34624/60000]\n",
            "loss: 0.237357  [35264/60000]\n",
            "loss: 0.226306  [35904/60000]\n",
            "loss: 0.203931  [36544/60000]\n",
            "loss: 0.346933  [37184/60000]\n",
            "loss: 0.237433  [37824/60000]\n",
            "loss: 0.301174  [38464/60000]\n",
            "loss: 0.344500  [39104/60000]\n",
            "loss: 0.192955  [39744/60000]\n",
            "loss: 0.413965  [40384/60000]\n",
            "loss: 0.204084  [41024/60000]\n",
            "loss: 0.479089  [41664/60000]\n",
            "loss: 0.320783  [42304/60000]\n",
            "loss: 0.183131  [42944/60000]\n",
            "loss: 0.181212  [43584/60000]\n",
            "loss: 0.194683  [44224/60000]\n",
            "loss: 0.243831  [44864/60000]\n",
            "loss: 0.318601  [45504/60000]\n",
            "loss: 0.266244  [46144/60000]\n",
            "loss: 0.271718  [46784/60000]\n",
            "loss: 0.273307  [47424/60000]\n",
            "loss: 0.340214  [48064/60000]\n",
            "loss: 0.246551  [48704/60000]\n",
            "loss: 0.252364  [49344/60000]\n",
            "loss: 0.131926  [49984/60000]\n",
            "loss: 0.459918  [50624/60000]\n",
            "loss: 0.270214  [51264/60000]\n",
            "loss: 0.228089  [51904/60000]\n",
            "loss: 0.288335  [52544/60000]\n",
            "loss: 0.378602  [53184/60000]\n",
            "loss: 0.155214  [53824/60000]\n",
            "loss: 0.167718  [54464/60000]\n",
            "loss: 0.264563  [55104/60000]\n",
            "loss: 0.327262  [55744/60000]\n",
            "loss: 0.285797  [56384/60000]\n",
            "loss: 0.230686  [57024/60000]\n",
            "loss: 0.248406  [57664/60000]\n",
            "loss: 0.282613  [58304/60000]\n",
            "loss: 0.240784  [58944/60000]\n",
            "loss: 0.219429  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.362461 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.242561  [   64/60000]\n",
            "loss: 0.255997  [  704/60000]\n",
            "loss: 0.247348  [ 1344/60000]\n",
            "loss: 0.253890  [ 1984/60000]\n",
            "loss: 0.313911  [ 2624/60000]\n",
            "loss: 0.248902  [ 3264/60000]\n",
            "loss: 0.223216  [ 3904/60000]\n",
            "loss: 0.223005  [ 4544/60000]\n",
            "loss: 0.161033  [ 5184/60000]\n",
            "loss: 0.242082  [ 5824/60000]\n",
            "loss: 0.219577  [ 6464/60000]\n",
            "loss: 0.145104  [ 7104/60000]\n",
            "loss: 0.232915  [ 7744/60000]\n",
            "loss: 0.159124  [ 8384/60000]\n",
            "loss: 0.286408  [ 9024/60000]\n",
            "loss: 0.349250  [ 9664/60000]\n",
            "loss: 0.199125  [10304/60000]\n",
            "loss: 0.345511  [10944/60000]\n",
            "loss: 0.493415  [11584/60000]\n",
            "loss: 0.264874  [12224/60000]\n",
            "loss: 0.167022  [12864/60000]\n",
            "loss: 0.249835  [13504/60000]\n",
            "loss: 0.212213  [14144/60000]\n",
            "loss: 0.265268  [14784/60000]\n",
            "loss: 0.308075  [15424/60000]\n",
            "loss: 0.171192  [16064/60000]\n",
            "loss: 0.161232  [16704/60000]\n",
            "loss: 0.295031  [17344/60000]\n",
            "loss: 0.132075  [17984/60000]\n",
            "loss: 0.216807  [18624/60000]\n",
            "loss: 0.227834  [19264/60000]\n",
            "loss: 0.287994  [19904/60000]\n",
            "loss: 0.121941  [20544/60000]\n",
            "loss: 0.248299  [21184/60000]\n",
            "loss: 0.277260  [21824/60000]\n",
            "loss: 0.218073  [22464/60000]\n",
            "loss: 0.248336  [23104/60000]\n",
            "loss: 0.361600  [23744/60000]\n",
            "loss: 0.242882  [24384/60000]\n",
            "loss: 0.393611  [25024/60000]\n",
            "loss: 0.175377  [25664/60000]\n",
            "loss: 0.170104  [26304/60000]\n",
            "loss: 0.415226  [26944/60000]\n",
            "loss: 0.328091  [27584/60000]\n",
            "loss: 0.333902  [28224/60000]\n",
            "loss: 0.256409  [28864/60000]\n",
            "loss: 0.215708  [29504/60000]\n",
            "loss: 0.449937  [30144/60000]\n",
            "loss: 0.242230  [30784/60000]\n",
            "loss: 0.156748  [31424/60000]\n",
            "loss: 0.196117  [32064/60000]\n",
            "loss: 0.320272  [32704/60000]\n",
            "loss: 0.299636  [33344/60000]\n",
            "loss: 0.175990  [33984/60000]\n",
            "loss: 0.191272  [34624/60000]\n",
            "loss: 0.217574  [35264/60000]\n",
            "loss: 0.221265  [35904/60000]\n",
            "loss: 0.282489  [36544/60000]\n",
            "loss: 0.098369  [37184/60000]\n",
            "loss: 0.247786  [37824/60000]\n",
            "loss: 0.295566  [38464/60000]\n",
            "loss: 0.274303  [39104/60000]\n",
            "loss: 0.366255  [39744/60000]\n",
            "loss: 0.313400  [40384/60000]\n",
            "loss: 0.293775  [41024/60000]\n",
            "loss: 0.206374  [41664/60000]\n",
            "loss: 0.226959  [42304/60000]\n",
            "loss: 0.317041  [42944/60000]\n",
            "loss: 0.224006  [43584/60000]\n",
            "loss: 0.312716  [44224/60000]\n",
            "loss: 0.229992  [44864/60000]\n",
            "loss: 0.183098  [45504/60000]\n",
            "loss: 0.264116  [46144/60000]\n",
            "loss: 0.271831  [46784/60000]\n",
            "loss: 0.242759  [47424/60000]\n",
            "loss: 0.249328  [48064/60000]\n",
            "loss: 0.499715  [48704/60000]\n",
            "loss: 0.266705  [49344/60000]\n",
            "loss: 0.176079  [49984/60000]\n",
            "loss: 0.298961  [50624/60000]\n",
            "loss: 0.506261  [51264/60000]\n",
            "loss: 0.368801  [51904/60000]\n",
            "loss: 0.278387  [52544/60000]\n",
            "loss: 0.235886  [53184/60000]\n",
            "loss: 0.155822  [53824/60000]\n",
            "loss: 0.281591  [54464/60000]\n",
            "loss: 0.353339  [55104/60000]\n",
            "loss: 0.170987  [55744/60000]\n",
            "loss: 0.111860  [56384/60000]\n",
            "loss: 0.129246  [57024/60000]\n",
            "loss: 0.265261  [57664/60000]\n",
            "loss: 0.378459  [58304/60000]\n",
            "loss: 0.311676  [58944/60000]\n",
            "loss: 0.353863  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.403208 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.199306  [   64/60000]\n",
            "loss: 0.263143  [  704/60000]\n",
            "loss: 0.275467  [ 1344/60000]\n",
            "loss: 0.288548  [ 1984/60000]\n",
            "loss: 0.286172  [ 2624/60000]\n",
            "loss: 0.370668  [ 3264/60000]\n",
            "loss: 0.264551  [ 3904/60000]\n",
            "loss: 0.173083  [ 4544/60000]\n",
            "loss: 0.390016  [ 5184/60000]\n",
            "loss: 0.323283  [ 5824/60000]\n",
            "loss: 0.313137  [ 6464/60000]\n",
            "loss: 0.195701  [ 7104/60000]\n",
            "loss: 0.196804  [ 7744/60000]\n",
            "loss: 0.268195  [ 8384/60000]\n",
            "loss: 0.256245  [ 9024/60000]\n",
            "loss: 0.142794  [ 9664/60000]\n",
            "loss: 0.353405  [10304/60000]\n",
            "loss: 0.158381  [10944/60000]\n",
            "loss: 0.269305  [11584/60000]\n",
            "loss: 0.332076  [12224/60000]\n",
            "loss: 0.241268  [12864/60000]\n",
            "loss: 0.339575  [13504/60000]\n",
            "loss: 0.125508  [14144/60000]\n",
            "loss: 0.275704  [14784/60000]\n",
            "loss: 0.262581  [15424/60000]\n",
            "loss: 0.274913  [16064/60000]\n",
            "loss: 0.414527  [16704/60000]\n",
            "loss: 0.189158  [17344/60000]\n",
            "loss: 0.198051  [17984/60000]\n",
            "loss: 0.169108  [18624/60000]\n",
            "loss: 0.416555  [19264/60000]\n",
            "loss: 0.222415  [19904/60000]\n",
            "loss: 0.258298  [20544/60000]\n",
            "loss: 0.263430  [21184/60000]\n",
            "loss: 0.233684  [21824/60000]\n",
            "loss: 0.195920  [22464/60000]\n",
            "loss: 0.220544  [23104/60000]\n",
            "loss: 0.249195  [23744/60000]\n",
            "loss: 0.415597  [24384/60000]\n",
            "loss: 0.093090  [25024/60000]\n",
            "loss: 0.386805  [25664/60000]\n",
            "loss: 0.177318  [26304/60000]\n",
            "loss: 0.355187  [26944/60000]\n",
            "loss: 0.394056  [27584/60000]\n",
            "loss: 0.158613  [28224/60000]\n",
            "loss: 0.239458  [28864/60000]\n",
            "loss: 0.225049  [29504/60000]\n",
            "loss: 0.147932  [30144/60000]\n",
            "loss: 0.164292  [30784/60000]\n",
            "loss: 0.219172  [31424/60000]\n",
            "loss: 0.175938  [32064/60000]\n",
            "loss: 0.257260  [32704/60000]\n",
            "loss: 0.228331  [33344/60000]\n",
            "loss: 0.198890  [33984/60000]\n",
            "loss: 0.213084  [34624/60000]\n",
            "loss: 0.216643  [35264/60000]\n",
            "loss: 0.290458  [35904/60000]\n",
            "loss: 0.231157  [36544/60000]\n",
            "loss: 0.177188  [37184/60000]\n",
            "loss: 0.237508  [37824/60000]\n",
            "loss: 0.148591  [38464/60000]\n",
            "loss: 0.302513  [39104/60000]\n",
            "loss: 0.289941  [39744/60000]\n",
            "loss: 0.272241  [40384/60000]\n",
            "loss: 0.263174  [41024/60000]\n",
            "loss: 0.413489  [41664/60000]\n",
            "loss: 0.341048  [42304/60000]\n",
            "loss: 0.232790  [42944/60000]\n",
            "loss: 0.194327  [43584/60000]\n",
            "loss: 0.140243  [44224/60000]\n",
            "loss: 0.226438  [44864/60000]\n",
            "loss: 0.342241  [45504/60000]\n",
            "loss: 0.266221  [46144/60000]\n",
            "loss: 0.202294  [46784/60000]\n",
            "loss: 0.159376  [47424/60000]\n",
            "loss: 0.171543  [48064/60000]\n",
            "loss: 0.155588  [48704/60000]\n",
            "loss: 0.260111  [49344/60000]\n",
            "loss: 0.184780  [49984/60000]\n",
            "loss: 0.245211  [50624/60000]\n",
            "loss: 0.159232  [51264/60000]\n",
            "loss: 0.484453  [51904/60000]\n",
            "loss: 0.296861  [52544/60000]\n",
            "loss: 0.202003  [53184/60000]\n",
            "loss: 0.257744  [53824/60000]\n",
            "loss: 0.398616  [54464/60000]\n",
            "loss: 0.195467  [55104/60000]\n",
            "loss: 0.160681  [55744/60000]\n",
            "loss: 0.177651  [56384/60000]\n",
            "loss: 0.106612  [57024/60000]\n",
            "loss: 0.260687  [57664/60000]\n",
            "loss: 0.279821  [58304/60000]\n",
            "loss: 0.286694  [58944/60000]\n",
            "loss: 0.398192  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.382166 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.238091  [   64/60000]\n",
            "loss: 0.146212  [  704/60000]\n",
            "loss: 0.215404  [ 1344/60000]\n",
            "loss: 0.195451  [ 1984/60000]\n",
            "loss: 0.121739  [ 2624/60000]\n",
            "loss: 0.240479  [ 3264/60000]\n",
            "loss: 0.200757  [ 3904/60000]\n",
            "loss: 0.179743  [ 4544/60000]\n",
            "loss: 0.203301  [ 5184/60000]\n",
            "loss: 0.266286  [ 5824/60000]\n",
            "loss: 0.198769  [ 6464/60000]\n",
            "loss: 0.225335  [ 7104/60000]\n",
            "loss: 0.212349  [ 7744/60000]\n",
            "loss: 0.263627  [ 8384/60000]\n",
            "loss: 0.248367  [ 9024/60000]\n",
            "loss: 0.198766  [ 9664/60000]\n",
            "loss: 0.179650  [10304/60000]\n",
            "loss: 0.301186  [10944/60000]\n",
            "loss: 0.135268  [11584/60000]\n",
            "loss: 0.579243  [12224/60000]\n",
            "loss: 0.273283  [12864/60000]\n",
            "loss: 0.171020  [13504/60000]\n",
            "loss: 0.307760  [14144/60000]\n",
            "loss: 0.180477  [14784/60000]\n",
            "loss: 0.304003  [15424/60000]\n",
            "loss: 0.174635  [16064/60000]\n",
            "loss: 0.436611  [16704/60000]\n",
            "loss: 0.214155  [17344/60000]\n",
            "loss: 0.084454  [17984/60000]\n",
            "loss: 0.190681  [18624/60000]\n",
            "loss: 0.229638  [19264/60000]\n",
            "loss: 0.299662  [19904/60000]\n",
            "loss: 0.431954  [20544/60000]\n",
            "loss: 0.182580  [21184/60000]\n",
            "loss: 0.268148  [21824/60000]\n",
            "loss: 0.224692  [22464/60000]\n",
            "loss: 0.251543  [23104/60000]\n",
            "loss: 0.150811  [23744/60000]\n",
            "loss: 0.238093  [24384/60000]\n",
            "loss: 0.197263  [25024/60000]\n",
            "loss: 0.315777  [25664/60000]\n",
            "loss: 0.229451  [26304/60000]\n",
            "loss: 0.342871  [26944/60000]\n",
            "loss: 0.218267  [27584/60000]\n",
            "loss: 0.334410  [28224/60000]\n",
            "loss: 0.153810  [28864/60000]\n",
            "loss: 0.185843  [29504/60000]\n",
            "loss: 0.323219  [30144/60000]\n",
            "loss: 0.217001  [30784/60000]\n",
            "loss: 0.148560  [31424/60000]\n",
            "loss: 0.141678  [32064/60000]\n",
            "loss: 0.324186  [32704/60000]\n",
            "loss: 0.413051  [33344/60000]\n",
            "loss: 0.167621  [33984/60000]\n",
            "loss: 0.147952  [34624/60000]\n",
            "loss: 0.162309  [35264/60000]\n",
            "loss: 0.330382  [35904/60000]\n",
            "loss: 0.208816  [36544/60000]\n",
            "loss: 0.294015  [37184/60000]\n",
            "loss: 0.589731  [37824/60000]\n",
            "loss: 0.216728  [38464/60000]\n",
            "loss: 0.100643  [39104/60000]\n",
            "loss: 0.217501  [39744/60000]\n",
            "loss: 0.290009  [40384/60000]\n",
            "loss: 0.166981  [41024/60000]\n",
            "loss: 0.155006  [41664/60000]\n",
            "loss: 0.347961  [42304/60000]\n",
            "loss: 0.272987  [42944/60000]\n",
            "loss: 0.216110  [43584/60000]\n",
            "loss: 0.258568  [44224/60000]\n",
            "loss: 0.328619  [44864/60000]\n",
            "loss: 0.251833  [45504/60000]\n",
            "loss: 0.464952  [46144/60000]\n",
            "loss: 0.251390  [46784/60000]\n",
            "loss: 0.250237  [47424/60000]\n",
            "loss: 0.231939  [48064/60000]\n",
            "loss: 0.188978  [48704/60000]\n",
            "loss: 0.146755  [49344/60000]\n",
            "loss: 0.188976  [49984/60000]\n",
            "loss: 0.366274  [50624/60000]\n",
            "loss: 0.339412  [51264/60000]\n",
            "loss: 0.228971  [51904/60000]\n",
            "loss: 0.445683  [52544/60000]\n",
            "loss: 0.252450  [53184/60000]\n",
            "loss: 0.299191  [53824/60000]\n",
            "loss: 0.452463  [54464/60000]\n",
            "loss: 0.111919  [55104/60000]\n",
            "loss: 0.316297  [55744/60000]\n",
            "loss: 0.235344  [56384/60000]\n",
            "loss: 0.342670  [57024/60000]\n",
            "loss: 0.184215  [57664/60000]\n",
            "loss: 0.301947  [58304/60000]\n",
            "loss: 0.102361  [58944/60000]\n",
            "loss: 0.252698  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.398015 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.196617  [   64/60000]\n",
            "loss: 0.387117  [  704/60000]\n",
            "loss: 0.168500  [ 1344/60000]\n",
            "loss: 0.278065  [ 1984/60000]\n",
            "loss: 0.171492  [ 2624/60000]\n",
            "loss: 0.241741  [ 3264/60000]\n",
            "loss: 0.244049  [ 3904/60000]\n",
            "loss: 0.152293  [ 4544/60000]\n",
            "loss: 0.233852  [ 5184/60000]\n",
            "loss: 0.165911  [ 5824/60000]\n",
            "loss: 0.443972  [ 6464/60000]\n",
            "loss: 0.214176  [ 7104/60000]\n",
            "loss: 0.402677  [ 7744/60000]\n",
            "loss: 0.237383  [ 8384/60000]\n",
            "loss: 0.350703  [ 9024/60000]\n",
            "loss: 0.161336  [ 9664/60000]\n",
            "loss: 0.183583  [10304/60000]\n",
            "loss: 0.086383  [10944/60000]\n",
            "loss: 0.231119  [11584/60000]\n",
            "loss: 0.314738  [12224/60000]\n",
            "loss: 0.324172  [12864/60000]\n",
            "loss: 0.432664  [13504/60000]\n",
            "loss: 0.234551  [14144/60000]\n",
            "loss: 0.361558  [14784/60000]\n",
            "loss: 0.265582  [15424/60000]\n",
            "loss: 0.436803  [16064/60000]\n",
            "loss: 0.257466  [16704/60000]\n",
            "loss: 0.124420  [17344/60000]\n",
            "loss: 0.299611  [17984/60000]\n",
            "loss: 0.481633  [18624/60000]\n",
            "loss: 0.556701  [19264/60000]\n",
            "loss: 0.213758  [19904/60000]\n",
            "loss: 0.265765  [20544/60000]\n",
            "loss: 0.203794  [21184/60000]\n",
            "loss: 0.300732  [21824/60000]\n",
            "loss: 0.178235  [22464/60000]\n",
            "loss: 0.227407  [23104/60000]\n",
            "loss: 0.356170  [23744/60000]\n",
            "loss: 0.245518  [24384/60000]\n",
            "loss: 0.161452  [25024/60000]\n",
            "loss: 0.319835  [25664/60000]\n",
            "loss: 0.202911  [26304/60000]\n",
            "loss: 0.124847  [26944/60000]\n",
            "loss: 0.154980  [27584/60000]\n",
            "loss: 0.273173  [28224/60000]\n",
            "loss: 0.308322  [28864/60000]\n",
            "loss: 0.247441  [29504/60000]\n",
            "loss: 0.188356  [30144/60000]\n",
            "loss: 0.136270  [30784/60000]\n",
            "loss: 0.266801  [31424/60000]\n",
            "loss: 0.244812  [32064/60000]\n",
            "loss: 0.153646  [32704/60000]\n",
            "loss: 0.295820  [33344/60000]\n",
            "loss: 0.228030  [33984/60000]\n",
            "loss: 0.195526  [34624/60000]\n",
            "loss: 0.140284  [35264/60000]\n",
            "loss: 0.210932  [35904/60000]\n",
            "loss: 0.227282  [36544/60000]\n",
            "loss: 0.277381  [37184/60000]\n",
            "loss: 0.189576  [37824/60000]\n",
            "loss: 0.320458  [38464/60000]\n",
            "loss: 0.253743  [39104/60000]\n",
            "loss: 0.545789  [39744/60000]\n",
            "loss: 0.313437  [40384/60000]\n",
            "loss: 0.253025  [41024/60000]\n",
            "loss: 0.257109  [41664/60000]\n",
            "loss: 0.273332  [42304/60000]\n",
            "loss: 0.362040  [42944/60000]\n",
            "loss: 0.338557  [43584/60000]\n",
            "loss: 0.395562  [44224/60000]\n",
            "loss: 0.351333  [44864/60000]\n",
            "loss: 0.118194  [45504/60000]\n",
            "loss: 0.210811  [46144/60000]\n",
            "loss: 0.190883  [46784/60000]\n",
            "loss: 0.285490  [47424/60000]\n",
            "loss: 0.292284  [48064/60000]\n",
            "loss: 0.255590  [48704/60000]\n",
            "loss: 0.298101  [49344/60000]\n",
            "loss: 0.290059  [49984/60000]\n",
            "loss: 0.325902  [50624/60000]\n",
            "loss: 0.300211  [51264/60000]\n",
            "loss: 0.210687  [51904/60000]\n",
            "loss: 0.291557  [52544/60000]\n",
            "loss: 0.239375  [53184/60000]\n",
            "loss: 0.277238  [53824/60000]\n",
            "loss: 0.257783  [54464/60000]\n",
            "loss: 0.423991  [55104/60000]\n",
            "loss: 0.153590  [55744/60000]\n",
            "loss: 0.209662  [56384/60000]\n",
            "loss: 0.268423  [57024/60000]\n",
            "loss: 0.327564  [57664/60000]\n",
            "loss: 0.324608  [58304/60000]\n",
            "loss: 0.366675  [58944/60000]\n",
            "loss: 0.278215  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.355069 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.262091  [   64/60000]\n",
            "loss: 0.147161  [  704/60000]\n",
            "loss: 0.214852  [ 1344/60000]\n",
            "loss: 0.296050  [ 1984/60000]\n",
            "loss: 0.270451  [ 2624/60000]\n",
            "loss: 0.352482  [ 3264/60000]\n",
            "loss: 0.214738  [ 3904/60000]\n",
            "loss: 0.204445  [ 4544/60000]\n",
            "loss: 0.329511  [ 5184/60000]\n",
            "loss: 0.187228  [ 5824/60000]\n",
            "loss: 0.301365  [ 6464/60000]\n",
            "loss: 0.181962  [ 7104/60000]\n",
            "loss: 0.142572  [ 7744/60000]\n",
            "loss: 0.245560  [ 8384/60000]\n",
            "loss: 0.188115  [ 9024/60000]\n",
            "loss: 0.344457  [ 9664/60000]\n",
            "loss: 0.254918  [10304/60000]\n",
            "loss: 0.243747  [10944/60000]\n",
            "loss: 0.237655  [11584/60000]\n",
            "loss: 0.253240  [12224/60000]\n",
            "loss: 0.213812  [12864/60000]\n",
            "loss: 0.364378  [13504/60000]\n",
            "loss: 0.178745  [14144/60000]\n",
            "loss: 0.257216  [14784/60000]\n",
            "loss: 0.222202  [15424/60000]\n",
            "loss: 0.319906  [16064/60000]\n",
            "loss: 0.300865  [16704/60000]\n",
            "loss: 0.108674  [17344/60000]\n",
            "loss: 0.158464  [17984/60000]\n",
            "loss: 0.309329  [18624/60000]\n",
            "loss: 0.256382  [19264/60000]\n",
            "loss: 0.192934  [19904/60000]\n",
            "loss: 0.284848  [20544/60000]\n",
            "loss: 0.155967  [21184/60000]\n",
            "loss: 0.227381  [21824/60000]\n",
            "loss: 0.279404  [22464/60000]\n",
            "loss: 0.174869  [23104/60000]\n",
            "loss: 0.147184  [23744/60000]\n",
            "loss: 0.260501  [24384/60000]\n",
            "loss: 0.140818  [25024/60000]\n",
            "loss: 0.199527  [25664/60000]\n",
            "loss: 0.316713  [26304/60000]\n",
            "loss: 0.298199  [26944/60000]\n",
            "loss: 0.435483  [27584/60000]\n",
            "loss: 0.121591  [28224/60000]\n",
            "loss: 0.220855  [28864/60000]\n",
            "loss: 0.267063  [29504/60000]\n",
            "loss: 0.219504  [30144/60000]\n",
            "loss: 0.133165  [30784/60000]\n",
            "loss: 0.207433  [31424/60000]\n",
            "loss: 0.155846  [32064/60000]\n",
            "loss: 0.403919  [32704/60000]\n",
            "loss: 0.221889  [33344/60000]\n",
            "loss: 0.224528  [33984/60000]\n",
            "loss: 0.261099  [34624/60000]\n",
            "loss: 0.219619  [35264/60000]\n",
            "loss: 0.318351  [35904/60000]\n",
            "loss: 0.165531  [36544/60000]\n",
            "loss: 0.196171  [37184/60000]\n",
            "loss: 0.145080  [37824/60000]\n",
            "loss: 0.253029  [38464/60000]\n",
            "loss: 0.158993  [39104/60000]\n",
            "loss: 0.346059  [39744/60000]\n",
            "loss: 0.231984  [40384/60000]\n",
            "loss: 0.288370  [41024/60000]\n",
            "loss: 0.092078  [41664/60000]\n",
            "loss: 0.169920  [42304/60000]\n",
            "loss: 0.200379  [42944/60000]\n",
            "loss: 0.216654  [43584/60000]\n",
            "loss: 0.269689  [44224/60000]\n",
            "loss: 0.193780  [44864/60000]\n",
            "loss: 0.266154  [45504/60000]\n",
            "loss: 0.333732  [46144/60000]\n",
            "loss: 0.298415  [46784/60000]\n",
            "loss: 0.324983  [47424/60000]\n",
            "loss: 0.236966  [48064/60000]\n",
            "loss: 0.448473  [48704/60000]\n",
            "loss: 0.173929  [49344/60000]\n",
            "loss: 0.158505  [49984/60000]\n",
            "loss: 0.332430  [50624/60000]\n",
            "loss: 0.263685  [51264/60000]\n",
            "loss: 0.201005  [51904/60000]\n",
            "loss: 0.329588  [52544/60000]\n",
            "loss: 0.146806  [53184/60000]\n",
            "loss: 0.316287  [53824/60000]\n",
            "loss: 0.161286  [54464/60000]\n",
            "loss: 0.160307  [55104/60000]\n",
            "loss: 0.336600  [55744/60000]\n",
            "loss: 0.349377  [56384/60000]\n",
            "loss: 0.216362  [57024/60000]\n",
            "loss: 0.224417  [57664/60000]\n",
            "loss: 0.195130  [58304/60000]\n",
            "loss: 0.312501  [58944/60000]\n",
            "loss: 0.313908  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.419518 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.197475  [   64/60000]\n",
            "loss: 0.240682  [  704/60000]\n",
            "loss: 0.208387  [ 1344/60000]\n",
            "loss: 0.251905  [ 1984/60000]\n",
            "loss: 0.194179  [ 2624/60000]\n",
            "loss: 0.123473  [ 3264/60000]\n",
            "loss: 0.200153  [ 3904/60000]\n",
            "loss: 0.332372  [ 4544/60000]\n",
            "loss: 0.203850  [ 5184/60000]\n",
            "loss: 0.258867  [ 5824/60000]\n",
            "loss: 0.314922  [ 6464/60000]\n",
            "loss: 0.279683  [ 7104/60000]\n",
            "loss: 0.142637  [ 7744/60000]\n",
            "loss: 0.279232  [ 8384/60000]\n",
            "loss: 0.257445  [ 9024/60000]\n",
            "loss: 0.284226  [ 9664/60000]\n",
            "loss: 0.167875  [10304/60000]\n",
            "loss: 0.137489  [10944/60000]\n",
            "loss: 0.295714  [11584/60000]\n",
            "loss: 0.216832  [12224/60000]\n",
            "loss: 0.193169  [12864/60000]\n",
            "loss: 0.337223  [13504/60000]\n",
            "loss: 0.392301  [14144/60000]\n",
            "loss: 0.217141  [14784/60000]\n",
            "loss: 0.164028  [15424/60000]\n",
            "loss: 0.158599  [16064/60000]\n",
            "loss: 0.251102  [16704/60000]\n",
            "loss: 0.113735  [17344/60000]\n",
            "loss: 0.288466  [17984/60000]\n",
            "loss: 0.277633  [18624/60000]\n",
            "loss: 0.277682  [19264/60000]\n",
            "loss: 0.308562  [19904/60000]\n",
            "loss: 0.173346  [20544/60000]\n",
            "loss: 0.192046  [21184/60000]\n",
            "loss: 0.325934  [21824/60000]\n",
            "loss: 0.382539  [22464/60000]\n",
            "loss: 0.444059  [23104/60000]\n",
            "loss: 0.259774  [23744/60000]\n",
            "loss: 0.259686  [24384/60000]\n",
            "loss: 0.280698  [25024/60000]\n",
            "loss: 0.332439  [25664/60000]\n",
            "loss: 0.221165  [26304/60000]\n",
            "loss: 0.160854  [26944/60000]\n",
            "loss: 0.259070  [27584/60000]\n",
            "loss: 0.284989  [28224/60000]\n",
            "loss: 0.262947  [28864/60000]\n",
            "loss: 0.352150  [29504/60000]\n",
            "loss: 0.107473  [30144/60000]\n",
            "loss: 0.163389  [30784/60000]\n",
            "loss: 0.240300  [31424/60000]\n",
            "loss: 0.453221  [32064/60000]\n",
            "loss: 0.198685  [32704/60000]\n",
            "loss: 0.296359  [33344/60000]\n",
            "loss: 0.368001  [33984/60000]\n",
            "loss: 0.274496  [34624/60000]\n",
            "loss: 0.204564  [35264/60000]\n",
            "loss: 0.509423  [35904/60000]\n",
            "loss: 0.230072  [36544/60000]\n",
            "loss: 0.150333  [37184/60000]\n",
            "loss: 0.214892  [37824/60000]\n",
            "loss: 0.170432  [38464/60000]\n",
            "loss: 0.105479  [39104/60000]\n",
            "loss: 0.219028  [39744/60000]\n",
            "loss: 0.292499  [40384/60000]\n",
            "loss: 0.162271  [41024/60000]\n",
            "loss: 0.360893  [41664/60000]\n",
            "loss: 0.270079  [42304/60000]\n",
            "loss: 0.259786  [42944/60000]\n",
            "loss: 0.287634  [43584/60000]\n",
            "loss: 0.235949  [44224/60000]\n",
            "loss: 0.166618  [44864/60000]\n",
            "loss: 0.348453  [45504/60000]\n",
            "loss: 0.219892  [46144/60000]\n",
            "loss: 0.226846  [46784/60000]\n",
            "loss: 0.271108  [47424/60000]\n",
            "loss: 0.330686  [48064/60000]\n",
            "loss: 0.322473  [48704/60000]\n",
            "loss: 0.108169  [49344/60000]\n",
            "loss: 0.311220  [49984/60000]\n",
            "loss: 0.287813  [50624/60000]\n",
            "loss: 0.194303  [51264/60000]\n",
            "loss: 0.254716  [51904/60000]\n",
            "loss: 0.343399  [52544/60000]\n",
            "loss: 0.200254  [53184/60000]\n",
            "loss: 0.329986  [53824/60000]\n",
            "loss: 0.192113  [54464/60000]\n",
            "loss: 0.293397  [55104/60000]\n",
            "loss: 0.165211  [55744/60000]\n",
            "loss: 0.208305  [56384/60000]\n",
            "loss: 0.289726  [57024/60000]\n",
            "loss: 0.343595  [57664/60000]\n",
            "loss: 0.224250  [58304/60000]\n",
            "loss: 0.159825  [58944/60000]\n",
            "loss: 0.243530  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.411209 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.213602  [   64/60000]\n",
            "loss: 0.294395  [  704/60000]\n",
            "loss: 0.189382  [ 1344/60000]\n",
            "loss: 0.256776  [ 1984/60000]\n",
            "loss: 0.215594  [ 2624/60000]\n",
            "loss: 0.154248  [ 3264/60000]\n",
            "loss: 0.229096  [ 3904/60000]\n",
            "loss: 0.311806  [ 4544/60000]\n",
            "loss: 0.446993  [ 5184/60000]\n",
            "loss: 0.165931  [ 5824/60000]\n",
            "loss: 0.280191  [ 6464/60000]\n",
            "loss: 0.232658  [ 7104/60000]\n",
            "loss: 0.112429  [ 7744/60000]\n",
            "loss: 0.178235  [ 8384/60000]\n",
            "loss: 0.209351  [ 9024/60000]\n",
            "loss: 0.213274  [ 9664/60000]\n",
            "loss: 0.159483  [10304/60000]\n",
            "loss: 0.180143  [10944/60000]\n",
            "loss: 0.229032  [11584/60000]\n",
            "loss: 0.108576  [12224/60000]\n",
            "loss: 0.411434  [12864/60000]\n",
            "loss: 0.294786  [13504/60000]\n",
            "loss: 0.239840  [14144/60000]\n",
            "loss: 0.289970  [14784/60000]\n",
            "loss: 0.187757  [15424/60000]\n",
            "loss: 0.257388  [16064/60000]\n",
            "loss: 0.278874  [16704/60000]\n",
            "loss: 0.290223  [17344/60000]\n",
            "loss: 0.171564  [17984/60000]\n",
            "loss: 0.102403  [18624/60000]\n",
            "loss: 0.140128  [19264/60000]\n",
            "loss: 0.211677  [19904/60000]\n",
            "loss: 0.210945  [20544/60000]\n",
            "loss: 0.130404  [21184/60000]\n",
            "loss: 0.278012  [21824/60000]\n",
            "loss: 0.278732  [22464/60000]\n",
            "loss: 0.291065  [23104/60000]\n",
            "loss: 0.192495  [23744/60000]\n",
            "loss: 0.368535  [24384/60000]\n",
            "loss: 0.105000  [25024/60000]\n",
            "loss: 0.299242  [25664/60000]\n",
            "loss: 0.250991  [26304/60000]\n",
            "loss: 0.267801  [26944/60000]\n",
            "loss: 0.350597  [27584/60000]\n",
            "loss: 0.282732  [28224/60000]\n",
            "loss: 0.203321  [28864/60000]\n",
            "loss: 0.266737  [29504/60000]\n",
            "loss: 0.164459  [30144/60000]\n",
            "loss: 0.272761  [30784/60000]\n",
            "loss: 0.167866  [31424/60000]\n",
            "loss: 0.332815  [32064/60000]\n",
            "loss: 0.287900  [32704/60000]\n",
            "loss: 0.188631  [33344/60000]\n",
            "loss: 0.190997  [33984/60000]\n",
            "loss: 0.168079  [34624/60000]\n",
            "loss: 0.070927  [35264/60000]\n",
            "loss: 0.294832  [35904/60000]\n",
            "loss: 0.309323  [36544/60000]\n",
            "loss: 0.200229  [37184/60000]\n",
            "loss: 0.191601  [37824/60000]\n",
            "loss: 0.236058  [38464/60000]\n",
            "loss: 0.304931  [39104/60000]\n",
            "loss: 0.220177  [39744/60000]\n",
            "loss: 0.310216  [40384/60000]\n",
            "loss: 0.250385  [41024/60000]\n",
            "loss: 0.225277  [41664/60000]\n",
            "loss: 0.205637  [42304/60000]\n",
            "loss: 0.290661  [42944/60000]\n",
            "loss: 0.175835  [43584/60000]\n",
            "loss: 0.188653  [44224/60000]\n",
            "loss: 0.243386  [44864/60000]\n",
            "loss: 0.169104  [45504/60000]\n",
            "loss: 0.315263  [46144/60000]\n",
            "loss: 0.169456  [46784/60000]\n",
            "loss: 0.249429  [47424/60000]\n",
            "loss: 0.172774  [48064/60000]\n",
            "loss: 0.171017  [48704/60000]\n",
            "loss: 0.202485  [49344/60000]\n",
            "loss: 0.304512  [49984/60000]\n",
            "loss: 0.106678  [50624/60000]\n",
            "loss: 0.204086  [51264/60000]\n",
            "loss: 0.278951  [51904/60000]\n",
            "loss: 0.165145  [52544/60000]\n",
            "loss: 0.309791  [53184/60000]\n",
            "loss: 0.460561  [53824/60000]\n",
            "loss: 0.222771  [54464/60000]\n",
            "loss: 0.232163  [55104/60000]\n",
            "loss: 0.117508  [55744/60000]\n",
            "loss: 0.279686  [56384/60000]\n",
            "loss: 0.236291  [57024/60000]\n",
            "loss: 0.213246  [57664/60000]\n",
            "loss: 0.179210  [58304/60000]\n",
            "loss: 0.295569  [58944/60000]\n",
            "loss: 0.228910  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.380710 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.339723  [   64/60000]\n",
            "loss: 0.289722  [  704/60000]\n",
            "loss: 0.112571  [ 1344/60000]\n",
            "loss: 0.251320  [ 1984/60000]\n",
            "loss: 0.170297  [ 2624/60000]\n",
            "loss: 0.271469  [ 3264/60000]\n",
            "loss: 0.173234  [ 3904/60000]\n",
            "loss: 0.226388  [ 4544/60000]\n",
            "loss: 0.298049  [ 5184/60000]\n",
            "loss: 0.197117  [ 5824/60000]\n",
            "loss: 0.092272  [ 6464/60000]\n",
            "loss: 0.194468  [ 7104/60000]\n",
            "loss: 0.257170  [ 7744/60000]\n",
            "loss: 0.216783  [ 8384/60000]\n",
            "loss: 0.375066  [ 9024/60000]\n",
            "loss: 0.244916  [ 9664/60000]\n",
            "loss: 0.214719  [10304/60000]\n",
            "loss: 0.284342  [10944/60000]\n",
            "loss: 0.221834  [11584/60000]\n",
            "loss: 0.370925  [12224/60000]\n",
            "loss: 0.241206  [12864/60000]\n",
            "loss: 0.178604  [13504/60000]\n",
            "loss: 0.332776  [14144/60000]\n",
            "loss: 0.231279  [14784/60000]\n",
            "loss: 0.372408  [15424/60000]\n",
            "loss: 0.206118  [16064/60000]\n",
            "loss: 0.232962  [16704/60000]\n",
            "loss: 0.156109  [17344/60000]\n",
            "loss: 0.260420  [17984/60000]\n",
            "loss: 0.164286  [18624/60000]\n",
            "loss: 0.275614  [19264/60000]\n",
            "loss: 0.244328  [19904/60000]\n",
            "loss: 0.182411  [20544/60000]\n",
            "loss: 0.169559  [21184/60000]\n",
            "loss: 0.377298  [21824/60000]\n",
            "loss: 0.200952  [22464/60000]\n",
            "loss: 0.334628  [23104/60000]\n",
            "loss: 0.276249  [23744/60000]\n",
            "loss: 0.298077  [24384/60000]\n",
            "loss: 0.173246  [25024/60000]\n",
            "loss: 0.235579  [25664/60000]\n",
            "loss: 0.172985  [26304/60000]\n",
            "loss: 0.102948  [26944/60000]\n",
            "loss: 0.274284  [27584/60000]\n",
            "loss: 0.188529  [28224/60000]\n",
            "loss: 0.308320  [28864/60000]\n",
            "loss: 0.243206  [29504/60000]\n",
            "loss: 0.483869  [30144/60000]\n",
            "loss: 0.214592  [30784/60000]\n",
            "loss: 0.208630  [31424/60000]\n",
            "loss: 0.347199  [32064/60000]\n",
            "loss: 0.338330  [32704/60000]\n",
            "loss: 0.273090  [33344/60000]\n",
            "loss: 0.184665  [33984/60000]\n",
            "loss: 0.218977  [34624/60000]\n",
            "loss: 0.256011  [35264/60000]\n",
            "loss: 0.159925  [35904/60000]\n",
            "loss: 0.176986  [36544/60000]\n",
            "loss: 0.118623  [37184/60000]\n",
            "loss: 0.261039  [37824/60000]\n",
            "loss: 0.283543  [38464/60000]\n",
            "loss: 0.142993  [39104/60000]\n",
            "loss: 0.219300  [39744/60000]\n",
            "loss: 0.165739  [40384/60000]\n",
            "loss: 0.315905  [41024/60000]\n",
            "loss: 0.219983  [41664/60000]\n",
            "loss: 0.323270  [42304/60000]\n",
            "loss: 0.180866  [42944/60000]\n",
            "loss: 0.166345  [43584/60000]\n",
            "loss: 0.195494  [44224/60000]\n",
            "loss: 0.282701  [44864/60000]\n",
            "loss: 0.222213  [45504/60000]\n",
            "loss: 0.391313  [46144/60000]\n",
            "loss: 0.197749  [46784/60000]\n",
            "loss: 0.112106  [47424/60000]\n",
            "loss: 0.227020  [48064/60000]\n",
            "loss: 0.125984  [48704/60000]\n",
            "loss: 0.246968  [49344/60000]\n",
            "loss: 0.259713  [49984/60000]\n",
            "loss: 0.330286  [50624/60000]\n",
            "loss: 0.237063  [51264/60000]\n",
            "loss: 0.117432  [51904/60000]\n",
            "loss: 0.213397  [52544/60000]\n",
            "loss: 0.208211  [53184/60000]\n",
            "loss: 0.175655  [53824/60000]\n",
            "loss: 0.252916  [54464/60000]\n",
            "loss: 0.355904  [55104/60000]\n",
            "loss: 0.255419  [55744/60000]\n",
            "loss: 0.151646  [56384/60000]\n",
            "loss: 0.168790  [57024/60000]\n",
            "loss: 0.274912  [57664/60000]\n",
            "loss: 0.293921  [58304/60000]\n",
            "loss: 0.400873  [58944/60000]\n",
            "loss: 0.148755  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.364616 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.188874  [   64/60000]\n",
            "loss: 0.173790  [  704/60000]\n",
            "loss: 0.187285  [ 1344/60000]\n",
            "loss: 0.158669  [ 1984/60000]\n",
            "loss: 0.237904  [ 2624/60000]\n",
            "loss: 0.289499  [ 3264/60000]\n",
            "loss: 0.253394  [ 3904/60000]\n",
            "loss: 0.165086  [ 4544/60000]\n",
            "loss: 0.256996  [ 5184/60000]\n",
            "loss: 0.279024  [ 5824/60000]\n",
            "loss: 0.383400  [ 6464/60000]\n",
            "loss: 0.225470  [ 7104/60000]\n",
            "loss: 0.214628  [ 7744/60000]\n",
            "loss: 0.154411  [ 8384/60000]\n",
            "loss: 0.192704  [ 9024/60000]\n",
            "loss: 0.152899  [ 9664/60000]\n",
            "loss: 0.212365  [10304/60000]\n",
            "loss: 0.266364  [10944/60000]\n",
            "loss: 0.285301  [11584/60000]\n",
            "loss: 0.133234  [12224/60000]\n",
            "loss: 0.321022  [12864/60000]\n",
            "loss: 0.187118  [13504/60000]\n",
            "loss: 0.344276  [14144/60000]\n",
            "loss: 0.240844  [14784/60000]\n",
            "loss: 0.123831  [15424/60000]\n",
            "loss: 0.194854  [16064/60000]\n",
            "loss: 0.186262  [16704/60000]\n",
            "loss: 0.329780  [17344/60000]\n",
            "loss: 0.237551  [17984/60000]\n",
            "loss: 0.173969  [18624/60000]\n",
            "loss: 0.088544  [19264/60000]\n",
            "loss: 0.295261  [19904/60000]\n",
            "loss: 0.128159  [20544/60000]\n",
            "loss: 0.135585  [21184/60000]\n",
            "loss: 0.231609  [21824/60000]\n",
            "loss: 0.417108  [22464/60000]\n",
            "loss: 0.202154  [23104/60000]\n",
            "loss: 0.279036  [23744/60000]\n",
            "loss: 0.200407  [24384/60000]\n",
            "loss: 0.387287  [25024/60000]\n",
            "loss: 0.371291  [25664/60000]\n",
            "loss: 0.213791  [26304/60000]\n",
            "loss: 0.170236  [26944/60000]\n",
            "loss: 0.197603  [27584/60000]\n",
            "loss: 0.335597  [28224/60000]\n",
            "loss: 0.276515  [28864/60000]\n",
            "loss: 0.120739  [29504/60000]\n",
            "loss: 0.284715  [30144/60000]\n",
            "loss: 0.311810  [30784/60000]\n",
            "loss: 0.311158  [31424/60000]\n",
            "loss: 0.188295  [32064/60000]\n",
            "loss: 0.335526  [32704/60000]\n",
            "loss: 0.133056  [33344/60000]\n",
            "loss: 0.149846  [33984/60000]\n",
            "loss: 0.212061  [34624/60000]\n",
            "loss: 0.107173  [35264/60000]\n",
            "loss: 0.320924  [35904/60000]\n",
            "loss: 0.131298  [36544/60000]\n",
            "loss: 0.377507  [37184/60000]\n",
            "loss: 0.391586  [37824/60000]\n",
            "loss: 0.183782  [38464/60000]\n",
            "loss: 0.404458  [39104/60000]\n",
            "loss: 0.169428  [39744/60000]\n",
            "loss: 0.270792  [40384/60000]\n",
            "loss: 0.323250  [41024/60000]\n",
            "loss: 0.394514  [41664/60000]\n",
            "loss: 0.280786  [42304/60000]\n",
            "loss: 0.222141  [42944/60000]\n",
            "loss: 0.322377  [43584/60000]\n",
            "loss: 0.287600  [44224/60000]\n",
            "loss: 0.296439  [44864/60000]\n",
            "loss: 0.163127  [45504/60000]\n",
            "loss: 0.227435  [46144/60000]\n",
            "loss: 0.327459  [46784/60000]\n",
            "loss: 0.214457  [47424/60000]\n",
            "loss: 0.547406  [48064/60000]\n",
            "loss: 0.152105  [48704/60000]\n",
            "loss: 0.369909  [49344/60000]\n",
            "loss: 0.376945  [49984/60000]\n",
            "loss: 0.136751  [50624/60000]\n",
            "loss: 0.127062  [51264/60000]\n",
            "loss: 0.229327  [51904/60000]\n",
            "loss: 0.389886  [52544/60000]\n",
            "loss: 0.316095  [53184/60000]\n",
            "loss: 0.206674  [53824/60000]\n",
            "loss: 0.231872  [54464/60000]\n",
            "loss: 0.189924  [55104/60000]\n",
            "loss: 0.169996  [55744/60000]\n",
            "loss: 0.167091  [56384/60000]\n",
            "loss: 0.401597  [57024/60000]\n",
            "loss: 0.261137  [57664/60000]\n",
            "loss: 0.206044  [58304/60000]\n",
            "loss: 0.255661  [58944/60000]\n",
            "loss: 0.391451  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.399232 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.231203  [   64/60000]\n",
            "loss: 0.161412  [  704/60000]\n",
            "loss: 0.128523  [ 1344/60000]\n",
            "loss: 0.302100  [ 1984/60000]\n",
            "loss: 0.162095  [ 2624/60000]\n",
            "loss: 0.280650  [ 3264/60000]\n",
            "loss: 0.118884  [ 3904/60000]\n",
            "loss: 0.494426  [ 4544/60000]\n",
            "loss: 0.202618  [ 5184/60000]\n",
            "loss: 0.264113  [ 5824/60000]\n",
            "loss: 0.207575  [ 6464/60000]\n",
            "loss: 0.279425  [ 7104/60000]\n",
            "loss: 0.252050  [ 7744/60000]\n",
            "loss: 0.276772  [ 8384/60000]\n",
            "loss: 0.299459  [ 9024/60000]\n",
            "loss: 0.143682  [ 9664/60000]\n",
            "loss: 0.218402  [10304/60000]\n",
            "loss: 0.166203  [10944/60000]\n",
            "loss: 0.297256  [11584/60000]\n",
            "loss: 0.137782  [12224/60000]\n",
            "loss: 0.171875  [12864/60000]\n",
            "loss: 0.316846  [13504/60000]\n",
            "loss: 0.226908  [14144/60000]\n",
            "loss: 0.216069  [14784/60000]\n",
            "loss: 0.148429  [15424/60000]\n",
            "loss: 0.235651  [16064/60000]\n",
            "loss: 0.292514  [16704/60000]\n",
            "loss: 0.126296  [17344/60000]\n",
            "loss: 0.191773  [17984/60000]\n",
            "loss: 0.247948  [18624/60000]\n",
            "loss: 0.115130  [19264/60000]\n",
            "loss: 0.317344  [19904/60000]\n",
            "loss: 0.260560  [20544/60000]\n",
            "loss: 0.278470  [21184/60000]\n",
            "loss: 0.261790  [21824/60000]\n",
            "loss: 0.100595  [22464/60000]\n",
            "loss: 0.199966  [23104/60000]\n",
            "loss: 0.173164  [23744/60000]\n",
            "loss: 0.219129  [24384/60000]\n",
            "loss: 0.129169  [25024/60000]\n",
            "loss: 0.227719  [25664/60000]\n",
            "loss: 0.242428  [26304/60000]\n",
            "loss: 0.340707  [26944/60000]\n",
            "loss: 0.221503  [27584/60000]\n",
            "loss: 0.371519  [28224/60000]\n",
            "loss: 0.215085  [28864/60000]\n",
            "loss: 0.221925  [29504/60000]\n",
            "loss: 0.274379  [30144/60000]\n",
            "loss: 0.185810  [30784/60000]\n",
            "loss: 0.375490  [31424/60000]\n",
            "loss: 0.133920  [32064/60000]\n",
            "loss: 0.307756  [32704/60000]\n",
            "loss: 0.171241  [33344/60000]\n",
            "loss: 0.266517  [33984/60000]\n",
            "loss: 0.220116  [34624/60000]\n",
            "loss: 0.292721  [35264/60000]\n",
            "loss: 0.207145  [35904/60000]\n",
            "loss: 0.205575  [36544/60000]\n",
            "loss: 0.166381  [37184/60000]\n",
            "loss: 0.212060  [37824/60000]\n",
            "loss: 0.167130  [38464/60000]\n",
            "loss: 0.181393  [39104/60000]\n",
            "loss: 0.220319  [39744/60000]\n",
            "loss: 0.184002  [40384/60000]\n",
            "loss: 0.258821  [41024/60000]\n",
            "loss: 0.159505  [41664/60000]\n",
            "loss: 0.358305  [42304/60000]\n",
            "loss: 0.186689  [42944/60000]\n",
            "loss: 0.252176  [43584/60000]\n",
            "loss: 0.374790  [44224/60000]\n",
            "loss: 0.128886  [44864/60000]\n",
            "loss: 0.174962  [45504/60000]\n",
            "loss: 0.284040  [46144/60000]\n",
            "loss: 0.151904  [46784/60000]\n",
            "loss: 0.272646  [47424/60000]\n",
            "loss: 0.141217  [48064/60000]\n",
            "loss: 0.297714  [48704/60000]\n",
            "loss: 0.283445  [49344/60000]\n",
            "loss: 0.379804  [49984/60000]\n",
            "loss: 0.176177  [50624/60000]\n",
            "loss: 0.309630  [51264/60000]\n",
            "loss: 0.357897  [51904/60000]\n",
            "loss: 0.196886  [52544/60000]\n",
            "loss: 0.319740  [53184/60000]\n",
            "loss: 0.347094  [53824/60000]\n",
            "loss: 0.316015  [54464/60000]\n",
            "loss: 0.200147  [55104/60000]\n",
            "loss: 0.230355  [55744/60000]\n",
            "loss: 0.293785  [56384/60000]\n",
            "loss: 0.253898  [57024/60000]\n",
            "loss: 0.188291  [57664/60000]\n",
            "loss: 0.275882  [58304/60000]\n",
            "loss: 0.245743  [58944/60000]\n",
            "loss: 0.314195  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.384162 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.234771  [   64/60000]\n",
            "loss: 0.448005  [  704/60000]\n",
            "loss: 0.313903  [ 1344/60000]\n",
            "loss: 0.263025  [ 1984/60000]\n",
            "loss: 0.284699  [ 2624/60000]\n",
            "loss: 0.203691  [ 3264/60000]\n",
            "loss: 0.298952  [ 3904/60000]\n",
            "loss: 0.260506  [ 4544/60000]\n",
            "loss: 0.220847  [ 5184/60000]\n",
            "loss: 0.374352  [ 5824/60000]\n",
            "loss: 0.351723  [ 6464/60000]\n",
            "loss: 0.190762  [ 7104/60000]\n",
            "loss: 0.174700  [ 7744/60000]\n",
            "loss: 0.201584  [ 8384/60000]\n",
            "loss: 0.190398  [ 9024/60000]\n",
            "loss: 0.127507  [ 9664/60000]\n",
            "loss: 0.275354  [10304/60000]\n",
            "loss: 0.247900  [10944/60000]\n",
            "loss: 0.168984  [11584/60000]\n",
            "loss: 0.384786  [12224/60000]\n",
            "loss: 0.278077  [12864/60000]\n",
            "loss: 0.229727  [13504/60000]\n",
            "loss: 0.238693  [14144/60000]\n",
            "loss: 0.288990  [14784/60000]\n",
            "loss: 0.266643  [15424/60000]\n",
            "loss: 0.233307  [16064/60000]\n",
            "loss: 0.332810  [16704/60000]\n",
            "loss: 0.335269  [17344/60000]\n",
            "loss: 0.141929  [17984/60000]\n",
            "loss: 0.231809  [18624/60000]\n",
            "loss: 0.265897  [19264/60000]\n",
            "loss: 0.268883  [19904/60000]\n",
            "loss: 0.323828  [20544/60000]\n",
            "loss: 0.333810  [21184/60000]\n",
            "loss: 0.417083  [21824/60000]\n",
            "loss: 0.371695  [22464/60000]\n",
            "loss: 0.308819  [23104/60000]\n",
            "loss: 0.168734  [23744/60000]\n",
            "loss: 0.156777  [24384/60000]\n",
            "loss: 0.284060  [25024/60000]\n",
            "loss: 0.205896  [25664/60000]\n",
            "loss: 0.321253  [26304/60000]\n",
            "loss: 0.367341  [26944/60000]\n",
            "loss: 0.229500  [27584/60000]\n",
            "loss: 0.286389  [28224/60000]\n",
            "loss: 0.217308  [28864/60000]\n",
            "loss: 0.237733  [29504/60000]\n",
            "loss: 0.191338  [30144/60000]\n",
            "loss: 0.294221  [30784/60000]\n",
            "loss: 0.229179  [31424/60000]\n",
            "loss: 0.214057  [32064/60000]\n",
            "loss: 0.307908  [32704/60000]\n",
            "loss: 0.167117  [33344/60000]\n",
            "loss: 0.227151  [33984/60000]\n",
            "loss: 0.364261  [34624/60000]\n",
            "loss: 0.324335  [35264/60000]\n",
            "loss: 0.189938  [35904/60000]\n",
            "loss: 0.228722  [36544/60000]\n",
            "loss: 0.287235  [37184/60000]\n",
            "loss: 0.233593  [37824/60000]\n",
            "loss: 0.251566  [38464/60000]\n",
            "loss: 0.210909  [39104/60000]\n",
            "loss: 0.077336  [39744/60000]\n",
            "loss: 0.284725  [40384/60000]\n",
            "loss: 0.157366  [41024/60000]\n",
            "loss: 0.317522  [41664/60000]\n",
            "loss: 0.265869  [42304/60000]\n",
            "loss: 0.279627  [42944/60000]\n",
            "loss: 0.218969  [43584/60000]\n",
            "loss: 0.116932  [44224/60000]\n",
            "loss: 0.205702  [44864/60000]\n",
            "loss: 0.164181  [45504/60000]\n",
            "loss: 0.331771  [46144/60000]\n",
            "loss: 0.261196  [46784/60000]\n",
            "loss: 0.148122  [47424/60000]\n",
            "loss: 0.448439  [48064/60000]\n",
            "loss: 0.264522  [48704/60000]\n",
            "loss: 0.247216  [49344/60000]\n",
            "loss: 0.228253  [49984/60000]\n",
            "loss: 0.251717  [50624/60000]\n",
            "loss: 0.196792  [51264/60000]\n",
            "loss: 0.365727  [51904/60000]\n",
            "loss: 0.237963  [52544/60000]\n",
            "loss: 0.206213  [53184/60000]\n",
            "loss: 0.202161  [53824/60000]\n",
            "loss: 0.323785  [54464/60000]\n",
            "loss: 0.191354  [55104/60000]\n",
            "loss: 0.187237  [55744/60000]\n",
            "loss: 0.224880  [56384/60000]\n",
            "loss: 0.247104  [57024/60000]\n",
            "loss: 0.251170  [57664/60000]\n",
            "loss: 0.248201  [58304/60000]\n",
            "loss: 0.210394  [58944/60000]\n",
            "loss: 0.208325  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.387055 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.213240  [   64/60000]\n",
            "loss: 0.274121  [  704/60000]\n",
            "loss: 0.175400  [ 1344/60000]\n",
            "loss: 0.280022  [ 1984/60000]\n",
            "loss: 0.153980  [ 2624/60000]\n",
            "loss: 0.149335  [ 3264/60000]\n",
            "loss: 0.223089  [ 3904/60000]\n",
            "loss: 0.131695  [ 4544/60000]\n",
            "loss: 0.180128  [ 5184/60000]\n",
            "loss: 0.229547  [ 5824/60000]\n",
            "loss: 0.180013  [ 6464/60000]\n",
            "loss: 0.221433  [ 7104/60000]\n",
            "loss: 0.324403  [ 7744/60000]\n",
            "loss: 0.215069  [ 8384/60000]\n",
            "loss: 0.292798  [ 9024/60000]\n",
            "loss: 0.289484  [ 9664/60000]\n",
            "loss: 0.226646  [10304/60000]\n",
            "loss: 0.291325  [10944/60000]\n",
            "loss: 0.161514  [11584/60000]\n",
            "loss: 0.198312  [12224/60000]\n",
            "loss: 0.134916  [12864/60000]\n",
            "loss: 0.278553  [13504/60000]\n",
            "loss: 0.263988  [14144/60000]\n",
            "loss: 0.216959  [14784/60000]\n",
            "loss: 0.225704  [15424/60000]\n",
            "loss: 0.314836  [16064/60000]\n",
            "loss: 0.176569  [16704/60000]\n",
            "loss: 0.150632  [17344/60000]\n",
            "loss: 0.309649  [17984/60000]\n",
            "loss: 0.427451  [18624/60000]\n",
            "loss: 0.288591  [19264/60000]\n",
            "loss: 0.168056  [19904/60000]\n",
            "loss: 0.291527  [20544/60000]\n",
            "loss: 0.167183  [21184/60000]\n",
            "loss: 0.179154  [21824/60000]\n",
            "loss: 0.165401  [22464/60000]\n",
            "loss: 0.288350  [23104/60000]\n",
            "loss: 0.180150  [23744/60000]\n",
            "loss: 0.230301  [24384/60000]\n",
            "loss: 0.219725  [25024/60000]\n",
            "loss: 0.229786  [25664/60000]\n",
            "loss: 0.224896  [26304/60000]\n",
            "loss: 0.249198  [26944/60000]\n",
            "loss: 0.384019  [27584/60000]\n",
            "loss: 0.397707  [28224/60000]\n",
            "loss: 0.184385  [28864/60000]\n",
            "loss: 0.181971  [29504/60000]\n",
            "loss: 0.240711  [30144/60000]\n",
            "loss: 0.325257  [30784/60000]\n",
            "loss: 0.315699  [31424/60000]\n",
            "loss: 0.127733  [32064/60000]\n",
            "loss: 0.185541  [32704/60000]\n",
            "loss: 0.311242  [33344/60000]\n",
            "loss: 0.194910  [33984/60000]\n",
            "loss: 0.163272  [34624/60000]\n",
            "loss: 0.235014  [35264/60000]\n",
            "loss: 0.239159  [35904/60000]\n",
            "loss: 0.174803  [36544/60000]\n",
            "loss: 0.269663  [37184/60000]\n",
            "loss: 0.166102  [37824/60000]\n",
            "loss: 0.209956  [38464/60000]\n",
            "loss: 0.275661  [39104/60000]\n",
            "loss: 0.216841  [39744/60000]\n",
            "loss: 0.211208  [40384/60000]\n",
            "loss: 0.142769  [41024/60000]\n",
            "loss: 0.248614  [41664/60000]\n",
            "loss: 0.192755  [42304/60000]\n",
            "loss: 0.177494  [42944/60000]\n",
            "loss: 0.258335  [43584/60000]\n",
            "loss: 0.304608  [44224/60000]\n",
            "loss: 0.218594  [44864/60000]\n",
            "loss: 0.181278  [45504/60000]\n",
            "loss: 0.244036  [46144/60000]\n",
            "loss: 0.369426  [46784/60000]\n",
            "loss: 0.396666  [47424/60000]\n",
            "loss: 0.185926  [48064/60000]\n",
            "loss: 0.319127  [48704/60000]\n",
            "loss: 0.258100  [49344/60000]\n",
            "loss: 0.333743  [49984/60000]\n",
            "loss: 0.207374  [50624/60000]\n",
            "loss: 0.215325  [51264/60000]\n",
            "loss: 0.238136  [51904/60000]\n",
            "loss: 0.259589  [52544/60000]\n",
            "loss: 0.194620  [53184/60000]\n",
            "loss: 0.225757  [53824/60000]\n",
            "loss: 0.159933  [54464/60000]\n",
            "loss: 0.196134  [55104/60000]\n",
            "loss: 0.197014  [55744/60000]\n",
            "loss: 0.223077  [56384/60000]\n",
            "loss: 0.170229  [57024/60000]\n",
            "loss: 0.312408  [57664/60000]\n",
            "loss: 0.222659  [58304/60000]\n",
            "loss: 0.477332  [58944/60000]\n",
            "loss: 0.360103  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.354794 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.246894  [   64/60000]\n",
            "loss: 0.164008  [  704/60000]\n",
            "loss: 0.195851  [ 1344/60000]\n",
            "loss: 0.141211  [ 1984/60000]\n",
            "loss: 0.208537  [ 2624/60000]\n",
            "loss: 0.150513  [ 3264/60000]\n",
            "loss: 0.169671  [ 3904/60000]\n",
            "loss: 0.229889  [ 4544/60000]\n",
            "loss: 0.315471  [ 5184/60000]\n",
            "loss: 0.195726  [ 5824/60000]\n",
            "loss: 0.184425  [ 6464/60000]\n",
            "loss: 0.224262  [ 7104/60000]\n",
            "loss: 0.175878  [ 7744/60000]\n",
            "loss: 0.307372  [ 8384/60000]\n",
            "loss: 0.145877  [ 9024/60000]\n",
            "loss: 0.098041  [ 9664/60000]\n",
            "loss: 0.273283  [10304/60000]\n",
            "loss: 0.213322  [10944/60000]\n",
            "loss: 0.181099  [11584/60000]\n",
            "loss: 0.111254  [12224/60000]\n",
            "loss: 0.406812  [12864/60000]\n",
            "loss: 0.206739  [13504/60000]\n",
            "loss: 0.195602  [14144/60000]\n",
            "loss: 0.131468  [14784/60000]\n",
            "loss: 0.327943  [15424/60000]\n",
            "loss: 0.346952  [16064/60000]\n",
            "loss: 0.236006  [16704/60000]\n",
            "loss: 0.145038  [17344/60000]\n",
            "loss: 0.274749  [17984/60000]\n",
            "loss: 0.189058  [18624/60000]\n",
            "loss: 0.140192  [19264/60000]\n",
            "loss: 0.204761  [19904/60000]\n",
            "loss: 0.215561  [20544/60000]\n",
            "loss: 0.163647  [21184/60000]\n",
            "loss: 0.175530  [21824/60000]\n",
            "loss: 0.241857  [22464/60000]\n",
            "loss: 0.336101  [23104/60000]\n",
            "loss: 0.171698  [23744/60000]\n",
            "loss: 0.245028  [24384/60000]\n",
            "loss: 0.139051  [25024/60000]\n",
            "loss: 0.291599  [25664/60000]\n",
            "loss: 0.319646  [26304/60000]\n",
            "loss: 0.261384  [26944/60000]\n",
            "loss: 0.247826  [27584/60000]\n",
            "loss: 0.175380  [28224/60000]\n",
            "loss: 0.142156  [28864/60000]\n",
            "loss: 0.187117  [29504/60000]\n",
            "loss: 0.193192  [30144/60000]\n",
            "loss: 0.178275  [30784/60000]\n",
            "loss: 0.104501  [31424/60000]\n",
            "loss: 0.124437  [32064/60000]\n",
            "loss: 0.240713  [32704/60000]\n",
            "loss: 0.193195  [33344/60000]\n",
            "loss: 0.256790  [33984/60000]\n",
            "loss: 0.188924  [34624/60000]\n",
            "loss: 0.520098  [35264/60000]\n",
            "loss: 0.130125  [35904/60000]\n",
            "loss: 0.180022  [36544/60000]\n",
            "loss: 0.209704  [37184/60000]\n",
            "loss: 0.164313  [37824/60000]\n",
            "loss: 0.172827  [38464/60000]\n",
            "loss: 0.149091  [39104/60000]\n",
            "loss: 0.144021  [39744/60000]\n",
            "loss: 0.226416  [40384/60000]\n",
            "loss: 0.123844  [41024/60000]\n",
            "loss: 0.256304  [41664/60000]\n",
            "loss: 0.344103  [42304/60000]\n",
            "loss: 0.244283  [42944/60000]\n",
            "loss: 0.222529  [43584/60000]\n",
            "loss: 0.134374  [44224/60000]\n",
            "loss: 0.311637  [44864/60000]\n",
            "loss: 0.393701  [45504/60000]\n",
            "loss: 0.220846  [46144/60000]\n",
            "loss: 0.148209  [46784/60000]\n",
            "loss: 0.223173  [47424/60000]\n",
            "loss: 0.266491  [48064/60000]\n",
            "loss: 0.224484  [48704/60000]\n",
            "loss: 0.270665  [49344/60000]\n",
            "loss: 0.278778  [49984/60000]\n",
            "loss: 0.265762  [50624/60000]\n",
            "loss: 0.275817  [51264/60000]\n",
            "loss: 0.303687  [51904/60000]\n",
            "loss: 0.164269  [52544/60000]\n",
            "loss: 0.375368  [53184/60000]\n",
            "loss: 0.174014  [53824/60000]\n",
            "loss: 0.274695  [54464/60000]\n",
            "loss: 0.446468  [55104/60000]\n",
            "loss: 0.381684  [55744/60000]\n",
            "loss: 0.248784  [56384/60000]\n",
            "loss: 0.147862  [57024/60000]\n",
            "loss: 0.212376  [57664/60000]\n",
            "loss: 0.257653  [58304/60000]\n",
            "loss: 0.181349  [58944/60000]\n",
            "loss: 0.428053  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.362376 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.108580  [   64/60000]\n",
            "loss: 0.191510  [  704/60000]\n",
            "loss: 0.209086  [ 1344/60000]\n",
            "loss: 0.222660  [ 1984/60000]\n",
            "loss: 0.241124  [ 2624/60000]\n",
            "loss: 0.154535  [ 3264/60000]\n",
            "loss: 0.265045  [ 3904/60000]\n",
            "loss: 0.213376  [ 4544/60000]\n",
            "loss: 0.259097  [ 5184/60000]\n",
            "loss: 0.352560  [ 5824/60000]\n",
            "loss: 0.193000  [ 6464/60000]\n",
            "loss: 0.096866  [ 7104/60000]\n",
            "loss: 0.111250  [ 7744/60000]\n",
            "loss: 0.286148  [ 8384/60000]\n",
            "loss: 0.228931  [ 9024/60000]\n",
            "loss: 0.269438  [ 9664/60000]\n",
            "loss: 0.218046  [10304/60000]\n",
            "loss: 0.305146  [10944/60000]\n",
            "loss: 0.306028  [11584/60000]\n",
            "loss: 0.334331  [12224/60000]\n",
            "loss: 0.244162  [12864/60000]\n",
            "loss: 0.237690  [13504/60000]\n",
            "loss: 0.253930  [14144/60000]\n",
            "loss: 0.159887  [14784/60000]\n",
            "loss: 0.163653  [15424/60000]\n",
            "loss: 0.337438  [16064/60000]\n",
            "loss: 0.336760  [16704/60000]\n",
            "loss: 0.205472  [17344/60000]\n",
            "loss: 0.340572  [17984/60000]\n",
            "loss: 0.211776  [18624/60000]\n",
            "loss: 0.261825  [19264/60000]\n",
            "loss: 0.188404  [19904/60000]\n",
            "loss: 0.179994  [20544/60000]\n",
            "loss: 0.168490  [21184/60000]\n",
            "loss: 0.135927  [21824/60000]\n",
            "loss: 0.138415  [22464/60000]\n",
            "loss: 0.287591  [23104/60000]\n",
            "loss: 0.081876  [23744/60000]\n",
            "loss: 0.210961  [24384/60000]\n",
            "loss: 0.397176  [25024/60000]\n",
            "loss: 0.096427  [25664/60000]\n",
            "loss: 0.226878  [26304/60000]\n",
            "loss: 0.205692  [26944/60000]\n",
            "loss: 0.324877  [27584/60000]\n",
            "loss: 0.278862  [28224/60000]\n",
            "loss: 0.132490  [28864/60000]\n",
            "loss: 0.278922  [29504/60000]\n",
            "loss: 0.259461  [30144/60000]\n",
            "loss: 0.180509  [30784/60000]\n",
            "loss: 0.305499  [31424/60000]\n",
            "loss: 0.183140  [32064/60000]\n",
            "loss: 0.211821  [32704/60000]\n",
            "loss: 0.307421  [33344/60000]\n",
            "loss: 0.166369  [33984/60000]\n",
            "loss: 0.357322  [34624/60000]\n",
            "loss: 0.200460  [35264/60000]\n",
            "loss: 0.270878  [35904/60000]\n",
            "loss: 0.251694  [36544/60000]\n",
            "loss: 0.267289  [37184/60000]\n",
            "loss: 0.209142  [37824/60000]\n",
            "loss: 0.263542  [38464/60000]\n",
            "loss: 0.186294  [39104/60000]\n",
            "loss: 0.217265  [39744/60000]\n",
            "loss: 0.319040  [40384/60000]\n",
            "loss: 0.356811  [41024/60000]\n",
            "loss: 0.287853  [41664/60000]\n",
            "loss: 0.200360  [42304/60000]\n",
            "loss: 0.231035  [42944/60000]\n",
            "loss: 0.231749  [43584/60000]\n",
            "loss: 0.248975  [44224/60000]\n",
            "loss: 0.228712  [44864/60000]\n",
            "loss: 0.374086  [45504/60000]\n",
            "loss: 0.115340  [46144/60000]\n",
            "loss: 0.222910  [46784/60000]\n",
            "loss: 0.307468  [47424/60000]\n",
            "loss: 0.233479  [48064/60000]\n",
            "loss: 0.210918  [48704/60000]\n",
            "loss: 0.175291  [49344/60000]\n",
            "loss: 0.086508  [49984/60000]\n",
            "loss: 0.349243  [50624/60000]\n",
            "loss: 0.183944  [51264/60000]\n",
            "loss: 0.056438  [51904/60000]\n",
            "loss: 0.285959  [52544/60000]\n",
            "loss: 0.296378  [53184/60000]\n",
            "loss: 0.168537  [53824/60000]\n",
            "loss: 0.216290  [54464/60000]\n",
            "loss: 0.267735  [55104/60000]\n",
            "loss: 0.287646  [55744/60000]\n",
            "loss: 0.290278  [56384/60000]\n",
            "loss: 0.173136  [57024/60000]\n",
            "loss: 0.259222  [57664/60000]\n",
            "loss: 0.157295  [58304/60000]\n",
            "loss: 0.417255  [58944/60000]\n",
            "loss: 0.335014  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.453871 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.221544  [   64/60000]\n",
            "loss: 0.231529  [  704/60000]\n",
            "loss: 0.153769  [ 1344/60000]\n",
            "loss: 0.114594  [ 1984/60000]\n",
            "loss: 0.173662  [ 2624/60000]\n",
            "loss: 0.247680  [ 3264/60000]\n",
            "loss: 0.174171  [ 3904/60000]\n",
            "loss: 0.384112  [ 4544/60000]\n",
            "loss: 0.236337  [ 5184/60000]\n",
            "loss: 0.158182  [ 5824/60000]\n",
            "loss: 0.215377  [ 6464/60000]\n",
            "loss: 0.148962  [ 7104/60000]\n",
            "loss: 0.152226  [ 7744/60000]\n",
            "loss: 0.138473  [ 8384/60000]\n",
            "loss: 0.125856  [ 9024/60000]\n",
            "loss: 0.211574  [ 9664/60000]\n",
            "loss: 0.225680  [10304/60000]\n",
            "loss: 0.228707  [10944/60000]\n",
            "loss: 0.125202  [11584/60000]\n",
            "loss: 0.201420  [12224/60000]\n",
            "loss: 0.335565  [12864/60000]\n",
            "loss: 0.179939  [13504/60000]\n",
            "loss: 0.148237  [14144/60000]\n",
            "loss: 0.205972  [14784/60000]\n",
            "loss: 0.134703  [15424/60000]\n",
            "loss: 0.384257  [16064/60000]\n",
            "loss: 0.254781  [16704/60000]\n",
            "loss: 0.432714  [17344/60000]\n",
            "loss: 0.276375  [17984/60000]\n",
            "loss: 0.263261  [18624/60000]\n",
            "loss: 0.241709  [19264/60000]\n",
            "loss: 0.165931  [19904/60000]\n",
            "loss: 0.460418  [20544/60000]\n",
            "loss: 0.147709  [21184/60000]\n",
            "loss: 0.129344  [21824/60000]\n",
            "loss: 0.134110  [22464/60000]\n",
            "loss: 0.148418  [23104/60000]\n",
            "loss: 0.188382  [23744/60000]\n",
            "loss: 0.120272  [24384/60000]\n",
            "loss: 0.159809  [25024/60000]\n",
            "loss: 0.378902  [25664/60000]\n",
            "loss: 0.152687  [26304/60000]\n",
            "loss: 0.343405  [26944/60000]\n",
            "loss: 0.201602  [27584/60000]\n",
            "loss: 0.230976  [28224/60000]\n",
            "loss: 0.206309  [28864/60000]\n",
            "loss: 0.245636  [29504/60000]\n",
            "loss: 0.252808  [30144/60000]\n",
            "loss: 0.189286  [30784/60000]\n",
            "loss: 0.217599  [31424/60000]\n",
            "loss: 0.306906  [32064/60000]\n",
            "loss: 0.362690  [32704/60000]\n",
            "loss: 0.228971  [33344/60000]\n",
            "loss: 0.216384  [33984/60000]\n",
            "loss: 0.245877  [34624/60000]\n",
            "loss: 0.268836  [35264/60000]\n",
            "loss: 0.143003  [35904/60000]\n",
            "loss: 0.215427  [36544/60000]\n",
            "loss: 0.107511  [37184/60000]\n",
            "loss: 0.119219  [37824/60000]\n",
            "loss: 0.228646  [38464/60000]\n",
            "loss: 0.217465  [39104/60000]\n",
            "loss: 0.225503  [39744/60000]\n",
            "loss: 0.168886  [40384/60000]\n",
            "loss: 0.219774  [41024/60000]\n",
            "loss: 0.154407  [41664/60000]\n",
            "loss: 0.128650  [42304/60000]\n",
            "loss: 0.291988  [42944/60000]\n",
            "loss: 0.131564  [43584/60000]\n",
            "loss: 0.167645  [44224/60000]\n",
            "loss: 0.317570  [44864/60000]\n",
            "loss: 0.255943  [45504/60000]\n",
            "loss: 0.220204  [46144/60000]\n",
            "loss: 0.188646  [46784/60000]\n",
            "loss: 0.266651  [47424/60000]\n",
            "loss: 0.244120  [48064/60000]\n",
            "loss: 0.353645  [48704/60000]\n",
            "loss: 0.237700  [49344/60000]\n",
            "loss: 0.145982  [49984/60000]\n",
            "loss: 0.153768  [50624/60000]\n",
            "loss: 0.196445  [51264/60000]\n",
            "loss: 0.303080  [51904/60000]\n",
            "loss: 0.140126  [52544/60000]\n",
            "loss: 0.156214  [53184/60000]\n",
            "loss: 0.459795  [53824/60000]\n",
            "loss: 0.177218  [54464/60000]\n",
            "loss: 0.221246  [55104/60000]\n",
            "loss: 0.169686  [55744/60000]\n",
            "loss: 0.089098  [56384/60000]\n",
            "loss: 0.247449  [57024/60000]\n",
            "loss: 0.122769  [57664/60000]\n",
            "loss: 0.234833  [58304/60000]\n",
            "loss: 0.194023  [58944/60000]\n",
            "loss: 0.216418  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.415572 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide if we are loading for predictions or more training\n",
        "model.eval()\n",
        "# - or -\n",
        "#model.train()\n",
        "\n",
        "# Make predictions\n",
        "pred = model(test_data.__getitem__(1)[0]).argmax()\n",
        "truth = test_data.__getitem__(1)[1]\n",
        "print(f\"This image is predicted to be a {pred}, and is labeled as {truth}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvaY3pFmkxoP",
        "outputId": "1b6a6561-866e-483f-b0a3-9faab3a8ee42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is predicted to be a 2, and is labeled as 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the evaluation only giving me the predicted image was not enough, I had now clue how good my training data was on the testdata. So I trained it once, which gave me a poor outcome. Than I added the transformers and this improved my prediction on the test data drastically."
      ],
      "metadata": {
        "id": "1r5rMssInQl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading for predictions\n",
        "model.eval()\n",
        "\n",
        "# Initialize counters for correct predictions and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for data, target in test_data:\n",
        "\n",
        "    output = model(data)\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()\n",
        "    total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100. * correct / total\n",
        "\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAwn1Ke_7j3g",
        "outputId": "27283318-15e1-4c9a-863d-90dd36534249"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-0c5b98c59a80>:14: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 85.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially I had a score of 7.92%, but for my machine learning research I found someone that said that angles and especially rotations matter. This will let it 'learn' better to make better predictions for new data it has never seen before. So that was what I did, with my transformers.\n"
      ],
      "metadata": {
        "id": "oSy96CZp9rz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the model\n"
      ],
      "metadata": {
        "id": "soA39MDHmD4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For saving the model I made some chances after I did not manage to save and export my file. Ultimately I think that it was not really necessary but I am able to see more about my data than only some basic information. With the export of the additional data I was better able to tell of my import was correct."
      ],
      "metadata": {
        "id": "g1CHUpCBoMC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our model for later, so we can train more or make predictions\n",
        "EPOCH = epochs\n",
        "\n",
        "PATH = \"model_BF.pt\"\n",
        "\n",
        "state = {\n",
        "    'epoch': EPOCH,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'avarage_loss': {loss_fn},\n",
        "    'validation_accuracy': {accuracy}, #added to see at import if it imports everything\n",
        "}\n",
        "torch.save(state, PATH)\n",
        "from google.colab import files\n",
        "files.download('model_BF.pt')\n"
      ],
      "metadata": {
        "id": "mNdU54dudGZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8a0acc48-d18a-491e-f56e-5383d71a3587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9eadc711-32de-4b5a-8fbd-0b6841af9135\", \"model_BF.pt\", 2315070)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "-GwIBdlSENZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loading the model I made use of the github repository. In my Github the model.pt is placed and in here imported. When using the code of the lecture I had now clue of what was saved. So I changed to see which epoch it uses and what it latest accuracy was. With that I was able to understand what was going on in the file and which information it contained.\n",
        "\n",
        "To load the data, you must run all the code chunks above, from the exception of the chapter 'Training of Data'. This because I placed the model and optimizer in there. I don't want hardcopy it or save it in a different way, as I also always need to load my .idx files because of there size."
      ],
      "metadata": {
        "id": "LRK21AltF7RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "\n",
        "filepath = 'https://github.com/Gilian2002/Assignment-3/raw/main/model_BF-4.pt' # Updated URL to raw file\n",
        "\n",
        "# Download the file from GitHub\n",
        "response = requests.get(filepath)\n",
        "\n",
        "# Save the downloaded file locally\n",
        "local_filepath = 'model_BF.pt'\n",
        "with open(local_filepath, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Now load the model using the local file path\n",
        "state = torch.load(local_filepath, weights_only=False)\n",
        "\n",
        "# Load model, optimizer, and epoch information\n",
        "model.load_state_dict(state['state_dict'])\n",
        "optimizer.load_state_dict(state['optimizer'])\n",
        "start_epoch = state['epoch']\n",
        "last_val_accuracy = state.get('validation_accuracy', None)\n",
        "last_avg_loss = state.get('avarage_loss', None)\n",
        "\n",
        "print(f\"Resuming from epoch {start_epoch}.\")\n",
        "print(f\"Last saved epoch validation accuracy: {last_val_accuracy}\")\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-7x1C-djAa",
        "outputId": "89ccaeb5-645c-412f-cea8-887b5c7f8278"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 20.\n",
            "Last saved epoch validation accuracy: {86.51}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FirstNet(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_model): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=16, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the imported model\n",
        "With the code below, which is the same as a little higher. A test is performed on how the model accuracy is on the testdata.\n",
        "\n",
        "I have done this, because I had troubles on my testdata and kept losing my model somehow. So instead of only seeing what my image does I wanted to now what the % was on the test data. That was the reason why I left the code in so that you could see how my model performs on the test data."
      ],
      "metadata": {
        "id": "wWsBlrXlAL_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# These are the counters for correct predictions and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for data, target in test_data: #load the dataset in an earlier prompt\n",
        "    output = model(data)\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()\n",
        "    total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100. * correct / total\n",
        "\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjQny1m0EqUM",
        "outputId": "377e3bd4-05ea-4bf7-b00f-3b808e1a47c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-f59aebf1a84a>:11: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 86.32%\n"
          ]
        }
      ]
    }
  ]
}