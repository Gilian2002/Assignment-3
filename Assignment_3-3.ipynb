{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework assignment 3**\n",
        "\n",
        "\n",
        "Course:  Business Forecasting\n",
        "\n",
        "Prof:    Dr. White\n",
        "\n",
        "<br>\n",
        "\n",
        "Student: Gilian Koenders\n",
        "\n",
        "Number:  59046858\n",
        "\n",
        "Date:    11-13-2024\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jt2ljHV1JVJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seting up the installation\n",
        "In the code chunks below, the installation is prepared with installing Torchvision and the necessary libraries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E9SdGdIJe9e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlSf_0PmdnN7",
        "outputId": "8a4d4f4d-86c1-4056-d0e8-8fdb30e8f220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For reading data\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# For visualizing\n",
        "import plotly.express as px\n",
        "# For model building\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Import numpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ilWWSMbEd06q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "LfczNPzH_O1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the dataloading I could innitialy not make use of the file becuase I got a whole load of warnings, it said that there was a  chance of malware detected. Worked around it with the school computer, but don't have a fancy way of loading it, I had to place it into Google Colab temporary drive.\n",
        "\n",
        "\n",
        "To make the model more robust, I added some image transformations, such as the random flips and slight rotations, and normalized the data. For the importing I used the IDX fiel format.\n",
        "\n",
        "After the data is loaded I checked it based on a example. Herefore I had to rewrite as it is no longer searching for a 0, but 0=T-shirt. That gave me the confirmation that the data is in the correct shape."
      ],
      "metadata": {
        "id": "P7G6LpDm_Q11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-mnist\n",
        "import struct\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from mnist import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, images_path, labels_path, transform=None):\n",
        "        self.images_path = images_path\n",
        "        self.labels_path = labels_path\n",
        "        self.transform = transform\n",
        "        self.images, self.labels = self.load_idx_files()\n",
        "\n",
        "    def load_idx_files(self):\n",
        "        with open(self.labels_path, 'rb') as lbpath:\n",
        "            magic, num = struct.unpack(\">II\", lbpath.read(8))\n",
        "            labels = torch.tensor(list(lbpath.read()), dtype=torch.long)\n",
        "\n",
        "        with open(self.images_path, 'rb') as imgpath:\n",
        "            magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
        "            images = torch.tensor(list(imgpath.read()), dtype=torch.uint8).view(num, rows, cols)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.fromarray(image.numpy(), mode='L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transforms parameters\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Og4lleKBbhWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d184fe8c-42c0-48fe-fcff-037fe0f01c72"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-mnist in /usr/local/lib/python3.10/dist-packages (0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data manually in the Google Colab drive, the file is too big for me to store on the drive or on github."
      ],
      "metadata": {
        "id": "qWxRih77KQ6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FashionMNIST dataset\n",
        "train_data = FashionMNIST('/content/train-images-idx3-ubyte.idx', '/content/train-labels-idx1-ubyte.idx', transform=transform)\n",
        "test_data = FashionMNIST('/content/t10k-images-idx3-ubyte.idx', '/content/t10k-labels-idx1-ubyte.idx', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "C4Y_CXPsFErw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label names for better understanding\n",
        "label_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "# Check that our data look right when we sample\n",
        "idx=1\n",
        "image, label = train_dataset.__getitem__(idx)\n",
        "\n",
        "# Rescale pixel values to 0-255 and convert to NumPy array\n",
        "image_np = image.squeeze().numpy()  # Remove channel dimension if present\n",
        "image_np = (image_np * 0.5 + 0.5) * 255  # Rescale to 0-255 range\n",
        "image_np = image_np.astype(np.uint8)  # Ensure data type is uint8\n",
        "\n",
        "print(f\"This image is labeled a {label_names[label.item()]}\")\n",
        "px.imshow(image_np, color_continuous_scale=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "iC6CersHpR0d",
        "outputId": "f1907303-94c1-442f-bff6-0714735b3c51"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is labeled a T-shirt/top\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e95907e1-dd59-40e4-800b-9bb3f1aa031d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e95907e1-dd59-40e4-800b-9bb3f1aa031d\")) {                    Plotly.newPlot(                        \"e95907e1-dd59-40e4-800b-9bb3f1aa031d\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0,0,0,0,0,0,44,160,231,254,133,168,87,42,47,53,103,188,40,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,164,224,218,209,201,215,217,255,255,255,255,236,228,216,0,0,0,0,0,0,0,0,0],[0,0,0,0,40,225,200,201,200,200,201,201,202,204,215,200,196,198,203,219,136,48,0,0,0,0,0,0],[0,0,0,0,140,222,203,203,207,235,200,200,198,198,196,201,199,199,198,212,224,222,176,13,0,0,0,0],[0,0,0,0,224,220,206,202,222,113,248,252,247,246,249,245,250,248,238,202,198,200,219,188,0,0,0,0],[0,0,0,37,250,210,214,206,219,0,62,50,44,49,47,73,46,75,101,212,203,203,199,219,50,0,0,0],[0,0,0,95,220,208,214,217,215,0,48,71,68,58,103,0,98,70,0,204,207,204,206,226,116,0,0,0],[0,0,0,156,212,206,212,216,189,0,205,255,251,174,255,139,243,254,0,200,215,203,209,222,200,0,0,0],[0,0,0,208,219,213,222,207,206,87,70,12,11,16,15,59,34,41,0,214,215,210,212,218,247,0,0,0],[0,0,113,193,215,225,212,201,239,252,76,70,78,83,94,89,89,71,51,205,224,215,214,214,226,44,0,0],[0,0,0,0,0,0,165,210,195,203,250,253,252,248,245,248,250,252,198,216,220,211,214,235,164,0,0,0],[0,0,0,0,0,0,107,218,197,212,201,200,200,200,200,197,192,208,193,211,248,237,223,187,106,0,0,0],[0,0,0,0,2,0,91,218,200,212,204,206,205,204,204,206,203,214,192,222,159,53,16,0,0,0,0,0],[0,0,0,0,4,0,77,218,205,212,204,206,205,204,205,206,202,212,195,225,46,0,0,0,0,0,0,0],[0,0,0,0,4,0,74,219,209,205,204,207,205,204,206,205,202,205,199,212,45,0,5,0,0,0,0,0],[0,0,0,0,3,0,72,221,207,199,206,209,206,205,205,206,205,201,198,197,10,0,0,0,0,0,0,0],[0,0,0,0,3,0,75,222,202,198,208,210,207,207,204,207,207,200,198,191,1,0,2,0,0,0,0,0],[0,0,0,0,2,0,80,221,198,200,208,210,207,207,205,208,206,203,197,188,0,0,1,0,0,0,0,0],[0,0,0,2,0,96,221,195,209,206,211,207,207,208,206,209,206,209,198,215,0,0,1,0,0,0,0,0],[0,0,0,1,0,105,217,194,210,210,210,205,211,205,207,208,205,201,212,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,115,213,193,211,209,210,206,213,205,207,209,205,201,204,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,118,210,195,212,208,210,207,215,205,207,208,205,204,202,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,121,207,196,211,207,210,210,215,206,206,208,207,207,204,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,124,207,197,210,207,210,212,212,207,204,208,208,208,204,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,127,206,197,205,207,211,212,213,209,206,208,207,210,198,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,162,224,214,243,201,204,205,207,204,199,201,203,210,198,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,76,130,114,119,240,244,244,244,238,236,234,214,221,172,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,121,125,124,125,137,135,135,130,146,188,0,0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(0, 0, 0)\"],[0.09090909090909091,\"rgb(16, 16, 16)\"],[0.18181818181818182,\"rgb(38, 38, 38)\"],[0.2727272727272727,\"rgb(59, 59, 59)\"],[0.36363636363636365,\"rgb(81, 80, 80)\"],[0.45454545454545453,\"rgb(102, 101, 101)\"],[0.5454545454545454,\"rgb(124, 123, 122)\"],[0.6363636363636364,\"rgb(146, 146, 145)\"],[0.7272727272727273,\"rgb(171, 171, 170)\"],[0.8181818181818182,\"rgb(197, 197, 195)\"],[0.9090909090909091,\"rgb(224, 224, 223)\"],[1.0,\"rgb(254, 254, 253)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e95907e1-dd59-40e4-800b-9bb3f1aa031d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now building the network\n",
        "For the network , the parameters are set below. I added some extra layers, after some online search. This will give the neural network a better understanding, and can then better predict on unseen data.\n",
        "\n",
        "To see if there are parametes print, I added a little line of code for it. This is used as check up if the model works fine. I left it inserted. After this code are the training parameters notated."
      ],
      "metadata": {
        "id": "ghU7f8TRf8eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstNet(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "      super(FirstNet, self).__init__()\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      self.linear_relu_model = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        output = self.linear_relu_model(x)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "1wMfQLuKgTTz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see what the model contains I print the parameters\n",
        "model = FirstNet()\n",
        "print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoSVa-mmgUg4",
        "outputId": "c62ae2d0-d338-460a-c543-5d7edfc80c65"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.0246, -0.0232, -0.0184,  ...,  0.0133, -0.0222,  0.0112],\n",
            "        [ 0.0108, -0.0244, -0.0269,  ..., -0.0026, -0.0044, -0.0095],\n",
            "        [-0.0107,  0.0341, -0.0126,  ...,  0.0011,  0.0285,  0.0297],\n",
            "        ...,\n",
            "        [ 0.0038, -0.0351,  0.0323,  ...,  0.0304, -0.0012,  0.0111],\n",
            "        [ 0.0125, -0.0251, -0.0129,  ..., -0.0140, -0.0312, -0.0101],\n",
            "        [ 0.0170, -0.0185,  0.0081,  ...,  0.0192, -0.0082,  0.0149]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-1.7902e-02, -4.1298e-03,  8.0646e-03,  5.0209e-04, -2.5547e-02,\n",
            "        -2.9838e-02,  1.7878e-02, -3.3107e-02, -6.2957e-04, -9.5603e-03,\n",
            "        -2.4075e-02,  1.3017e-02, -3.2575e-02,  4.6848e-03,  2.2747e-02,\n",
            "        -8.5725e-03,  3.4180e-02,  1.2396e-03,  1.8773e-02, -9.2093e-03,\n",
            "        -2.7021e-02,  4.1580e-03,  9.2639e-03, -3.1691e-02, -9.8415e-03,\n",
            "        -1.6930e-02, -1.5769e-02, -2.0876e-02, -3.2203e-02, -1.9554e-02,\n",
            "         3.5290e-02,  4.5764e-03,  3.0999e-02,  3.2096e-02, -2.3103e-02,\n",
            "        -1.3442e-02, -1.5503e-02,  1.7580e-02,  5.1495e-03, -1.8833e-02,\n",
            "         1.1425e-02, -1.7045e-02, -3.0848e-02, -2.9293e-02,  2.4842e-02,\n",
            "         1.3828e-02, -2.9604e-02, -3.1517e-02, -2.7314e-02,  5.1800e-03,\n",
            "         3.0776e-02,  2.9057e-02,  1.5959e-02,  3.5044e-02,  1.6446e-02,\n",
            "         8.4229e-03,  3.0549e-02,  1.4670e-02,  1.9787e-02,  4.9586e-03,\n",
            "         2.5002e-02, -3.4232e-02, -9.9033e-03, -2.4508e-02,  8.0693e-03,\n",
            "        -3.0525e-02,  2.0919e-02, -1.9286e-02,  3.5507e-02, -1.8686e-02,\n",
            "         1.2976e-02, -2.3617e-02,  2.1202e-02,  3.7249e-03, -2.7426e-02,\n",
            "        -1.4783e-02,  1.9692e-02, -1.2416e-02,  1.4253e-02,  3.0523e-02,\n",
            "         2.9530e-02,  6.9420e-03, -2.4697e-02, -3.5257e-02, -3.5121e-02,\n",
            "        -2.5692e-02, -4.0659e-03,  1.2545e-02, -2.5082e-02,  2.0105e-02,\n",
            "        -3.3199e-02, -2.1954e-02,  1.7407e-02,  2.9413e-02,  7.8201e-03,\n",
            "         7.0807e-03,  3.1511e-02,  3.4189e-02, -3.2486e-02, -2.1433e-02,\n",
            "        -2.6363e-02,  1.6626e-03,  4.6357e-03, -2.0119e-02,  1.4848e-02,\n",
            "         3.0535e-02, -1.0688e-02,  1.1365e-03, -2.1195e-02,  3.2428e-02,\n",
            "         3.1386e-02,  9.3689e-03,  1.1723e-02, -7.3002e-03,  3.4854e-02,\n",
            "        -2.5207e-02, -4.3064e-04,  2.7201e-02,  3.2028e-02, -2.7620e-02,\n",
            "         1.1672e-02,  1.0043e-02,  2.0220e-02, -7.5603e-03,  3.5088e-02,\n",
            "         4.4850e-03, -2.1052e-02,  2.4297e-02, -1.9241e-02,  3.2603e-02,\n",
            "         2.8213e-02, -2.1730e-02, -3.5253e-02,  1.8799e-02, -5.6194e-03,\n",
            "        -7.6433e-03, -2.9948e-02,  1.6914e-02, -6.9307e-03, -9.1155e-03,\n",
            "         1.0677e-02, -2.6931e-02, -1.3280e-02, -1.7798e-02,  1.5284e-02,\n",
            "         3.0959e-02, -2.7029e-02,  3.4039e-02,  2.7813e-04, -1.7254e-02,\n",
            "        -5.0746e-03, -2.8265e-02,  1.1407e-02, -3.2724e-02,  2.0356e-02,\n",
            "        -1.9392e-02, -1.4200e-02,  2.9946e-03, -2.9582e-02, -2.9917e-02,\n",
            "        -2.1349e-02,  2.8468e-02, -1.7518e-02, -1.5874e-02, -1.4577e-02,\n",
            "        -9.3164e-03, -4.1762e-04, -1.5033e-02,  3.1595e-03, -2.7805e-03,\n",
            "         3.7518e-03, -1.8757e-02, -3.9410e-03, -3.3279e-02, -2.8730e-03,\n",
            "        -3.1672e-02,  2.2030e-02, -2.0394e-02, -2.6051e-04,  1.4194e-02,\n",
            "        -3.3150e-03, -7.9873e-03, -1.2681e-02, -1.8852e-02,  6.9302e-03,\n",
            "        -2.1918e-02,  2.1516e-03,  6.8998e-03,  2.9396e-03, -2.8706e-03,\n",
            "         9.0469e-03, -2.3115e-02, -2.1070e-02, -2.1084e-02,  1.5925e-02,\n",
            "         3.4471e-04, -1.5733e-02, -2.6833e-02,  2.7622e-02, -3.2826e-02,\n",
            "         3.1051e-03,  1.3048e-03,  1.4325e-02,  3.5563e-02, -6.2712e-03,\n",
            "        -2.8446e-02, -1.0202e-02, -1.5071e-02,  1.0463e-02,  3.1458e-02,\n",
            "        -1.3789e-02, -1.3046e-03, -2.7024e-02,  1.8118e-02,  2.6420e-02,\n",
            "         9.2713e-03, -7.8430e-03,  3.3648e-02, -1.7792e-02,  1.2125e-03,\n",
            "        -1.9665e-02,  3.4209e-03, -2.3915e-02,  9.4314e-03, -7.5190e-03,\n",
            "        -2.2469e-02,  7.4874e-03,  1.4573e-02,  1.7534e-02,  3.4458e-02,\n",
            "         6.6481e-03, -2.0337e-02,  1.9953e-02, -1.9743e-02, -3.4192e-02,\n",
            "        -3.2902e-02,  3.2261e-02,  1.1316e-02, -2.0652e-02, -1.4577e-02,\n",
            "        -2.2639e-02, -1.5754e-02, -2.5107e-02, -3.4291e-02, -3.3913e-02,\n",
            "         4.0174e-03,  3.2444e-02,  1.7324e-02, -2.8174e-02,  2.0910e-02,\n",
            "         9.8851e-03, -5.8048e-03, -3.0584e-02, -1.3204e-02, -2.1492e-02,\n",
            "        -2.1539e-02,  1.4562e-02, -2.2882e-03,  4.2795e-03,  2.8214e-02,\n",
            "        -7.6479e-03,  3.1448e-02, -1.1664e-02, -3.4456e-02,  2.1025e-02,\n",
            "         2.9714e-02, -1.2719e-02, -2.9430e-02,  8.1471e-03,  1.8645e-02,\n",
            "        -3.0126e-02, -3.5581e-02, -1.0381e-02, -3.5385e-02,  3.2475e-02,\n",
            "         2.9554e-02, -5.5083e-05, -2.8578e-02, -5.4295e-03, -3.5246e-02,\n",
            "        -2.4635e-02,  2.9739e-02,  7.9662e-03, -3.3896e-02, -9.1359e-03,\n",
            "        -3.5702e-02,  1.3551e-02, -2.0907e-02, -1.4212e-02,  1.2348e-02,\n",
            "        -3.4440e-02, -2.2249e-02, -3.1167e-02,  7.7424e-03, -3.4834e-02,\n",
            "        -3.1069e-02, -2.6247e-02, -2.2801e-02, -3.2938e-02,  2.4457e-02,\n",
            "        -2.6542e-02,  8.1561e-03, -2.9725e-02, -1.8541e-02,  8.4283e-03,\n",
            "        -2.9612e-02, -3.2723e-02, -2.4817e-03,  2.9411e-02,  3.3600e-02,\n",
            "         2.0201e-02, -1.0211e-02,  4.8425e-03, -2.6011e-02,  1.7687e-02,\n",
            "         2.7063e-02,  3.3084e-02,  2.0414e-02, -1.3862e-02, -1.2495e-02,\n",
            "         2.6942e-03, -1.4188e-02, -9.1822e-03, -2.7388e-02, -2.1069e-02,\n",
            "        -2.7595e-02, -1.5184e-02, -1.2113e-02,  1.4939e-02,  9.2649e-03,\n",
            "         3.4269e-02, -3.0969e-02, -8.7564e-03,  3.0016e-03, -3.2018e-02,\n",
            "        -1.7256e-02,  1.7557e-03,  3.5230e-02,  2.4212e-02, -1.2615e-02,\n",
            "        -1.9721e-02, -2.9491e-02, -6.8228e-03, -2.4554e-02, -2.6781e-02,\n",
            "        -3.0900e-02,  2.3309e-02,  2.5938e-02, -2.4493e-02, -7.1400e-03,\n",
            "        -3.2513e-02,  1.7339e-03,  1.2248e-02,  3.2420e-02,  1.7319e-02,\n",
            "        -2.1153e-02, -2.4583e-02,  1.2336e-02,  1.4460e-02,  1.3969e-02,\n",
            "         1.2145e-02, -3.1348e-02,  3.1682e-02, -1.4427e-02, -2.9831e-02,\n",
            "         1.9499e-02,  2.0638e-02,  2.9100e-02, -7.1001e-03,  2.7538e-02,\n",
            "         4.3237e-03,  6.2032e-03, -2.2993e-02,  3.4001e-02, -2.9389e-02,\n",
            "         1.0638e-03,  1.1480e-02, -3.0782e-02,  3.0444e-02, -1.8975e-02,\n",
            "         1.8475e-03,  3.1179e-02,  2.2467e-02,  1.2376e-02, -3.2288e-03,\n",
            "        -1.5574e-02,  2.7253e-02,  1.7545e-02,  3.2806e-02,  4.1286e-03,\n",
            "         2.6717e-02, -3.3621e-02,  2.1143e-02, -5.4380e-03,  3.3270e-02,\n",
            "        -3.0209e-02,  1.5064e-02, -2.4974e-02, -3.1183e-02, -5.0645e-03,\n",
            "         2.5871e-02,  2.9737e-03,  9.9007e-03, -2.1598e-03, -1.1009e-02,\n",
            "        -2.4042e-02, -9.1176e-03, -9.7934e-03, -2.4839e-02,  1.2387e-03,\n",
            "        -3.1712e-02, -6.0668e-03,  9.0654e-03, -2.0169e-02, -9.7671e-03,\n",
            "        -2.1326e-02, -5.8120e-03, -6.4617e-03,  2.9495e-02,  2.9041e-02,\n",
            "        -3.1935e-02,  2.2856e-02,  3.3589e-02, -2.6087e-02,  2.9708e-02,\n",
            "        -3.2280e-03, -1.1606e-02, -1.9095e-02,  2.6528e-02, -3.3025e-03,\n",
            "         2.4084e-04,  5.4697e-03,  3.0637e-02, -1.0283e-02, -1.2657e-02,\n",
            "         8.1755e-03, -5.2064e-03, -3.1370e-02,  4.0665e-03, -7.1143e-03,\n",
            "        -2.0394e-02, -3.8057e-03, -6.8636e-03,  2.7430e-02,  2.5896e-03,\n",
            "         2.6904e-02,  1.6711e-02, -2.7499e-02, -1.6780e-02,  1.3834e-02,\n",
            "         2.2144e-02,  1.8421e-02,  9.3160e-03,  3.3147e-02,  8.7660e-03,\n",
            "         6.5955e-03,  2.9460e-02, -3.2538e-02,  3.8521e-03,  3.3900e-02,\n",
            "         1.9187e-02,  1.9700e-02,  2.5202e-02, -1.3273e-02,  3.5256e-02,\n",
            "        -1.5186e-02, -2.2830e-02,  1.0637e-03, -1.6868e-02,  2.1526e-02,\n",
            "        -3.4326e-02, -2.4062e-02, -6.9832e-03, -3.4368e-02,  6.1074e-03,\n",
            "        -1.7920e-03,  3.0814e-02,  2.3904e-02,  1.8902e-02, -8.0936e-03,\n",
            "         1.5755e-02, -1.0705e-02, -7.8916e-03, -9.8174e-03,  3.0258e-02,\n",
            "        -2.3640e-02,  1.9391e-02, -1.3836e-02,  1.0420e-02,  2.5627e-02,\n",
            "        -1.7077e-02, -2.2765e-02,  1.3054e-02,  2.3157e-03,  4.0440e-04,\n",
            "         1.4999e-02,  7.6721e-03,  2.6456e-02, -1.4708e-02,  3.2998e-02,\n",
            "         1.6970e-02, -2.0604e-02,  9.7321e-03, -1.7898e-02, -6.9463e-04,\n",
            "         3.1503e-02, -2.5776e-02, -2.2427e-02, -2.0124e-02,  2.6406e-02,\n",
            "         2.8123e-02,  2.6903e-02], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0029, -0.0340,  0.0240,  ...,  0.0109, -0.0024,  0.0030],\n",
            "        [-0.0005,  0.0437, -0.0390,  ..., -0.0423, -0.0323,  0.0095],\n",
            "        [ 0.0121, -0.0281,  0.0102,  ..., -0.0178,  0.0197,  0.0119],\n",
            "        ...,\n",
            "        [-0.0283,  0.0279,  0.0218,  ...,  0.0052,  0.0355,  0.0321],\n",
            "        [ 0.0322,  0.0311, -0.0183,  ...,  0.0289,  0.0108,  0.0103],\n",
            "        [-0.0259,  0.0337,  0.0035,  ..., -0.0213,  0.0343,  0.0299]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0211,  0.0390,  0.0145, -0.0265,  0.0343, -0.0376, -0.0251, -0.0400,\n",
            "         0.0065, -0.0280, -0.0277,  0.0031,  0.0117,  0.0086, -0.0417, -0.0258,\n",
            "        -0.0428, -0.0244, -0.0343,  0.0020,  0.0369,  0.0132, -0.0028, -0.0214,\n",
            "         0.0232,  0.0031, -0.0085,  0.0301, -0.0042,  0.0072, -0.0432,  0.0302,\n",
            "         0.0234, -0.0368, -0.0005, -0.0314, -0.0090,  0.0152,  0.0203, -0.0356,\n",
            "         0.0239,  0.0360, -0.0243, -0.0171, -0.0244, -0.0194, -0.0180, -0.0171,\n",
            "         0.0248,  0.0020, -0.0345,  0.0164,  0.0341, -0.0407,  0.0178,  0.0216,\n",
            "        -0.0199, -0.0288,  0.0074,  0.0021,  0.0171,  0.0184, -0.0067,  0.0373,\n",
            "         0.0359,  0.0134, -0.0243,  0.0109,  0.0061,  0.0232,  0.0036,  0.0316,\n",
            "         0.0040,  0.0063, -0.0197,  0.0273,  0.0384, -0.0435,  0.0078,  0.0273,\n",
            "        -0.0259, -0.0357, -0.0197,  0.0122,  0.0389, -0.0249, -0.0373, -0.0385,\n",
            "         0.0380,  0.0426,  0.0083,  0.0064,  0.0054, -0.0067,  0.0238,  0.0364,\n",
            "        -0.0125, -0.0355,  0.0012,  0.0326, -0.0323,  0.0378, -0.0220,  0.0389,\n",
            "        -0.0073, -0.0392, -0.0328,  0.0014, -0.0398,  0.0006,  0.0228,  0.0188,\n",
            "        -0.0426, -0.0387, -0.0109,  0.0384,  0.0300,  0.0198, -0.0437,  0.0172,\n",
            "        -0.0281,  0.0294,  0.0087, -0.0285,  0.0024,  0.0418,  0.0435,  0.0377,\n",
            "         0.0101, -0.0123, -0.0104, -0.0286,  0.0110, -0.0229,  0.0185, -0.0143,\n",
            "         0.0354,  0.0038, -0.0442, -0.0069,  0.0226, -0.0348, -0.0179, -0.0022,\n",
            "        -0.0175, -0.0323,  0.0053,  0.0073, -0.0361,  0.0043, -0.0189, -0.0300,\n",
            "        -0.0371, -0.0267, -0.0089,  0.0207,  0.0361, -0.0237,  0.0184, -0.0327,\n",
            "         0.0308,  0.0221,  0.0048,  0.0077,  0.0303, -0.0014,  0.0345, -0.0287,\n",
            "         0.0052,  0.0436,  0.0305,  0.0072,  0.0006, -0.0136,  0.0148,  0.0353,\n",
            "         0.0094,  0.0135,  0.0210,  0.0251,  0.0309,  0.0199,  0.0066,  0.0328,\n",
            "        -0.0275,  0.0378, -0.0256,  0.0328, -0.0050, -0.0134, -0.0204, -0.0140,\n",
            "         0.0294, -0.0004,  0.0330,  0.0123,  0.0144,  0.0060,  0.0378,  0.0094,\n",
            "        -0.0089,  0.0274,  0.0327,  0.0010, -0.0019, -0.0268, -0.0024, -0.0197,\n",
            "        -0.0126,  0.0131,  0.0086, -0.0277, -0.0086,  0.0412,  0.0144, -0.0322,\n",
            "        -0.0230,  0.0331,  0.0064, -0.0297,  0.0046, -0.0105,  0.0306,  0.0331,\n",
            "        -0.0087,  0.0162,  0.0255,  0.0086,  0.0239,  0.0157, -0.0247,  0.0424,\n",
            "         0.0166,  0.0129,  0.0042, -0.0381,  0.0114, -0.0040, -0.0197,  0.0086,\n",
            "        -0.0056, -0.0317,  0.0149,  0.0338,  0.0412, -0.0281,  0.0071,  0.0111,\n",
            "         0.0182,  0.0115,  0.0334,  0.0351,  0.0318,  0.0159, -0.0295, -0.0441],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0006, -0.0233,  0.0617,  ...,  0.0504, -0.0338, -0.0297],\n",
            "        [ 0.0450, -0.0504, -0.0099,  ...,  0.0421,  0.0047,  0.0413],\n",
            "        [-0.0001,  0.0398,  0.0429,  ..., -0.0229,  0.0093, -0.0507],\n",
            "        ...,\n",
            "        [-0.0438, -0.0249,  0.0321,  ..., -0.0588, -0.0561, -0.0602],\n",
            "        [ 0.0066, -0.0228,  0.0091,  ..., -0.0623, -0.0216,  0.0063],\n",
            "        [-0.0159, -0.0164, -0.0470,  ...,  0.0041, -0.0612,  0.0049]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 2.6288e-02,  4.2925e-02, -3.9368e-02, -6.2008e-02,  9.0031e-04,\n",
            "        -4.9057e-02,  5.3340e-03, -1.0683e-02,  5.0337e-02,  1.6214e-02,\n",
            "        -1.9421e-02,  3.6229e-02, -1.7937e-02,  1.4232e-02,  1.0567e-02,\n",
            "        -4.8815e-03, -5.2617e-02, -2.3618e-03,  7.7937e-03,  1.7326e-02,\n",
            "        -5.5712e-02, -2.2509e-02, -1.0714e-03, -1.3749e-02,  7.5698e-05,\n",
            "         4.4377e-02, -2.4157e-02, -1.5830e-02,  5.5626e-03,  3.7686e-02,\n",
            "         1.0519e-02,  3.5002e-02,  4.0663e-02,  4.3375e-02,  4.1936e-02,\n",
            "         3.5162e-02, -7.0866e-03, -3.0268e-02, -5.1682e-02, -3.0298e-02,\n",
            "         3.6442e-03, -4.6306e-02,  2.3430e-02, -1.8063e-02, -5.1950e-02,\n",
            "         3.8783e-02, -9.6641e-03, -5.2188e-02, -7.4667e-03, -5.1292e-02,\n",
            "        -5.0691e-02, -6.0042e-02,  3.1621e-02, -4.0979e-02,  3.7340e-02,\n",
            "        -4.5696e-02,  4.2953e-02, -4.2355e-02,  1.3935e-02, -5.6942e-02,\n",
            "        -2.6760e-02,  6.3695e-03,  1.4282e-02, -4.4879e-02,  3.7044e-05,\n",
            "         9.4831e-03, -3.3709e-02,  9.2122e-03, -4.1967e-02, -4.4554e-02,\n",
            "        -6.1450e-02,  3.0798e-02, -4.2142e-02,  2.6761e-02, -2.0836e-02,\n",
            "         4.4738e-02, -6.1472e-02,  1.0400e-03,  5.6965e-02, -3.9599e-02,\n",
            "         4.8058e-02, -4.4821e-02,  4.9217e-02, -1.5772e-02,  2.7290e-02,\n",
            "        -2.0557e-02, -1.1631e-02, -3.7352e-02, -7.9983e-03, -2.3729e-03,\n",
            "        -4.5794e-02, -4.3470e-02, -1.7990e-02,  3.3607e-02,  5.9504e-02,\n",
            "        -2.0879e-02, -1.3027e-02,  2.3261e-02,  1.5222e-02, -2.9543e-02,\n",
            "        -3.5289e-02,  4.1208e-03,  5.0617e-02,  3.0439e-03,  2.5520e-02,\n",
            "         2.0661e-02,  5.0681e-02, -6.2092e-02,  8.3503e-04,  4.9236e-02,\n",
            "        -4.8687e-03, -2.4015e-02,  1.7061e-02,  3.4254e-02,  2.4812e-02,\n",
            "        -1.7740e-02,  1.6748e-02, -6.8821e-03, -2.7661e-02,  4.2136e-03,\n",
            "         4.1373e-02,  2.7080e-02,  5.0051e-02,  5.8824e-02,  4.2866e-03,\n",
            "        -2.6188e-02, -3.1820e-02, -2.8350e-02], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0154,  0.0728,  0.0296,  ...,  0.0599,  0.0069,  0.0446],\n",
            "        [-0.0875, -0.0043,  0.0683,  ..., -0.0239, -0.0089, -0.0488],\n",
            "        [-0.0455, -0.0301, -0.0577,  ...,  0.0467,  0.0751,  0.0627],\n",
            "        ...,\n",
            "        [ 0.0252,  0.0335,  0.0423,  ..., -0.0750, -0.0201, -0.0496],\n",
            "        [-0.0820,  0.0530,  0.0656,  ..., -0.0055,  0.0176, -0.0055],\n",
            "        [-0.0145, -0.0225, -0.0862,  ...,  0.0833, -0.0319, -0.0148]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0699,  0.0491, -0.0350,  0.0529,  0.0485, -0.0511,  0.0470,  0.0365,\n",
            "        -0.0434,  0.0853,  0.0470, -0.0276,  0.0622, -0.0475, -0.0045,  0.0079,\n",
            "         0.0097, -0.0101, -0.0753, -0.0413,  0.0178, -0.0826, -0.0795,  0.0028,\n",
            "        -0.0706,  0.0877,  0.0047, -0.0818, -0.0091,  0.0811,  0.0714, -0.0121,\n",
            "         0.0001,  0.0009,  0.0134,  0.0704,  0.0339, -0.0526,  0.0521, -0.0066,\n",
            "        -0.0664, -0.0698,  0.0771, -0.0235,  0.0834, -0.0459, -0.0672,  0.0183,\n",
            "         0.0412,  0.0259, -0.0862,  0.0138, -0.0248,  0.0370,  0.0580, -0.0881,\n",
            "         0.0762, -0.0598,  0.0252,  0.0707, -0.0632, -0.0635, -0.0015,  0.0419],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0563,  0.1020, -0.0811,  ..., -0.0477,  0.0370,  0.0285],\n",
            "        [-0.0898,  0.0838,  0.1014,  ...,  0.0812, -0.0154,  0.0278],\n",
            "        [-0.0409,  0.0879, -0.0255,  ..., -0.0611, -0.1227,  0.0596],\n",
            "        ...,\n",
            "        [ 0.0916, -0.0068,  0.0083,  ..., -0.1106,  0.0032,  0.0433],\n",
            "        [ 0.0422,  0.1153, -0.0459,  ...,  0.0578, -0.0222, -0.0917],\n",
            "        [-0.0406, -0.0498, -0.0288,  ..., -0.0940, -0.0709,  0.1104]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0964,  0.0290,  0.0558, -0.0481, -0.1055, -0.0842, -0.1153,  0.0368,\n",
            "         0.0977, -0.0993,  0.0717,  0.0379,  0.0700, -0.1108,  0.0418, -0.1154,\n",
            "         0.1177, -0.0815,  0.0857, -0.0322,  0.0723, -0.0651, -0.0692, -0.0554,\n",
            "        -0.0299, -0.0113, -0.0338, -0.1137,  0.0765, -0.0377, -0.0623,  0.0707],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-7.8258e-04, -5.6233e-02,  1.5331e-01,  6.9821e-02,  1.3311e-01,\n",
            "         -2.8353e-02, -3.6059e-02, -1.6239e-01,  1.3005e-01, -9.8630e-02,\n",
            "          1.7574e-01, -1.2432e-01, -1.4835e-01, -1.2707e-01, -1.2332e-01,\n",
            "          6.3583e-02, -1.5089e-03,  4.6254e-03, -9.6416e-02,  1.4064e-01,\n",
            "          1.6971e-01, -1.6551e-01, -1.6940e-01, -8.2942e-02,  7.3289e-02,\n",
            "          1.6683e-01,  1.4514e-01, -2.1075e-02, -1.3212e-01,  4.9189e-02,\n",
            "          1.5647e-01, -9.2486e-02],\n",
            "        [ 1.2947e-01, -6.4808e-02, -1.2138e-01,  1.2693e-01, -1.8376e-02,\n",
            "         -6.9971e-02, -1.4621e-01,  4.0496e-02,  1.3174e-01,  5.2608e-02,\n",
            "         -4.9378e-02,  1.6444e-01,  4.9287e-03, -1.2368e-01,  1.3609e-01,\n",
            "         -8.8192e-05, -3.6323e-02,  1.5048e-01,  1.6829e-01,  1.1858e-01,\n",
            "          8.0835e-02, -3.1625e-02, -7.9907e-02, -6.9712e-02,  1.1955e-01,\n",
            "         -1.4501e-01, -6.2009e-02,  6.2146e-02,  9.2263e-02,  1.9710e-02,\n",
            "          2.6217e-02,  1.5138e-01],\n",
            "        [ 7.2981e-02,  1.0436e-01,  1.2630e-01,  1.3722e-02,  1.7094e-01,\n",
            "          1.7389e-01,  7.4635e-03, -1.5575e-03, -1.4481e-01, -8.2371e-03,\n",
            "          8.6647e-02,  1.7347e-01,  1.6740e-01, -1.1910e-01, -1.1033e-01,\n",
            "          1.0602e-01, -2.5212e-02, -4.6605e-02,  1.6746e-01,  7.9295e-02,\n",
            "         -6.6141e-02, -4.7510e-02,  1.1081e-01,  1.2849e-01, -6.2555e-02,\n",
            "         -1.4786e-01, -1.5254e-01,  7.6613e-02, -1.1757e-01,  5.8261e-03,\n",
            "         -1.6332e-02,  7.1695e-02],\n",
            "        [ 8.8586e-02, -1.8473e-02, -1.5856e-01,  1.2523e-01,  9.9250e-02,\n",
            "         -1.4116e-01, -1.2286e-01, -9.4841e-03, -1.2975e-02, -1.4647e-01,\n",
            "          7.3621e-03, -1.3006e-01, -1.2156e-01,  1.6761e-01, -1.6338e-01,\n",
            "         -1.1607e-01,  1.1719e-02, -4.6969e-02,  1.5301e-01, -9.4057e-02,\n",
            "          4.9072e-02,  1.1649e-01, -6.2208e-02,  3.0423e-02,  1.0142e-01,\n",
            "         -9.9261e-02,  3.8319e-02, -9.8750e-02, -4.9495e-02, -8.3842e-02,\n",
            "         -1.4830e-01,  1.2751e-01],\n",
            "        [-1.6746e-01,  9.5258e-02, -1.4144e-01, -5.0052e-02,  1.3006e-01,\n",
            "         -1.3143e-01,  2.6722e-02, -1.1989e-01,  1.6838e-01,  1.6038e-01,\n",
            "          1.6012e-01, -1.6791e-01,  1.1305e-02, -1.6319e-01, -2.8579e-02,\n",
            "         -1.0785e-01,  1.6254e-01,  2.7073e-02,  1.2600e-01, -1.3294e-01,\n",
            "         -9.8171e-02,  7.4567e-02, -8.9888e-02, -3.8318e-03,  1.2176e-01,\n",
            "          1.1380e-02,  1.2044e-01,  1.3966e-01,  3.6923e-02,  7.1960e-02,\n",
            "          6.8505e-02,  2.1558e-02],\n",
            "        [ 1.0026e-01,  1.4953e-01,  1.3218e-02,  1.4684e-01, -4.9699e-02,\n",
            "          1.6668e-01,  1.3591e-01, -3.4934e-02,  1.0933e-01, -1.1897e-01,\n",
            "          3.3555e-02, -1.1035e-01,  1.6160e-01,  4.1120e-02,  5.5463e-02,\n",
            "         -1.6968e-02, -1.7585e-01, -6.8768e-02, -5.4602e-02, -8.8438e-02,\n",
            "          6.6240e-02,  1.6902e-01, -1.2262e-01, -1.4724e-02, -1.2058e-01,\n",
            "          8.1541e-02,  1.6415e-01,  1.7353e-01,  1.0989e-01,  1.9427e-03,\n",
            "          9.6585e-02, -1.4193e-01],\n",
            "        [ 8.8204e-02, -3.0058e-02,  4.6983e-02,  1.2544e-01,  3.9500e-02,\n",
            "         -1.1782e-01, -7.0555e-02,  1.5476e-01, -1.4642e-01, -2.5205e-02,\n",
            "          5.7118e-02, -7.7812e-02, -8.0476e-02,  7.7506e-02, -2.1266e-02,\n",
            "         -1.5634e-01,  1.3177e-01,  9.3462e-02,  1.1829e-01,  1.3752e-01,\n",
            "         -9.9460e-02,  7.1741e-02, -1.3577e-02, -5.7962e-02,  2.1109e-02,\n",
            "          7.4854e-02, -1.6459e-01, -2.6735e-03, -9.8672e-02, -1.1477e-01,\n",
            "          1.7330e-01, -1.6001e-01],\n",
            "        [-7.4596e-04, -1.5932e-01, -1.2573e-01,  7.7205e-02, -1.1530e-01,\n",
            "          6.7642e-02, -1.4748e-01,  1.6707e-01, -3.4474e-02, -6.3037e-02,\n",
            "          1.0096e-01, -1.4609e-01, -7.7337e-02, -5.9772e-02, -1.4952e-01,\n",
            "         -2.5740e-02, -1.1426e-01,  1.5244e-01, -7.8885e-02, -5.6505e-03,\n",
            "         -1.3770e-01, -9.0487e-02,  1.4567e-01, -1.0918e-01,  6.0080e-02,\n",
            "          1.5984e-01,  4.6239e-02,  4.9268e-02,  4.0594e-02,  3.9973e-02,\n",
            "         -2.4185e-02,  1.5645e-01],\n",
            "        [ 4.7633e-02, -6.2278e-02, -1.2371e-01, -5.8948e-02,  2.2005e-02,\n",
            "         -1.4564e-01, -3.4838e-02, -1.1077e-01, -1.7276e-01, -4.3015e-02,\n",
            "          1.0711e-01, -1.4656e-01, -6.7716e-02, -1.5631e-01,  6.5404e-02,\n",
            "          1.0244e-01, -1.4914e-01, -1.6042e-01, -1.3852e-01,  7.1873e-03,\n",
            "         -1.7227e-02,  1.9483e-02, -6.0344e-02, -8.4314e-02,  1.3704e-01,\n",
            "          5.0309e-02,  1.0771e-01, -1.5189e-01, -1.6972e-01, -1.3564e-01,\n",
            "         -1.6903e-01, -8.8994e-02],\n",
            "        [ 1.0799e-01,  1.6172e-01,  4.7413e-02,  3.2826e-02,  2.7288e-02,\n",
            "         -1.2391e-01, -5.2831e-02, -5.9323e-02,  4.4497e-02,  1.5596e-01,\n",
            "          1.7208e-01, -1.0871e-03,  1.6219e-01,  3.1893e-04, -1.5202e-01,\n",
            "         -1.5019e-01,  5.7555e-02, -1.0710e-01,  4.6571e-02, -7.6433e-02,\n",
            "         -1.8520e-02,  1.4251e-01,  1.6990e-01, -3.1265e-02, -1.7488e-02,\n",
            "         -3.9112e-02,  1.2831e-01, -1.4987e-02, -2.8354e-02,  3.0778e-02,\n",
            "          1.1368e-03, -1.2005e-01],\n",
            "        [ 4.0493e-02,  8.7798e-02, -1.0371e-01, -5.9348e-02,  1.0378e-01,\n",
            "          2.8429e-02,  7.4019e-02, -1.4490e-01,  7.9968e-02,  1.0248e-01,\n",
            "         -1.5483e-01, -1.5586e-01, -1.1946e-01, -1.5363e-01,  5.9286e-02,\n",
            "          1.6668e-01, -1.6076e-01, -4.0287e-02,  1.1133e-01, -2.1705e-02,\n",
            "         -1.4530e-01, -5.9354e-02,  4.9114e-02,  1.4563e-01, -2.5055e-02,\n",
            "          7.0740e-02,  9.8989e-02,  1.1375e-01, -1.2160e-02, -1.0491e-01,\n",
            "          1.4533e-01,  3.3865e-02],\n",
            "        [-2.8159e-02,  1.6290e-01,  7.5402e-02,  1.5007e-01,  7.1426e-02,\n",
            "         -1.6104e-01,  1.6561e-01,  8.3672e-03, -8.0825e-02, -1.3866e-01,\n",
            "         -1.7128e-01,  1.6427e-01, -9.5599e-02, -9.1954e-02, -1.0290e-01,\n",
            "         -5.0609e-02, -1.6333e-01, -1.3016e-01,  1.6698e-01, -9.2476e-02,\n",
            "         -1.4689e-01, -9.1796e-02, -1.6287e-01, -6.8146e-02,  2.0948e-02,\n",
            "         -1.5331e-01, -1.3062e-01, -1.1811e-01,  4.9512e-02,  7.4828e-02,\n",
            "          1.3757e-01,  5.9655e-02],\n",
            "        [ 1.0207e-01,  6.6705e-02,  7.7931e-03, -2.8770e-02,  5.2514e-02,\n",
            "          6.0671e-02, -1.5084e-01, -2.0784e-02,  1.5073e-01,  1.2650e-01,\n",
            "         -1.6287e-01, -8.5423e-02,  5.6644e-02,  1.0936e-01, -7.7311e-02,\n",
            "          6.5564e-02,  1.7276e-01,  1.6699e-01, -2.3647e-02, -1.6575e-01,\n",
            "         -1.0952e-01,  1.7404e-01,  1.3952e-01,  7.3175e-02, -9.7631e-02,\n",
            "         -8.0827e-02,  1.8471e-02, -1.0402e-01, -7.6048e-02, -4.8097e-02,\n",
            "         -8.0139e-02, -3.7460e-02],\n",
            "        [ 3.5607e-02, -1.4911e-01, -1.3411e-01, -5.4935e-02,  1.5735e-01,\n",
            "         -5.0125e-02, -5.5373e-02, -1.2012e-02, -3.6672e-02, -1.2996e-01,\n",
            "          6.8688e-02, -1.0581e-02, -1.5379e-01, -6.7269e-02, -5.4743e-02,\n",
            "          1.6774e-01, -2.3391e-02,  3.1671e-03, -1.2966e-01,  1.4799e-01,\n",
            "         -8.0228e-02, -7.4215e-02,  1.3391e-01, -1.6085e-01,  1.5695e-01,\n",
            "          3.2631e-02, -1.4064e-01, -6.5605e-02, -4.6347e-02,  1.6978e-01,\n",
            "          1.5463e-01, -1.9247e-02],\n",
            "        [ 1.6723e-01,  1.7520e-01, -1.4178e-01, -9.1149e-02,  1.2925e-01,\n",
            "          8.1053e-03,  6.4072e-02, -1.7407e-01,  3.7670e-02, -1.7357e-02,\n",
            "         -5.4293e-02, -1.1113e-01, -9.1437e-02, -3.2894e-02,  8.3163e-02,\n",
            "          7.2865e-02,  3.4876e-02, -7.0898e-02,  3.1139e-02, -3.0873e-02,\n",
            "         -1.5259e-01, -1.3257e-01,  1.0107e-01,  1.0207e-01,  1.1456e-01,\n",
            "          1.2848e-02, -1.1898e-01,  2.2607e-02,  1.1950e-01, -2.7311e-05,\n",
            "         -3.9453e-02, -2.8134e-02],\n",
            "        [ 8.7041e-02,  1.5781e-01, -1.4421e-01, -5.5365e-02, -1.7188e-01,\n",
            "         -1.2467e-01,  1.6444e-01,  1.1214e-01,  6.6861e-02, -2.0236e-02,\n",
            "          1.5995e-01,  1.4541e-01,  8.6619e-02,  8.1545e-02,  1.0376e-01,\n",
            "          3.0409e-02, -3.0200e-02, -4.7228e-02,  1.4800e-01,  9.3496e-02,\n",
            "         -1.4920e-01, -1.6207e-01,  5.6466e-02, -4.8443e-02,  5.8388e-02,\n",
            "          9.8099e-02,  1.3219e-01, -1.3823e-02,  4.4599e-02,  3.2251e-02,\n",
            "         -1.1035e-01,  3.9598e-02]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.0011,  0.1145,  0.1003,  0.0787, -0.1051, -0.1396,  0.0611, -0.0617,\n",
            "         0.1530,  0.0609, -0.0171, -0.0880,  0.0106,  0.0681, -0.0015,  0.0400],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.1166,  0.1216,  0.0910,  0.0660,  0.1547,  0.0843, -0.0934, -0.1293,\n",
            "          0.0765,  0.1073,  0.0894, -0.2350,  0.2371,  0.1768, -0.2055, -0.0588],\n",
            "        [ 0.1487, -0.0938,  0.0698,  0.1548, -0.1612,  0.1366, -0.0131, -0.1355,\n",
            "          0.0928,  0.0844, -0.2476, -0.2089,  0.2318,  0.0776,  0.0643,  0.0430],\n",
            "        [-0.0653,  0.1444,  0.2404, -0.0278,  0.0054, -0.1482,  0.0672, -0.1217,\n",
            "          0.2213,  0.0406, -0.1291,  0.0293,  0.1899, -0.1931,  0.0679, -0.0734],\n",
            "        [-0.1649, -0.1271, -0.1735, -0.1347, -0.2292,  0.1572, -0.0572,  0.0217,\n",
            "          0.0468, -0.2369, -0.1053, -0.2432,  0.1651,  0.2458, -0.0194,  0.1649],\n",
            "        [ 0.1359,  0.0987,  0.2468, -0.0694, -0.1664,  0.1237,  0.1221, -0.1625,\n",
            "         -0.0100, -0.1530,  0.1819,  0.1107, -0.2026, -0.0293,  0.1921,  0.0199],\n",
            "        [ 0.1879, -0.0178,  0.0228,  0.2224, -0.1478, -0.0158, -0.2153,  0.0926,\n",
            "         -0.1296, -0.0858,  0.0639,  0.1341, -0.1197, -0.0385,  0.0722, -0.0355],\n",
            "        [ 0.1144, -0.0770, -0.1465, -0.1631, -0.0618,  0.0398, -0.0785, -0.2126,\n",
            "         -0.1658, -0.1642,  0.1137,  0.2114, -0.1602, -0.0665, -0.1908, -0.0499],\n",
            "        [ 0.1586,  0.0291,  0.0589,  0.1516,  0.2272, -0.2188,  0.2483,  0.0170,\n",
            "         -0.1964,  0.0509, -0.0094, -0.2093, -0.1231, -0.0468,  0.1400, -0.0304],\n",
            "        [-0.0969,  0.1000, -0.0275, -0.2029, -0.1926,  0.1278,  0.1335, -0.2056,\n",
            "          0.2239,  0.0342, -0.0890, -0.1216,  0.0612, -0.2213, -0.0034, -0.1693],\n",
            "        [ 0.2263, -0.2401,  0.0999, -0.2175,  0.0646, -0.0304,  0.2286, -0.1938,\n",
            "          0.2213, -0.0776,  0.1126, -0.1359,  0.0675, -0.1270,  0.0129,  0.1365]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.1413, -0.0843, -0.0693,  0.1114,  0.0652,  0.1877,  0.0643, -0.1874,\n",
            "         0.1029, -0.2262], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some training parameters\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Define our loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "GmzTy1fDgX4N"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training of Data"
      ],
      "metadata": {
        "id": "_VkKq4ALggFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training part, only minor changes have been made because I had an error once. For the rest I left it as default. Only for the training part I eventually made some changes, to let it continue after 20 epochs"
      ],
      "metadata": {
        "id": "uo7hUKjpm0rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "kWsmCkzMibcq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    # Loop over batches via the dataloader\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print progress update every few loops\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "LDWHoJcugtiL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Printing some output after a testing round\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "sqvqI0I_gv1z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to repeat the training process for each epoch.\n",
        "#   In each epoch, the model will eventually see EVERY\n",
        "#   observations in the data\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_loader, model, loss_fn, optimizer)\n",
        "    test_loop(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmqYzymag9UA",
        "outputId": "8593f051-cf5e-43de-94c6-5b4d32a291b3",
        "collapsed": true
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.394870  [   64/60000]\n",
            "loss: 0.427703  [  704/60000]\n",
            "loss: 0.389410  [ 1344/60000]\n",
            "loss: 0.184290  [ 1984/60000]\n",
            "loss: 0.335696  [ 2624/60000]\n",
            "loss: 0.451108  [ 3264/60000]\n",
            "loss: 0.447659  [ 3904/60000]\n",
            "loss: 0.371577  [ 4544/60000]\n",
            "loss: 0.448362  [ 5184/60000]\n",
            "loss: 0.436154  [ 5824/60000]\n",
            "loss: 0.297752  [ 6464/60000]\n",
            "loss: 0.211888  [ 7104/60000]\n",
            "loss: 0.475894  [ 7744/60000]\n",
            "loss: 0.495837  [ 8384/60000]\n",
            "loss: 0.228082  [ 9024/60000]\n",
            "loss: 0.481939  [ 9664/60000]\n",
            "loss: 0.533957  [10304/60000]\n",
            "loss: 0.260951  [10944/60000]\n",
            "loss: 0.299694  [11584/60000]\n",
            "loss: 0.486860  [12224/60000]\n",
            "loss: 0.388095  [12864/60000]\n",
            "loss: 0.389307  [13504/60000]\n",
            "loss: 0.273795  [14144/60000]\n",
            "loss: 0.463262  [14784/60000]\n",
            "loss: 0.525119  [15424/60000]\n",
            "loss: 0.269840  [16064/60000]\n",
            "loss: 0.400248  [16704/60000]\n",
            "loss: 0.330636  [17344/60000]\n",
            "loss: 0.411118  [17984/60000]\n",
            "loss: 0.463027  [18624/60000]\n",
            "loss: 0.153935  [19264/60000]\n",
            "loss: 0.366815  [19904/60000]\n",
            "loss: 0.315597  [20544/60000]\n",
            "loss: 0.313709  [21184/60000]\n",
            "loss: 0.325863  [21824/60000]\n",
            "loss: 0.511464  [22464/60000]\n",
            "loss: 0.522544  [23104/60000]\n",
            "loss: 0.216532  [23744/60000]\n",
            "loss: 0.286342  [24384/60000]\n",
            "loss: 0.348838  [25024/60000]\n",
            "loss: 0.513388  [25664/60000]\n",
            "loss: 0.449102  [26304/60000]\n",
            "loss: 0.165646  [26944/60000]\n",
            "loss: 0.340675  [27584/60000]\n",
            "loss: 0.278541  [28224/60000]\n",
            "loss: 0.362057  [28864/60000]\n",
            "loss: 0.438312  [29504/60000]\n",
            "loss: 0.268246  [30144/60000]\n",
            "loss: 0.306937  [30784/60000]\n",
            "loss: 0.282420  [31424/60000]\n",
            "loss: 0.345848  [32064/60000]\n",
            "loss: 0.254800  [32704/60000]\n",
            "loss: 0.479478  [33344/60000]\n",
            "loss: 0.384821  [33984/60000]\n",
            "loss: 0.458420  [34624/60000]\n",
            "loss: 0.385590  [35264/60000]\n",
            "loss: 0.280663  [35904/60000]\n",
            "loss: 0.219362  [36544/60000]\n",
            "loss: 0.441342  [37184/60000]\n",
            "loss: 0.299878  [37824/60000]\n",
            "loss: 0.232411  [38464/60000]\n",
            "loss: 0.306570  [39104/60000]\n",
            "loss: 0.418450  [39744/60000]\n",
            "loss: 0.312970  [40384/60000]\n",
            "loss: 0.489663  [41024/60000]\n",
            "loss: 0.496001  [41664/60000]\n",
            "loss: 0.381971  [42304/60000]\n",
            "loss: 0.308956  [42944/60000]\n",
            "loss: 0.483077  [43584/60000]\n",
            "loss: 0.502620  [44224/60000]\n",
            "loss: 0.459501  [44864/60000]\n",
            "loss: 0.195244  [45504/60000]\n",
            "loss: 0.319632  [46144/60000]\n",
            "loss: 0.489747  [46784/60000]\n",
            "loss: 0.379068  [47424/60000]\n",
            "loss: 0.346541  [48064/60000]\n",
            "loss: 0.421597  [48704/60000]\n",
            "loss: 0.504847  [49344/60000]\n",
            "loss: 0.400298  [49984/60000]\n",
            "loss: 0.206078  [50624/60000]\n",
            "loss: 0.419013  [51264/60000]\n",
            "loss: 0.396346  [51904/60000]\n",
            "loss: 0.300682  [52544/60000]\n",
            "loss: 0.405453  [53184/60000]\n",
            "loss: 0.437194  [53824/60000]\n",
            "loss: 0.363144  [54464/60000]\n",
            "loss: 0.405049  [55104/60000]\n",
            "loss: 0.525668  [55744/60000]\n",
            "loss: 0.464963  [56384/60000]\n",
            "loss: 0.285706  [57024/60000]\n",
            "loss: 0.331264  [57664/60000]\n",
            "loss: 0.254991  [58304/60000]\n",
            "loss: 0.608040  [58944/60000]\n",
            "loss: 0.288424  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.448402 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.292217  [   64/60000]\n",
            "loss: 0.302615  [  704/60000]\n",
            "loss: 0.240297  [ 1344/60000]\n",
            "loss: 0.363309  [ 1984/60000]\n",
            "loss: 0.548048  [ 2624/60000]\n",
            "loss: 0.363849  [ 3264/60000]\n",
            "loss: 0.332262  [ 3904/60000]\n",
            "loss: 0.292449  [ 4544/60000]\n",
            "loss: 0.288600  [ 5184/60000]\n",
            "loss: 0.204048  [ 5824/60000]\n",
            "loss: 0.385545  [ 6464/60000]\n",
            "loss: 0.235464  [ 7104/60000]\n",
            "loss: 0.324916  [ 7744/60000]\n",
            "loss: 0.376394  [ 8384/60000]\n",
            "loss: 0.303091  [ 9024/60000]\n",
            "loss: 0.701738  [ 9664/60000]\n",
            "loss: 0.315550  [10304/60000]\n",
            "loss: 0.543154  [10944/60000]\n",
            "loss: 0.474283  [11584/60000]\n",
            "loss: 0.441024  [12224/60000]\n",
            "loss: 0.140130  [12864/60000]\n",
            "loss: 0.343642  [13504/60000]\n",
            "loss: 0.342122  [14144/60000]\n",
            "loss: 0.517306  [14784/60000]\n",
            "loss: 0.360815  [15424/60000]\n",
            "loss: 0.506520  [16064/60000]\n",
            "loss: 0.332820  [16704/60000]\n",
            "loss: 0.340308  [17344/60000]\n",
            "loss: 0.231099  [17984/60000]\n",
            "loss: 0.304442  [18624/60000]\n",
            "loss: 0.474002  [19264/60000]\n",
            "loss: 0.232409  [19904/60000]\n",
            "loss: 0.328772  [20544/60000]\n",
            "loss: 0.346264  [21184/60000]\n",
            "loss: 0.283781  [21824/60000]\n",
            "loss: 0.329227  [22464/60000]\n",
            "loss: 0.204250  [23104/60000]\n",
            "loss: 0.316227  [23744/60000]\n",
            "loss: 0.429721  [24384/60000]\n",
            "loss: 0.336563  [25024/60000]\n",
            "loss: 0.306367  [25664/60000]\n",
            "loss: 0.505177  [26304/60000]\n",
            "loss: 0.368277  [26944/60000]\n",
            "loss: 0.500947  [27584/60000]\n",
            "loss: 0.439342  [28224/60000]\n",
            "loss: 0.440097  [28864/60000]\n",
            "loss: 0.342515  [29504/60000]\n",
            "loss: 0.326961  [30144/60000]\n",
            "loss: 0.286365  [30784/60000]\n",
            "loss: 0.393408  [31424/60000]\n",
            "loss: 0.393300  [32064/60000]\n",
            "loss: 0.431104  [32704/60000]\n",
            "loss: 0.338351  [33344/60000]\n",
            "loss: 0.572612  [33984/60000]\n",
            "loss: 0.518572  [34624/60000]\n",
            "loss: 0.510186  [35264/60000]\n",
            "loss: 0.232179  [35904/60000]\n",
            "loss: 0.305459  [36544/60000]\n",
            "loss: 0.319821  [37184/60000]\n",
            "loss: 0.313078  [37824/60000]\n",
            "loss: 0.476019  [38464/60000]\n",
            "loss: 0.365075  [39104/60000]\n",
            "loss: 0.473882  [39744/60000]\n",
            "loss: 0.248089  [40384/60000]\n",
            "loss: 0.211307  [41024/60000]\n",
            "loss: 0.346357  [41664/60000]\n",
            "loss: 0.307586  [42304/60000]\n",
            "loss: 0.489595  [42944/60000]\n",
            "loss: 0.336269  [43584/60000]\n",
            "loss: 0.369186  [44224/60000]\n",
            "loss: 0.276477  [44864/60000]\n",
            "loss: 0.487121  [45504/60000]\n",
            "loss: 0.275296  [46144/60000]\n",
            "loss: 0.466759  [46784/60000]\n",
            "loss: 0.366792  [47424/60000]\n",
            "loss: 0.477492  [48064/60000]\n",
            "loss: 0.425410  [48704/60000]\n",
            "loss: 0.288388  [49344/60000]\n",
            "loss: 0.296203  [49984/60000]\n",
            "loss: 0.284269  [50624/60000]\n",
            "loss: 0.236244  [51264/60000]\n",
            "loss: 0.166795  [51904/60000]\n",
            "loss: 0.352456  [52544/60000]\n",
            "loss: 0.517053  [53184/60000]\n",
            "loss: 0.510701  [53824/60000]\n",
            "loss: 0.244639  [54464/60000]\n",
            "loss: 0.375073  [55104/60000]\n",
            "loss: 0.155027  [55744/60000]\n",
            "loss: 0.417062  [56384/60000]\n",
            "loss: 0.236275  [57024/60000]\n",
            "loss: 0.363773  [57664/60000]\n",
            "loss: 0.259918  [58304/60000]\n",
            "loss: 0.355657  [58944/60000]\n",
            "loss: 0.613861  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.471990 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.357958  [   64/60000]\n",
            "loss: 0.318639  [  704/60000]\n",
            "loss: 0.369146  [ 1344/60000]\n",
            "loss: 0.470273  [ 1984/60000]\n",
            "loss: 0.291215  [ 2624/60000]\n",
            "loss: 0.305894  [ 3264/60000]\n",
            "loss: 0.323232  [ 3904/60000]\n",
            "loss: 0.411293  [ 4544/60000]\n",
            "loss: 0.324522  [ 5184/60000]\n",
            "loss: 0.537490  [ 5824/60000]\n",
            "loss: 0.146370  [ 6464/60000]\n",
            "loss: 0.178003  [ 7104/60000]\n",
            "loss: 0.335962  [ 7744/60000]\n",
            "loss: 0.278032  [ 8384/60000]\n",
            "loss: 0.276579  [ 9024/60000]\n",
            "loss: 0.329123  [ 9664/60000]\n",
            "loss: 0.417375  [10304/60000]\n",
            "loss: 0.248072  [10944/60000]\n",
            "loss: 0.148722  [11584/60000]\n",
            "loss: 0.364680  [12224/60000]\n",
            "loss: 0.386205  [12864/60000]\n",
            "loss: 0.401977  [13504/60000]\n",
            "loss: 0.280099  [14144/60000]\n",
            "loss: 0.436269  [14784/60000]\n",
            "loss: 0.577678  [15424/60000]\n",
            "loss: 0.439726  [16064/60000]\n",
            "loss: 0.495694  [16704/60000]\n",
            "loss: 0.349532  [17344/60000]\n",
            "loss: 0.365957  [17984/60000]\n",
            "loss: 0.192382  [18624/60000]\n",
            "loss: 0.347437  [19264/60000]\n",
            "loss: 0.199033  [19904/60000]\n",
            "loss: 0.241531  [20544/60000]\n",
            "loss: 0.384075  [21184/60000]\n",
            "loss: 0.430416  [21824/60000]\n",
            "loss: 0.272173  [22464/60000]\n",
            "loss: 0.368920  [23104/60000]\n",
            "loss: 0.292050  [23744/60000]\n",
            "loss: 0.337928  [24384/60000]\n",
            "loss: 0.427344  [25024/60000]\n",
            "loss: 0.512362  [25664/60000]\n",
            "loss: 0.334667  [26304/60000]\n",
            "loss: 0.224309  [26944/60000]\n",
            "loss: 0.392001  [27584/60000]\n",
            "loss: 0.482666  [28224/60000]\n",
            "loss: 0.307063  [28864/60000]\n",
            "loss: 0.223575  [29504/60000]\n",
            "loss: 0.143640  [30144/60000]\n",
            "loss: 0.361582  [30784/60000]\n",
            "loss: 0.239226  [31424/60000]\n",
            "loss: 0.294575  [32064/60000]\n",
            "loss: 0.303584  [32704/60000]\n",
            "loss: 0.394854  [33344/60000]\n",
            "loss: 0.394907  [33984/60000]\n",
            "loss: 0.215916  [34624/60000]\n",
            "loss: 0.232771  [35264/60000]\n",
            "loss: 0.399116  [35904/60000]\n",
            "loss: 0.394679  [36544/60000]\n",
            "loss: 0.264720  [37184/60000]\n",
            "loss: 0.395153  [37824/60000]\n",
            "loss: 0.263752  [38464/60000]\n",
            "loss: 0.317962  [39104/60000]\n",
            "loss: 0.355809  [39744/60000]\n",
            "loss: 0.259839  [40384/60000]\n",
            "loss: 0.344403  [41024/60000]\n",
            "loss: 0.352561  [41664/60000]\n",
            "loss: 0.383378  [42304/60000]\n",
            "loss: 0.379194  [42944/60000]\n",
            "loss: 0.305159  [43584/60000]\n",
            "loss: 0.309360  [44224/60000]\n",
            "loss: 0.289881  [44864/60000]\n",
            "loss: 0.302187  [45504/60000]\n",
            "loss: 0.310414  [46144/60000]\n",
            "loss: 0.449440  [46784/60000]\n",
            "loss: 0.440154  [47424/60000]\n",
            "loss: 0.406640  [48064/60000]\n",
            "loss: 0.373354  [48704/60000]\n",
            "loss: 0.327181  [49344/60000]\n",
            "loss: 0.304050  [49984/60000]\n",
            "loss: 0.190641  [50624/60000]\n",
            "loss: 0.393391  [51264/60000]\n",
            "loss: 0.333208  [51904/60000]\n",
            "loss: 0.308487  [52544/60000]\n",
            "loss: 0.404000  [53184/60000]\n",
            "loss: 0.350933  [53824/60000]\n",
            "loss: 0.530990  [54464/60000]\n",
            "loss: 0.324324  [55104/60000]\n",
            "loss: 0.342096  [55744/60000]\n",
            "loss: 0.472362  [56384/60000]\n",
            "loss: 0.323386  [57024/60000]\n",
            "loss: 0.522058  [57664/60000]\n",
            "loss: 0.292277  [58304/60000]\n",
            "loss: 0.536420  [58944/60000]\n",
            "loss: 0.342267  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.401654 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.317592  [   64/60000]\n",
            "loss: 0.435206  [  704/60000]\n",
            "loss: 0.435789  [ 1344/60000]\n",
            "loss: 0.373547  [ 1984/60000]\n",
            "loss: 0.351520  [ 2624/60000]\n",
            "loss: 0.381909  [ 3264/60000]\n",
            "loss: 0.223340  [ 3904/60000]\n",
            "loss: 0.439813  [ 4544/60000]\n",
            "loss: 0.196020  [ 5184/60000]\n",
            "loss: 0.420312  [ 5824/60000]\n",
            "loss: 0.401402  [ 6464/60000]\n",
            "loss: 0.435081  [ 7104/60000]\n",
            "loss: 0.333199  [ 7744/60000]\n",
            "loss: 0.320294  [ 8384/60000]\n",
            "loss: 0.225915  [ 9024/60000]\n",
            "loss: 0.304376  [ 9664/60000]\n",
            "loss: 0.478767  [10304/60000]\n",
            "loss: 0.266837  [10944/60000]\n",
            "loss: 0.227737  [11584/60000]\n",
            "loss: 0.328140  [12224/60000]\n",
            "loss: 0.459690  [12864/60000]\n",
            "loss: 0.436050  [13504/60000]\n",
            "loss: 0.289571  [14144/60000]\n",
            "loss: 0.485132  [14784/60000]\n",
            "loss: 0.415149  [15424/60000]\n",
            "loss: 0.236418  [16064/60000]\n",
            "loss: 0.243400  [16704/60000]\n",
            "loss: 0.225798  [17344/60000]\n",
            "loss: 0.429611  [17984/60000]\n",
            "loss: 0.596596  [18624/60000]\n",
            "loss: 0.228016  [19264/60000]\n",
            "loss: 0.418278  [19904/60000]\n",
            "loss: 0.356240  [20544/60000]\n",
            "loss: 0.398517  [21184/60000]\n",
            "loss: 0.374626  [21824/60000]\n",
            "loss: 0.221817  [22464/60000]\n",
            "loss: 0.266146  [23104/60000]\n",
            "loss: 0.235438  [23744/60000]\n",
            "loss: 0.424240  [24384/60000]\n",
            "loss: 0.367257  [25024/60000]\n",
            "loss: 0.351541  [25664/60000]\n",
            "loss: 0.545123  [26304/60000]\n",
            "loss: 0.448722  [26944/60000]\n",
            "loss: 0.460724  [27584/60000]\n",
            "loss: 0.337361  [28224/60000]\n",
            "loss: 0.433293  [28864/60000]\n",
            "loss: 0.359840  [29504/60000]\n",
            "loss: 0.378408  [30144/60000]\n",
            "loss: 0.422365  [30784/60000]\n",
            "loss: 0.337146  [31424/60000]\n",
            "loss: 0.289186  [32064/60000]\n",
            "loss: 0.419383  [32704/60000]\n",
            "loss: 0.219895  [33344/60000]\n",
            "loss: 0.293936  [33984/60000]\n",
            "loss: 0.444584  [34624/60000]\n",
            "loss: 0.524631  [35264/60000]\n",
            "loss: 0.599044  [35904/60000]\n",
            "loss: 0.219372  [36544/60000]\n",
            "loss: 0.213465  [37184/60000]\n",
            "loss: 0.231751  [37824/60000]\n",
            "loss: 0.462915  [38464/60000]\n",
            "loss: 0.396847  [39104/60000]\n",
            "loss: 0.423591  [39744/60000]\n",
            "loss: 0.275219  [40384/60000]\n",
            "loss: 0.524411  [41024/60000]\n",
            "loss: 0.203391  [41664/60000]\n",
            "loss: 0.257297  [42304/60000]\n",
            "loss: 0.328102  [42944/60000]\n",
            "loss: 0.298743  [43584/60000]\n",
            "loss: 0.320523  [44224/60000]\n",
            "loss: 0.304173  [44864/60000]\n",
            "loss: 0.358275  [45504/60000]\n",
            "loss: 0.339406  [46144/60000]\n",
            "loss: 0.361592  [46784/60000]\n",
            "loss: 0.346832  [47424/60000]\n",
            "loss: 0.367552  [48064/60000]\n",
            "loss: 0.411905  [48704/60000]\n",
            "loss: 0.464427  [49344/60000]\n",
            "loss: 0.409967  [49984/60000]\n",
            "loss: 0.201402  [50624/60000]\n",
            "loss: 0.326459  [51264/60000]\n",
            "loss: 0.351393  [51904/60000]\n",
            "loss: 0.382163  [52544/60000]\n",
            "loss: 0.438414  [53184/60000]\n",
            "loss: 0.438398  [53824/60000]\n",
            "loss: 0.418282  [54464/60000]\n",
            "loss: 0.318467  [55104/60000]\n",
            "loss: 0.571884  [55744/60000]\n",
            "loss: 0.337740  [56384/60000]\n",
            "loss: 0.306274  [57024/60000]\n",
            "loss: 0.547974  [57664/60000]\n",
            "loss: 0.270706  [58304/60000]\n",
            "loss: 0.370653  [58944/60000]\n",
            "loss: 0.312000  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.465794 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.511776  [   64/60000]\n",
            "loss: 0.533984  [  704/60000]\n",
            "loss: 0.421225  [ 1344/60000]\n",
            "loss: 0.381962  [ 1984/60000]\n",
            "loss: 0.308657  [ 2624/60000]\n",
            "loss: 0.301543  [ 3264/60000]\n",
            "loss: 0.299859  [ 3904/60000]\n",
            "loss: 0.305730  [ 4544/60000]\n",
            "loss: 0.249747  [ 5184/60000]\n",
            "loss: 0.223507  [ 5824/60000]\n",
            "loss: 0.276456  [ 6464/60000]\n",
            "loss: 0.345072  [ 7104/60000]\n",
            "loss: 0.399143  [ 7744/60000]\n",
            "loss: 0.388378  [ 8384/60000]\n",
            "loss: 0.331088  [ 9024/60000]\n",
            "loss: 0.347652  [ 9664/60000]\n",
            "loss: 0.449546  [10304/60000]\n",
            "loss: 0.255888  [10944/60000]\n",
            "loss: 0.519075  [11584/60000]\n",
            "loss: 0.511179  [12224/60000]\n",
            "loss: 0.256621  [12864/60000]\n",
            "loss: 0.292689  [13504/60000]\n",
            "loss: 0.344821  [14144/60000]\n",
            "loss: 0.341866  [14784/60000]\n",
            "loss: 0.284676  [15424/60000]\n",
            "loss: 0.379640  [16064/60000]\n",
            "loss: 0.372553  [16704/60000]\n",
            "loss: 0.265743  [17344/60000]\n",
            "loss: 0.719924  [17984/60000]\n",
            "loss: 0.293467  [18624/60000]\n",
            "loss: 0.369346  [19264/60000]\n",
            "loss: 0.291245  [19904/60000]\n",
            "loss: 0.250957  [20544/60000]\n",
            "loss: 0.233415  [21184/60000]\n",
            "loss: 0.571083  [21824/60000]\n",
            "loss: 0.354413  [22464/60000]\n",
            "loss: 0.315875  [23104/60000]\n",
            "loss: 0.304329  [23744/60000]\n",
            "loss: 0.377315  [24384/60000]\n",
            "loss: 0.291023  [25024/60000]\n",
            "loss: 0.470607  [25664/60000]\n",
            "loss: 0.289753  [26304/60000]\n",
            "loss: 0.379263  [26944/60000]\n",
            "loss: 0.324852  [27584/60000]\n",
            "loss: 0.285606  [28224/60000]\n",
            "loss: 0.255890  [28864/60000]\n",
            "loss: 0.374000  [29504/60000]\n",
            "loss: 0.256411  [30144/60000]\n",
            "loss: 0.252044  [30784/60000]\n",
            "loss: 0.260000  [31424/60000]\n",
            "loss: 0.334349  [32064/60000]\n",
            "loss: 0.351343  [32704/60000]\n",
            "loss: 0.319767  [33344/60000]\n",
            "loss: 0.346368  [33984/60000]\n",
            "loss: 0.448603  [34624/60000]\n",
            "loss: 0.395368  [35264/60000]\n",
            "loss: 0.166454  [35904/60000]\n",
            "loss: 0.599197  [36544/60000]\n",
            "loss: 0.478485  [37184/60000]\n",
            "loss: 0.344774  [37824/60000]\n",
            "loss: 0.118209  [38464/60000]\n",
            "loss: 0.329141  [39104/60000]\n",
            "loss: 0.300362  [39744/60000]\n",
            "loss: 0.382084  [40384/60000]\n",
            "loss: 0.443732  [41024/60000]\n",
            "loss: 0.385633  [41664/60000]\n",
            "loss: 0.243855  [42304/60000]\n",
            "loss: 0.461202  [42944/60000]\n",
            "loss: 0.358946  [43584/60000]\n",
            "loss: 0.231892  [44224/60000]\n",
            "loss: 0.340779  [44864/60000]\n",
            "loss: 0.375591  [45504/60000]\n",
            "loss: 0.485643  [46144/60000]\n",
            "loss: 0.497575  [46784/60000]\n",
            "loss: 0.384206  [47424/60000]\n",
            "loss: 0.442411  [48064/60000]\n",
            "loss: 0.448236  [48704/60000]\n",
            "loss: 0.408269  [49344/60000]\n",
            "loss: 0.395920  [49984/60000]\n",
            "loss: 0.331257  [50624/60000]\n",
            "loss: 0.204749  [51264/60000]\n",
            "loss: 0.334406  [51904/60000]\n",
            "loss: 0.321785  [52544/60000]\n",
            "loss: 0.334710  [53184/60000]\n",
            "loss: 0.458084  [53824/60000]\n",
            "loss: 0.322584  [54464/60000]\n",
            "loss: 0.408962  [55104/60000]\n",
            "loss: 0.217553  [55744/60000]\n",
            "loss: 0.471266  [56384/60000]\n",
            "loss: 0.319741  [57024/60000]\n",
            "loss: 0.289308  [57664/60000]\n",
            "loss: 0.327811  [58304/60000]\n",
            "loss: 0.342748  [58944/60000]\n",
            "loss: 0.363119  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.441388 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.405551  [   64/60000]\n",
            "loss: 0.651069  [  704/60000]\n",
            "loss: 0.472692  [ 1344/60000]\n",
            "loss: 0.397237  [ 1984/60000]\n",
            "loss: 0.347929  [ 2624/60000]\n",
            "loss: 0.348730  [ 3264/60000]\n",
            "loss: 0.525947  [ 3904/60000]\n",
            "loss: 0.483889  [ 4544/60000]\n",
            "loss: 0.347019  [ 5184/60000]\n",
            "loss: 0.253827  [ 5824/60000]\n",
            "loss: 0.267046  [ 6464/60000]\n",
            "loss: 0.292565  [ 7104/60000]\n",
            "loss: 0.414036  [ 7744/60000]\n",
            "loss: 0.235010  [ 8384/60000]\n",
            "loss: 0.392053  [ 9024/60000]\n",
            "loss: 0.430811  [ 9664/60000]\n",
            "loss: 0.267284  [10304/60000]\n",
            "loss: 0.326328  [10944/60000]\n",
            "loss: 0.194722  [11584/60000]\n",
            "loss: 0.390424  [12224/60000]\n",
            "loss: 0.337921  [12864/60000]\n",
            "loss: 0.303693  [13504/60000]\n",
            "loss: 0.349455  [14144/60000]\n",
            "loss: 0.252986  [14784/60000]\n",
            "loss: 0.263275  [15424/60000]\n",
            "loss: 0.286995  [16064/60000]\n",
            "loss: 0.341433  [16704/60000]\n",
            "loss: 0.466991  [17344/60000]\n",
            "loss: 0.358890  [17984/60000]\n",
            "loss: 0.398472  [18624/60000]\n",
            "loss: 0.198990  [19264/60000]\n",
            "loss: 0.300855  [19904/60000]\n",
            "loss: 0.296075  [20544/60000]\n",
            "loss: 0.273819  [21184/60000]\n",
            "loss: 0.487901  [21824/60000]\n",
            "loss: 0.448545  [22464/60000]\n",
            "loss: 0.399566  [23104/60000]\n",
            "loss: 0.336554  [23744/60000]\n",
            "loss: 0.236311  [24384/60000]\n",
            "loss: 0.474595  [25024/60000]\n",
            "loss: 0.275524  [25664/60000]\n",
            "loss: 0.367761  [26304/60000]\n",
            "loss: 0.333637  [26944/60000]\n",
            "loss: 0.258659  [27584/60000]\n",
            "loss: 0.256958  [28224/60000]\n",
            "loss: 0.331703  [28864/60000]\n",
            "loss: 0.326821  [29504/60000]\n",
            "loss: 0.617143  [30144/60000]\n",
            "loss: 0.286802  [30784/60000]\n",
            "loss: 0.353857  [31424/60000]\n",
            "loss: 0.286787  [32064/60000]\n",
            "loss: 0.447898  [32704/60000]\n",
            "loss: 0.355182  [33344/60000]\n",
            "loss: 0.414633  [33984/60000]\n",
            "loss: 0.402755  [34624/60000]\n",
            "loss: 0.221342  [35264/60000]\n",
            "loss: 0.299088  [35904/60000]\n",
            "loss: 0.388884  [36544/60000]\n",
            "loss: 0.402958  [37184/60000]\n",
            "loss: 0.529457  [37824/60000]\n",
            "loss: 0.279255  [38464/60000]\n",
            "loss: 0.323305  [39104/60000]\n",
            "loss: 0.450368  [39744/60000]\n",
            "loss: 0.472627  [40384/60000]\n",
            "loss: 0.290099  [41024/60000]\n",
            "loss: 0.439391  [41664/60000]\n",
            "loss: 0.485373  [42304/60000]\n",
            "loss: 0.321945  [42944/60000]\n",
            "loss: 0.263084  [43584/60000]\n",
            "loss: 0.441154  [44224/60000]\n",
            "loss: 0.456888  [44864/60000]\n",
            "loss: 0.244439  [45504/60000]\n",
            "loss: 0.389123  [46144/60000]\n",
            "loss: 0.401641  [46784/60000]\n",
            "loss: 0.476279  [47424/60000]\n",
            "loss: 0.515157  [48064/60000]\n",
            "loss: 0.581743  [48704/60000]\n",
            "loss: 0.290178  [49344/60000]\n",
            "loss: 0.286998  [49984/60000]\n",
            "loss: 0.418073  [50624/60000]\n",
            "loss: 0.317735  [51264/60000]\n",
            "loss: 0.293019  [51904/60000]\n",
            "loss: 0.405949  [52544/60000]\n",
            "loss: 0.191531  [53184/60000]\n",
            "loss: 0.413197  [53824/60000]\n",
            "loss: 0.551863  [54464/60000]\n",
            "loss: 0.299267  [55104/60000]\n",
            "loss: 0.381736  [55744/60000]\n",
            "loss: 0.269534  [56384/60000]\n",
            "loss: 0.753594  [57024/60000]\n",
            "loss: 0.192471  [57664/60000]\n",
            "loss: 0.295050  [58304/60000]\n",
            "loss: 0.292285  [58944/60000]\n",
            "loss: 0.370086  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 0.414754 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.288451  [   64/60000]\n",
            "loss: 0.189234  [  704/60000]\n",
            "loss: 0.244845  [ 1344/60000]\n",
            "loss: 0.286121  [ 1984/60000]\n",
            "loss: 0.362113  [ 2624/60000]\n",
            "loss: 0.233768  [ 3264/60000]\n",
            "loss: 0.534712  [ 3904/60000]\n",
            "loss: 0.391117  [ 4544/60000]\n",
            "loss: 0.171008  [ 5184/60000]\n",
            "loss: 0.167219  [ 5824/60000]\n",
            "loss: 0.395247  [ 6464/60000]\n",
            "loss: 0.373237  [ 7104/60000]\n",
            "loss: 0.292343  [ 7744/60000]\n",
            "loss: 0.317272  [ 8384/60000]\n",
            "loss: 0.199576  [ 9024/60000]\n",
            "loss: 0.358315  [ 9664/60000]\n",
            "loss: 0.372527  [10304/60000]\n",
            "loss: 0.395809  [10944/60000]\n",
            "loss: 0.184482  [11584/60000]\n",
            "loss: 0.361874  [12224/60000]\n",
            "loss: 0.563784  [12864/60000]\n",
            "loss: 0.278260  [13504/60000]\n",
            "loss: 0.294047  [14144/60000]\n",
            "loss: 0.359525  [14784/60000]\n",
            "loss: 0.385753  [15424/60000]\n",
            "loss: 0.326592  [16064/60000]\n",
            "loss: 0.415849  [16704/60000]\n",
            "loss: 0.375607  [17344/60000]\n",
            "loss: 0.247159  [17984/60000]\n",
            "loss: 0.467801  [18624/60000]\n",
            "loss: 0.272822  [19264/60000]\n",
            "loss: 0.252997  [19904/60000]\n",
            "loss: 0.341480  [20544/60000]\n",
            "loss: 0.473941  [21184/60000]\n",
            "loss: 0.417394  [21824/60000]\n",
            "loss: 0.297046  [22464/60000]\n",
            "loss: 0.361187  [23104/60000]\n",
            "loss: 0.247830  [23744/60000]\n",
            "loss: 0.230087  [24384/60000]\n",
            "loss: 0.196317  [25024/60000]\n",
            "loss: 0.478722  [25664/60000]\n",
            "loss: 0.427501  [26304/60000]\n",
            "loss: 0.290914  [26944/60000]\n",
            "loss: 0.269546  [27584/60000]\n",
            "loss: 0.214257  [28224/60000]\n",
            "loss: 0.563814  [28864/60000]\n",
            "loss: 0.354669  [29504/60000]\n",
            "loss: 0.743168  [30144/60000]\n",
            "loss: 0.381655  [30784/60000]\n",
            "loss: 0.267510  [31424/60000]\n",
            "loss: 0.348229  [32064/60000]\n",
            "loss: 0.392904  [32704/60000]\n",
            "loss: 0.221674  [33344/60000]\n",
            "loss: 0.310028  [33984/60000]\n",
            "loss: 0.323217  [34624/60000]\n",
            "loss: 0.192560  [35264/60000]\n",
            "loss: 0.486882  [35904/60000]\n",
            "loss: 0.352422  [36544/60000]\n",
            "loss: 0.328019  [37184/60000]\n",
            "loss: 0.347391  [37824/60000]\n",
            "loss: 0.353611  [38464/60000]\n",
            "loss: 0.189139  [39104/60000]\n",
            "loss: 0.293097  [39744/60000]\n",
            "loss: 0.254778  [40384/60000]\n",
            "loss: 0.330258  [41024/60000]\n",
            "loss: 0.343487  [41664/60000]\n",
            "loss: 0.210237  [42304/60000]\n",
            "loss: 0.300880  [42944/60000]\n",
            "loss: 0.324388  [43584/60000]\n",
            "loss: 0.257727  [44224/60000]\n",
            "loss: 0.364657  [44864/60000]\n",
            "loss: 0.238302  [45504/60000]\n",
            "loss: 0.363393  [46144/60000]\n",
            "loss: 0.266484  [46784/60000]\n",
            "loss: 0.202897  [47424/60000]\n",
            "loss: 0.585945  [48064/60000]\n",
            "loss: 0.457485  [48704/60000]\n",
            "loss: 0.212673  [49344/60000]\n",
            "loss: 0.271743  [49984/60000]\n",
            "loss: 0.499650  [50624/60000]\n",
            "loss: 0.271163  [51264/60000]\n",
            "loss: 0.411423  [51904/60000]\n",
            "loss: 0.652860  [52544/60000]\n",
            "loss: 0.475334  [53184/60000]\n",
            "loss: 0.305101  [53824/60000]\n",
            "loss: 0.360829  [54464/60000]\n",
            "loss: 0.267462  [55104/60000]\n",
            "loss: 0.335425  [55744/60000]\n",
            "loss: 0.289332  [56384/60000]\n",
            "loss: 0.347314  [57024/60000]\n",
            "loss: 0.170740  [57664/60000]\n",
            "loss: 0.292355  [58304/60000]\n",
            "loss: 0.344687  [58944/60000]\n",
            "loss: 0.375458  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.445207 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.236389  [   64/60000]\n",
            "loss: 0.217117  [  704/60000]\n",
            "loss: 0.369284  [ 1344/60000]\n",
            "loss: 0.276323  [ 1984/60000]\n",
            "loss: 0.431544  [ 2624/60000]\n",
            "loss: 0.329710  [ 3264/60000]\n",
            "loss: 0.381093  [ 3904/60000]\n",
            "loss: 0.261831  [ 4544/60000]\n",
            "loss: 0.280404  [ 5184/60000]\n",
            "loss: 0.367479  [ 5824/60000]\n",
            "loss: 0.401933  [ 6464/60000]\n",
            "loss: 0.345973  [ 7104/60000]\n",
            "loss: 0.255104  [ 7744/60000]\n",
            "loss: 0.301944  [ 8384/60000]\n",
            "loss: 0.247198  [ 9024/60000]\n",
            "loss: 0.364885  [ 9664/60000]\n",
            "loss: 0.236848  [10304/60000]\n",
            "loss: 0.268484  [10944/60000]\n",
            "loss: 0.230945  [11584/60000]\n",
            "loss: 0.350841  [12224/60000]\n",
            "loss: 0.332560  [12864/60000]\n",
            "loss: 0.151291  [13504/60000]\n",
            "loss: 0.406051  [14144/60000]\n",
            "loss: 0.214405  [14784/60000]\n",
            "loss: 0.214424  [15424/60000]\n",
            "loss: 0.270320  [16064/60000]\n",
            "loss: 0.246011  [16704/60000]\n",
            "loss: 0.367934  [17344/60000]\n",
            "loss: 0.319927  [17984/60000]\n",
            "loss: 0.308161  [18624/60000]\n",
            "loss: 0.261806  [19264/60000]\n",
            "loss: 0.295555  [19904/60000]\n",
            "loss: 0.240745  [20544/60000]\n",
            "loss: 0.256905  [21184/60000]\n",
            "loss: 0.273486  [21824/60000]\n",
            "loss: 0.266803  [22464/60000]\n",
            "loss: 0.418303  [23104/60000]\n",
            "loss: 0.417853  [23744/60000]\n",
            "loss: 0.424237  [24384/60000]\n",
            "loss: 0.222039  [25024/60000]\n",
            "loss: 0.360681  [25664/60000]\n",
            "loss: 0.345124  [26304/60000]\n",
            "loss: 0.400347  [26944/60000]\n",
            "loss: 0.340773  [27584/60000]\n",
            "loss: 0.411150  [28224/60000]\n",
            "loss: 0.296944  [28864/60000]\n",
            "loss: 0.186201  [29504/60000]\n",
            "loss: 0.393461  [30144/60000]\n",
            "loss: 0.278231  [30784/60000]\n",
            "loss: 0.147334  [31424/60000]\n",
            "loss: 0.333383  [32064/60000]\n",
            "loss: 0.436250  [32704/60000]\n",
            "loss: 0.229454  [33344/60000]\n",
            "loss: 0.530902  [33984/60000]\n",
            "loss: 0.188260  [34624/60000]\n",
            "loss: 0.291060  [35264/60000]\n",
            "loss: 0.568676  [35904/60000]\n",
            "loss: 0.271440  [36544/60000]\n",
            "loss: 0.296145  [37184/60000]\n",
            "loss: 0.285309  [37824/60000]\n",
            "loss: 0.238235  [38464/60000]\n",
            "loss: 0.570750  [39104/60000]\n",
            "loss: 0.394230  [39744/60000]\n",
            "loss: 0.320944  [40384/60000]\n",
            "loss: 0.486192  [41024/60000]\n",
            "loss: 0.446534  [41664/60000]\n",
            "loss: 0.462120  [42304/60000]\n",
            "loss: 0.207262  [42944/60000]\n",
            "loss: 0.380975  [43584/60000]\n",
            "loss: 0.578884  [44224/60000]\n",
            "loss: 0.164886  [44864/60000]\n",
            "loss: 0.314424  [45504/60000]\n",
            "loss: 0.274446  [46144/60000]\n",
            "loss: 0.292410  [46784/60000]\n",
            "loss: 0.330179  [47424/60000]\n",
            "loss: 0.268049  [48064/60000]\n",
            "loss: 0.481646  [48704/60000]\n",
            "loss: 0.304084  [49344/60000]\n",
            "loss: 0.470469  [49984/60000]\n",
            "loss: 0.454985  [50624/60000]\n",
            "loss: 0.357298  [51264/60000]\n",
            "loss: 0.177617  [51904/60000]\n",
            "loss: 0.325333  [52544/60000]\n",
            "loss: 0.307396  [53184/60000]\n",
            "loss: 0.413937  [53824/60000]\n",
            "loss: 0.401865  [54464/60000]\n",
            "loss: 0.282906  [55104/60000]\n",
            "loss: 0.384592  [55744/60000]\n",
            "loss: 0.259796  [56384/60000]\n",
            "loss: 0.466684  [57024/60000]\n",
            "loss: 0.338940  [57664/60000]\n",
            "loss: 0.534323  [58304/60000]\n",
            "loss: 0.323051  [58944/60000]\n",
            "loss: 0.225927  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.545731 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.518457  [   64/60000]\n",
            "loss: 0.308190  [  704/60000]\n",
            "loss: 0.299421  [ 1344/60000]\n",
            "loss: 0.248126  [ 1984/60000]\n",
            "loss: 0.419948  [ 2624/60000]\n",
            "loss: 0.400659  [ 3264/60000]\n",
            "loss: 0.361407  [ 3904/60000]\n",
            "loss: 0.411391  [ 4544/60000]\n",
            "loss: 0.457238  [ 5184/60000]\n",
            "loss: 0.217935  [ 5824/60000]\n",
            "loss: 0.402613  [ 6464/60000]\n",
            "loss: 0.379292  [ 7104/60000]\n",
            "loss: 0.289712  [ 7744/60000]\n",
            "loss: 0.233547  [ 8384/60000]\n",
            "loss: 0.229789  [ 9024/60000]\n",
            "loss: 0.403245  [ 9664/60000]\n",
            "loss: 0.345619  [10304/60000]\n",
            "loss: 0.275453  [10944/60000]\n",
            "loss: 0.280317  [11584/60000]\n",
            "loss: 0.276203  [12224/60000]\n",
            "loss: 0.314405  [12864/60000]\n",
            "loss: 0.392956  [13504/60000]\n",
            "loss: 0.262521  [14144/60000]\n",
            "loss: 0.384956  [14784/60000]\n",
            "loss: 0.425735  [15424/60000]\n",
            "loss: 0.305372  [16064/60000]\n",
            "loss: 0.320009  [16704/60000]\n",
            "loss: 0.230082  [17344/60000]\n",
            "loss: 0.222861  [17984/60000]\n",
            "loss: 0.346480  [18624/60000]\n",
            "loss: 0.362610  [19264/60000]\n",
            "loss: 0.294952  [19904/60000]\n",
            "loss: 0.307537  [20544/60000]\n",
            "loss: 0.261821  [21184/60000]\n",
            "loss: 0.429857  [21824/60000]\n",
            "loss: 0.322188  [22464/60000]\n",
            "loss: 0.270219  [23104/60000]\n",
            "loss: 0.241533  [23744/60000]\n",
            "loss: 0.280051  [24384/60000]\n",
            "loss: 0.272720  [25024/60000]\n",
            "loss: 0.279146  [25664/60000]\n",
            "loss: 0.167219  [26304/60000]\n",
            "loss: 0.314053  [26944/60000]\n",
            "loss: 0.353095  [27584/60000]\n",
            "loss: 0.305064  [28224/60000]\n",
            "loss: 0.432939  [28864/60000]\n",
            "loss: 0.326051  [29504/60000]\n",
            "loss: 0.399260  [30144/60000]\n",
            "loss: 0.276527  [30784/60000]\n",
            "loss: 0.301356  [31424/60000]\n",
            "loss: 0.346712  [32064/60000]\n",
            "loss: 0.190993  [32704/60000]\n",
            "loss: 0.233735  [33344/60000]\n",
            "loss: 0.324789  [33984/60000]\n",
            "loss: 0.204779  [34624/60000]\n",
            "loss: 0.340479  [35264/60000]\n",
            "loss: 0.207397  [35904/60000]\n",
            "loss: 0.452671  [36544/60000]\n",
            "loss: 0.331499  [37184/60000]\n",
            "loss: 0.145505  [37824/60000]\n",
            "loss: 0.183762  [38464/60000]\n",
            "loss: 0.497755  [39104/60000]\n",
            "loss: 0.315152  [39744/60000]\n",
            "loss: 0.370283  [40384/60000]\n",
            "loss: 0.184629  [41024/60000]\n",
            "loss: 0.464158  [41664/60000]\n",
            "loss: 0.470153  [42304/60000]\n",
            "loss: 0.223560  [42944/60000]\n",
            "loss: 0.124262  [43584/60000]\n",
            "loss: 0.407130  [44224/60000]\n",
            "loss: 0.359416  [44864/60000]\n",
            "loss: 0.266262  [45504/60000]\n",
            "loss: 0.248557  [46144/60000]\n",
            "loss: 0.189094  [46784/60000]\n",
            "loss: 0.368500  [47424/60000]\n",
            "loss: 0.284712  [48064/60000]\n",
            "loss: 0.294978  [48704/60000]\n",
            "loss: 0.326220  [49344/60000]\n",
            "loss: 0.204037  [49984/60000]\n",
            "loss: 0.226792  [50624/60000]\n",
            "loss: 0.389136  [51264/60000]\n",
            "loss: 0.521113  [51904/60000]\n",
            "loss: 0.417466  [52544/60000]\n",
            "loss: 0.456396  [53184/60000]\n",
            "loss: 0.311005  [53824/60000]\n",
            "loss: 0.378820  [54464/60000]\n",
            "loss: 0.320003  [55104/60000]\n",
            "loss: 0.300776  [55744/60000]\n",
            "loss: 0.375590  [56384/60000]\n",
            "loss: 0.326029  [57024/60000]\n",
            "loss: 0.370067  [57664/60000]\n",
            "loss: 0.480005  [58304/60000]\n",
            "loss: 0.318795  [58944/60000]\n",
            "loss: 0.382388  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.446055 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.428615  [   64/60000]\n",
            "loss: 0.284969  [  704/60000]\n",
            "loss: 0.465383  [ 1344/60000]\n",
            "loss: 0.237839  [ 1984/60000]\n",
            "loss: 0.274281  [ 2624/60000]\n",
            "loss: 0.262471  [ 3264/60000]\n",
            "loss: 0.418679  [ 3904/60000]\n",
            "loss: 0.266720  [ 4544/60000]\n",
            "loss: 0.216055  [ 5184/60000]\n",
            "loss: 0.390846  [ 5824/60000]\n",
            "loss: 0.409687  [ 6464/60000]\n",
            "loss: 0.327891  [ 7104/60000]\n",
            "loss: 0.256377  [ 7744/60000]\n",
            "loss: 0.289035  [ 8384/60000]\n",
            "loss: 0.307833  [ 9024/60000]\n",
            "loss: 0.336068  [ 9664/60000]\n",
            "loss: 0.324270  [10304/60000]\n",
            "loss: 0.522944  [10944/60000]\n",
            "loss: 0.391464  [11584/60000]\n",
            "loss: 0.273026  [12224/60000]\n",
            "loss: 0.158353  [12864/60000]\n",
            "loss: 0.395030  [13504/60000]\n",
            "loss: 0.319065  [14144/60000]\n",
            "loss: 0.204243  [14784/60000]\n",
            "loss: 0.210331  [15424/60000]\n",
            "loss: 0.349939  [16064/60000]\n",
            "loss: 0.184774  [16704/60000]\n",
            "loss: 0.438639  [17344/60000]\n",
            "loss: 0.304386  [17984/60000]\n",
            "loss: 0.383422  [18624/60000]\n",
            "loss: 0.372271  [19264/60000]\n",
            "loss: 0.436718  [19904/60000]\n",
            "loss: 0.237944  [20544/60000]\n",
            "loss: 0.436791  [21184/60000]\n",
            "loss: 0.216510  [21824/60000]\n",
            "loss: 0.295772  [22464/60000]\n",
            "loss: 0.106970  [23104/60000]\n",
            "loss: 0.376987  [23744/60000]\n",
            "loss: 0.424754  [24384/60000]\n",
            "loss: 0.281622  [25024/60000]\n",
            "loss: 0.261879  [25664/60000]\n",
            "loss: 0.211403  [26304/60000]\n",
            "loss: 0.376177  [26944/60000]\n",
            "loss: 0.373061  [27584/60000]\n",
            "loss: 0.427381  [28224/60000]\n",
            "loss: 0.302659  [28864/60000]\n",
            "loss: 0.136302  [29504/60000]\n",
            "loss: 0.300670  [30144/60000]\n",
            "loss: 0.348505  [30784/60000]\n",
            "loss: 0.188317  [31424/60000]\n",
            "loss: 0.486430  [32064/60000]\n",
            "loss: 0.315062  [32704/60000]\n",
            "loss: 0.352718  [33344/60000]\n",
            "loss: 0.156666  [33984/60000]\n",
            "loss: 0.191160  [34624/60000]\n",
            "loss: 0.236497  [35264/60000]\n",
            "loss: 0.380534  [35904/60000]\n",
            "loss: 0.475846  [36544/60000]\n",
            "loss: 0.148630  [37184/60000]\n",
            "loss: 0.371038  [37824/60000]\n",
            "loss: 0.535821  [38464/60000]\n",
            "loss: 0.249675  [39104/60000]\n",
            "loss: 0.269103  [39744/60000]\n",
            "loss: 0.496706  [40384/60000]\n",
            "loss: 0.229759  [41024/60000]\n",
            "loss: 0.336446  [41664/60000]\n",
            "loss: 0.458496  [42304/60000]\n",
            "loss: 0.244890  [42944/60000]\n",
            "loss: 0.397138  [43584/60000]\n",
            "loss: 0.211156  [44224/60000]\n",
            "loss: 0.229417  [44864/60000]\n",
            "loss: 0.174608  [45504/60000]\n",
            "loss: 0.450674  [46144/60000]\n",
            "loss: 0.290924  [46784/60000]\n",
            "loss: 0.336341  [47424/60000]\n",
            "loss: 0.367519  [48064/60000]\n",
            "loss: 0.217244  [48704/60000]\n",
            "loss: 0.421172  [49344/60000]\n",
            "loss: 0.273613  [49984/60000]\n",
            "loss: 0.269110  [50624/60000]\n",
            "loss: 0.575740  [51264/60000]\n",
            "loss: 0.341224  [51904/60000]\n",
            "loss: 0.283408  [52544/60000]\n",
            "loss: 0.278165  [53184/60000]\n",
            "loss: 0.235748  [53824/60000]\n",
            "loss: 0.433032  [54464/60000]\n",
            "loss: 0.195645  [55104/60000]\n",
            "loss: 0.177675  [55744/60000]\n",
            "loss: 0.489428  [56384/60000]\n",
            "loss: 0.304001  [57024/60000]\n",
            "loss: 0.271790  [57664/60000]\n",
            "loss: 0.389640  [58304/60000]\n",
            "loss: 0.421584  [58944/60000]\n",
            "loss: 0.238142  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.8%, Avg loss: 0.409102 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.166542  [   64/60000]\n",
            "loss: 0.294080  [  704/60000]\n",
            "loss: 0.296701  [ 1344/60000]\n",
            "loss: 0.520389  [ 1984/60000]\n",
            "loss: 0.460846  [ 2624/60000]\n",
            "loss: 0.406835  [ 3264/60000]\n",
            "loss: 0.142318  [ 3904/60000]\n",
            "loss: 0.265072  [ 4544/60000]\n",
            "loss: 0.244732  [ 5184/60000]\n",
            "loss: 0.182315  [ 5824/60000]\n",
            "loss: 0.249950  [ 6464/60000]\n",
            "loss: 0.163970  [ 7104/60000]\n",
            "loss: 0.495908  [ 7744/60000]\n",
            "loss: 0.284854  [ 8384/60000]\n",
            "loss: 0.419546  [ 9024/60000]\n",
            "loss: 0.223495  [ 9664/60000]\n",
            "loss: 0.226886  [10304/60000]\n",
            "loss: 0.278177  [10944/60000]\n",
            "loss: 0.239028  [11584/60000]\n",
            "loss: 0.580390  [12224/60000]\n",
            "loss: 0.639299  [12864/60000]\n",
            "loss: 0.340459  [13504/60000]\n",
            "loss: 0.279921  [14144/60000]\n",
            "loss: 0.179924  [14784/60000]\n",
            "loss: 0.205535  [15424/60000]\n",
            "loss: 0.176342  [16064/60000]\n",
            "loss: 0.235951  [16704/60000]\n",
            "loss: 0.287259  [17344/60000]\n",
            "loss: 0.160680  [17984/60000]\n",
            "loss: 0.215186  [18624/60000]\n",
            "loss: 0.246324  [19264/60000]\n",
            "loss: 0.315055  [19904/60000]\n",
            "loss: 0.292371  [20544/60000]\n",
            "loss: 0.226322  [21184/60000]\n",
            "loss: 0.292831  [21824/60000]\n",
            "loss: 0.426839  [22464/60000]\n",
            "loss: 0.160568  [23104/60000]\n",
            "loss: 0.274947  [23744/60000]\n",
            "loss: 0.267754  [24384/60000]\n",
            "loss: 0.238940  [25024/60000]\n",
            "loss: 0.172367  [25664/60000]\n",
            "loss: 0.399261  [26304/60000]\n",
            "loss: 0.314662  [26944/60000]\n",
            "loss: 0.352695  [27584/60000]\n",
            "loss: 0.290396  [28224/60000]\n",
            "loss: 0.384386  [28864/60000]\n",
            "loss: 0.521430  [29504/60000]\n",
            "loss: 0.317943  [30144/60000]\n",
            "loss: 0.141751  [30784/60000]\n",
            "loss: 0.489842  [31424/60000]\n",
            "loss: 0.349366  [32064/60000]\n",
            "loss: 0.137472  [32704/60000]\n",
            "loss: 0.324188  [33344/60000]\n",
            "loss: 0.496223  [33984/60000]\n",
            "loss: 0.368233  [34624/60000]\n",
            "loss: 0.235370  [35264/60000]\n",
            "loss: 0.326585  [35904/60000]\n",
            "loss: 0.381103  [36544/60000]\n",
            "loss: 0.332195  [37184/60000]\n",
            "loss: 0.317727  [37824/60000]\n",
            "loss: 0.234292  [38464/60000]\n",
            "loss: 0.386259  [39104/60000]\n",
            "loss: 0.212692  [39744/60000]\n",
            "loss: 0.231717  [40384/60000]\n",
            "loss: 0.206650  [41024/60000]\n",
            "loss: 0.429864  [41664/60000]\n",
            "loss: 0.293352  [42304/60000]\n",
            "loss: 0.426184  [42944/60000]\n",
            "loss: 0.331400  [43584/60000]\n",
            "loss: 0.503391  [44224/60000]\n",
            "loss: 0.401860  [44864/60000]\n",
            "loss: 0.189463  [45504/60000]\n",
            "loss: 0.145739  [46144/60000]\n",
            "loss: 0.177131  [46784/60000]\n",
            "loss: 0.396396  [47424/60000]\n",
            "loss: 0.239956  [48064/60000]\n",
            "loss: 0.392880  [48704/60000]\n",
            "loss: 0.325413  [49344/60000]\n",
            "loss: 0.240182  [49984/60000]\n",
            "loss: 0.205869  [50624/60000]\n",
            "loss: 0.426499  [51264/60000]\n",
            "loss: 0.394365  [51904/60000]\n",
            "loss: 0.271602  [52544/60000]\n",
            "loss: 0.168801  [53184/60000]\n",
            "loss: 0.449999  [53824/60000]\n",
            "loss: 0.293870  [54464/60000]\n",
            "loss: 0.306629  [55104/60000]\n",
            "loss: 0.345040  [55744/60000]\n",
            "loss: 0.348899  [56384/60000]\n",
            "loss: 0.464942  [57024/60000]\n",
            "loss: 0.140691  [57664/60000]\n",
            "loss: 0.245655  [58304/60000]\n",
            "loss: 0.455749  [58944/60000]\n",
            "loss: 0.276475  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.2%, Avg loss: 0.414439 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.194414  [   64/60000]\n",
            "loss: 0.265947  [  704/60000]\n",
            "loss: 0.199583  [ 1344/60000]\n",
            "loss: 0.418282  [ 1984/60000]\n",
            "loss: 0.228843  [ 2624/60000]\n",
            "loss: 0.169392  [ 3264/60000]\n",
            "loss: 0.340605  [ 3904/60000]\n",
            "loss: 0.201461  [ 4544/60000]\n",
            "loss: 0.333520  [ 5184/60000]\n",
            "loss: 0.364083  [ 5824/60000]\n",
            "loss: 0.199991  [ 6464/60000]\n",
            "loss: 0.201816  [ 7104/60000]\n",
            "loss: 0.208561  [ 7744/60000]\n",
            "loss: 0.249143  [ 8384/60000]\n",
            "loss: 0.262846  [ 9024/60000]\n",
            "loss: 0.320963  [ 9664/60000]\n",
            "loss: 0.274077  [10304/60000]\n",
            "loss: 0.228831  [10944/60000]\n",
            "loss: 0.433830  [11584/60000]\n",
            "loss: 0.500964  [12224/60000]\n",
            "loss: 0.329324  [12864/60000]\n",
            "loss: 0.249298  [13504/60000]\n",
            "loss: 0.255539  [14144/60000]\n",
            "loss: 0.218902  [14784/60000]\n",
            "loss: 0.369365  [15424/60000]\n",
            "loss: 0.315559  [16064/60000]\n",
            "loss: 0.250852  [16704/60000]\n",
            "loss: 0.258182  [17344/60000]\n",
            "loss: 0.379967  [17984/60000]\n",
            "loss: 0.161043  [18624/60000]\n",
            "loss: 0.247975  [19264/60000]\n",
            "loss: 0.355074  [19904/60000]\n",
            "loss: 0.257952  [20544/60000]\n",
            "loss: 0.207828  [21184/60000]\n",
            "loss: 0.480608  [21824/60000]\n",
            "loss: 0.224597  [22464/60000]\n",
            "loss: 0.311432  [23104/60000]\n",
            "loss: 0.287368  [23744/60000]\n",
            "loss: 0.193215  [24384/60000]\n",
            "loss: 0.292125  [25024/60000]\n",
            "loss: 0.409269  [25664/60000]\n",
            "loss: 0.328654  [26304/60000]\n",
            "loss: 0.562474  [26944/60000]\n",
            "loss: 0.178235  [27584/60000]\n",
            "loss: 0.393919  [28224/60000]\n",
            "loss: 0.455548  [28864/60000]\n",
            "loss: 0.249023  [29504/60000]\n",
            "loss: 0.435220  [30144/60000]\n",
            "loss: 0.326220  [30784/60000]\n",
            "loss: 0.237933  [31424/60000]\n",
            "loss: 0.536274  [32064/60000]\n",
            "loss: 0.247440  [32704/60000]\n",
            "loss: 0.400396  [33344/60000]\n",
            "loss: 0.290917  [33984/60000]\n",
            "loss: 0.245147  [34624/60000]\n",
            "loss: 0.339521  [35264/60000]\n",
            "loss: 0.269760  [35904/60000]\n",
            "loss: 0.244483  [36544/60000]\n",
            "loss: 0.262478  [37184/60000]\n",
            "loss: 0.210259  [37824/60000]\n",
            "loss: 0.242450  [38464/60000]\n",
            "loss: 0.281818  [39104/60000]\n",
            "loss: 0.322012  [39744/60000]\n",
            "loss: 0.492470  [40384/60000]\n",
            "loss: 0.370935  [41024/60000]\n",
            "loss: 0.266913  [41664/60000]\n",
            "loss: 0.333569  [42304/60000]\n",
            "loss: 0.269637  [42944/60000]\n",
            "loss: 0.267428  [43584/60000]\n",
            "loss: 0.388528  [44224/60000]\n",
            "loss: 0.224005  [44864/60000]\n",
            "loss: 0.330602  [45504/60000]\n",
            "loss: 0.193329  [46144/60000]\n",
            "loss: 0.390518  [46784/60000]\n",
            "loss: 0.253469  [47424/60000]\n",
            "loss: 0.189771  [48064/60000]\n",
            "loss: 0.439561  [48704/60000]\n",
            "loss: 0.280870  [49344/60000]\n",
            "loss: 0.264158  [49984/60000]\n",
            "loss: 0.335931  [50624/60000]\n",
            "loss: 0.198706  [51264/60000]\n",
            "loss: 0.325738  [51904/60000]\n",
            "loss: 0.311805  [52544/60000]\n",
            "loss: 0.352072  [53184/60000]\n",
            "loss: 0.392887  [53824/60000]\n",
            "loss: 0.287206  [54464/60000]\n",
            "loss: 0.316859  [55104/60000]\n",
            "loss: 0.259088  [55744/60000]\n",
            "loss: 0.207246  [56384/60000]\n",
            "loss: 0.228411  [57024/60000]\n",
            "loss: 0.228230  [57664/60000]\n",
            "loss: 0.211133  [58304/60000]\n",
            "loss: 0.227412  [58944/60000]\n",
            "loss: 0.282001  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.378620 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.420626  [   64/60000]\n",
            "loss: 0.157046  [  704/60000]\n",
            "loss: 0.196533  [ 1344/60000]\n",
            "loss: 0.183616  [ 1984/60000]\n",
            "loss: 0.443484  [ 2624/60000]\n",
            "loss: 0.343788  [ 3264/60000]\n",
            "loss: 0.302242  [ 3904/60000]\n",
            "loss: 0.333138  [ 4544/60000]\n",
            "loss: 0.245344  [ 5184/60000]\n",
            "loss: 0.413553  [ 5824/60000]\n",
            "loss: 0.236490  [ 6464/60000]\n",
            "loss: 0.223848  [ 7104/60000]\n",
            "loss: 0.495714  [ 7744/60000]\n",
            "loss: 0.374079  [ 8384/60000]\n",
            "loss: 0.296350  [ 9024/60000]\n",
            "loss: 0.258583  [ 9664/60000]\n",
            "loss: 0.354854  [10304/60000]\n",
            "loss: 0.504977  [10944/60000]\n",
            "loss: 0.258350  [11584/60000]\n",
            "loss: 0.408680  [12224/60000]\n",
            "loss: 0.330360  [12864/60000]\n",
            "loss: 0.376889  [13504/60000]\n",
            "loss: 0.381883  [14144/60000]\n",
            "loss: 0.410389  [14784/60000]\n",
            "loss: 0.256188  [15424/60000]\n",
            "loss: 0.237673  [16064/60000]\n",
            "loss: 0.309741  [16704/60000]\n",
            "loss: 0.216457  [17344/60000]\n",
            "loss: 0.226109  [17984/60000]\n",
            "loss: 0.307306  [18624/60000]\n",
            "loss: 0.393703  [19264/60000]\n",
            "loss: 0.127795  [19904/60000]\n",
            "loss: 0.299812  [20544/60000]\n",
            "loss: 0.268677  [21184/60000]\n",
            "loss: 0.356429  [21824/60000]\n",
            "loss: 0.226185  [22464/60000]\n",
            "loss: 0.196753  [23104/60000]\n",
            "loss: 0.296606  [23744/60000]\n",
            "loss: 0.312180  [24384/60000]\n",
            "loss: 0.211450  [25024/60000]\n",
            "loss: 0.603735  [25664/60000]\n",
            "loss: 0.204099  [26304/60000]\n",
            "loss: 0.226700  [26944/60000]\n",
            "loss: 0.190337  [27584/60000]\n",
            "loss: 0.200120  [28224/60000]\n",
            "loss: 0.210378  [28864/60000]\n",
            "loss: 0.351103  [29504/60000]\n",
            "loss: 0.259749  [30144/60000]\n",
            "loss: 0.221417  [30784/60000]\n",
            "loss: 0.388396  [31424/60000]\n",
            "loss: 0.303806  [32064/60000]\n",
            "loss: 0.364548  [32704/60000]\n",
            "loss: 0.227106  [33344/60000]\n",
            "loss: 0.353985  [33984/60000]\n",
            "loss: 0.221208  [34624/60000]\n",
            "loss: 0.414929  [35264/60000]\n",
            "loss: 0.309766  [35904/60000]\n",
            "loss: 0.273900  [36544/60000]\n",
            "loss: 0.234683  [37184/60000]\n",
            "loss: 0.337126  [37824/60000]\n",
            "loss: 0.255131  [38464/60000]\n",
            "loss: 0.270434  [39104/60000]\n",
            "loss: 0.238006  [39744/60000]\n",
            "loss: 0.256579  [40384/60000]\n",
            "loss: 0.170425  [41024/60000]\n",
            "loss: 0.395779  [41664/60000]\n",
            "loss: 0.213133  [42304/60000]\n",
            "loss: 0.240945  [42944/60000]\n",
            "loss: 0.424208  [43584/60000]\n",
            "loss: 0.488457  [44224/60000]\n",
            "loss: 0.483625  [44864/60000]\n",
            "loss: 0.247261  [45504/60000]\n",
            "loss: 0.203910  [46144/60000]\n",
            "loss: 0.196024  [46784/60000]\n",
            "loss: 0.232782  [47424/60000]\n",
            "loss: 0.278391  [48064/60000]\n",
            "loss: 0.303393  [48704/60000]\n",
            "loss: 0.300011  [49344/60000]\n",
            "loss: 0.371054  [49984/60000]\n",
            "loss: 0.483658  [50624/60000]\n",
            "loss: 0.305029  [51264/60000]\n",
            "loss: 0.284997  [51904/60000]\n",
            "loss: 0.286580  [52544/60000]\n",
            "loss: 0.369899  [53184/60000]\n",
            "loss: 0.215269  [53824/60000]\n",
            "loss: 0.287085  [54464/60000]\n",
            "loss: 0.408782  [55104/60000]\n",
            "loss: 0.441006  [55744/60000]\n",
            "loss: 0.118416  [56384/60000]\n",
            "loss: 0.226210  [57024/60000]\n",
            "loss: 0.161869  [57664/60000]\n",
            "loss: 0.190838  [58304/60000]\n",
            "loss: 0.434359  [58944/60000]\n",
            "loss: 0.374412  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.6%, Avg loss: 0.408566 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.360257  [   64/60000]\n",
            "loss: 0.464913  [  704/60000]\n",
            "loss: 0.190562  [ 1344/60000]\n",
            "loss: 0.376591  [ 1984/60000]\n",
            "loss: 0.311609  [ 2624/60000]\n",
            "loss: 0.240789  [ 3264/60000]\n",
            "loss: 0.187582  [ 3904/60000]\n",
            "loss: 0.312645  [ 4544/60000]\n",
            "loss: 0.231947  [ 5184/60000]\n",
            "loss: 0.285722  [ 5824/60000]\n",
            "loss: 0.272645  [ 6464/60000]\n",
            "loss: 0.249416  [ 7104/60000]\n",
            "loss: 0.200295  [ 7744/60000]\n",
            "loss: 0.202543  [ 8384/60000]\n",
            "loss: 0.583705  [ 9024/60000]\n",
            "loss: 0.323041  [ 9664/60000]\n",
            "loss: 0.339416  [10304/60000]\n",
            "loss: 0.316910  [10944/60000]\n",
            "loss: 0.288336  [11584/60000]\n",
            "loss: 0.367487  [12224/60000]\n",
            "loss: 0.245966  [12864/60000]\n",
            "loss: 0.261936  [13504/60000]\n",
            "loss: 0.168506  [14144/60000]\n",
            "loss: 0.418792  [14784/60000]\n",
            "loss: 0.237616  [15424/60000]\n",
            "loss: 0.212515  [16064/60000]\n",
            "loss: 0.265801  [16704/60000]\n",
            "loss: 0.270934  [17344/60000]\n",
            "loss: 0.462556  [17984/60000]\n",
            "loss: 0.370457  [18624/60000]\n",
            "loss: 0.247974  [19264/60000]\n",
            "loss: 0.225767  [19904/60000]\n",
            "loss: 0.390814  [20544/60000]\n",
            "loss: 0.285661  [21184/60000]\n",
            "loss: 0.302707  [21824/60000]\n",
            "loss: 0.296269  [22464/60000]\n",
            "loss: 0.216159  [23104/60000]\n",
            "loss: 0.208627  [23744/60000]\n",
            "loss: 0.240348  [24384/60000]\n",
            "loss: 0.217093  [25024/60000]\n",
            "loss: 0.190267  [25664/60000]\n",
            "loss: 0.199442  [26304/60000]\n",
            "loss: 0.288472  [26944/60000]\n",
            "loss: 0.385752  [27584/60000]\n",
            "loss: 0.173020  [28224/60000]\n",
            "loss: 0.280485  [28864/60000]\n",
            "loss: 0.371358  [29504/60000]\n",
            "loss: 0.234167  [30144/60000]\n",
            "loss: 0.267731  [30784/60000]\n",
            "loss: 0.325638  [31424/60000]\n",
            "loss: 0.294709  [32064/60000]\n",
            "loss: 0.282905  [32704/60000]\n",
            "loss: 0.312261  [33344/60000]\n",
            "loss: 0.214665  [33984/60000]\n",
            "loss: 0.370500  [34624/60000]\n",
            "loss: 0.310953  [35264/60000]\n",
            "loss: 0.382720  [35904/60000]\n",
            "loss: 0.298344  [36544/60000]\n",
            "loss: 0.433374  [37184/60000]\n",
            "loss: 0.235914  [37824/60000]\n",
            "loss: 0.335072  [38464/60000]\n",
            "loss: 0.210790  [39104/60000]\n",
            "loss: 0.313801  [39744/60000]\n",
            "loss: 0.310397  [40384/60000]\n",
            "loss: 0.323209  [41024/60000]\n",
            "loss: 0.344236  [41664/60000]\n",
            "loss: 0.257706  [42304/60000]\n",
            "loss: 0.210220  [42944/60000]\n",
            "loss: 0.498540  [43584/60000]\n",
            "loss: 0.320416  [44224/60000]\n",
            "loss: 0.332918  [44864/60000]\n",
            "loss: 0.388111  [45504/60000]\n",
            "loss: 0.332859  [46144/60000]\n",
            "loss: 0.431272  [46784/60000]\n",
            "loss: 0.191915  [47424/60000]\n",
            "loss: 0.239081  [48064/60000]\n",
            "loss: 0.271440  [48704/60000]\n",
            "loss: 0.346449  [49344/60000]\n",
            "loss: 0.535297  [49984/60000]\n",
            "loss: 0.319187  [50624/60000]\n",
            "loss: 0.311794  [51264/60000]\n",
            "loss: 0.285479  [51904/60000]\n",
            "loss: 0.425413  [52544/60000]\n",
            "loss: 0.515155  [53184/60000]\n",
            "loss: 0.545504  [53824/60000]\n",
            "loss: 0.450821  [54464/60000]\n",
            "loss: 0.134571  [55104/60000]\n",
            "loss: 0.250417  [55744/60000]\n",
            "loss: 0.227527  [56384/60000]\n",
            "loss: 0.136783  [57024/60000]\n",
            "loss: 0.192507  [57664/60000]\n",
            "loss: 0.202142  [58304/60000]\n",
            "loss: 0.290052  [58944/60000]\n",
            "loss: 0.364175  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.384805 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.254941  [   64/60000]\n",
            "loss: 0.323517  [  704/60000]\n",
            "loss: 0.337467  [ 1344/60000]\n",
            "loss: 0.195147  [ 1984/60000]\n",
            "loss: 0.502011  [ 2624/60000]\n",
            "loss: 0.287264  [ 3264/60000]\n",
            "loss: 0.195277  [ 3904/60000]\n",
            "loss: 0.332804  [ 4544/60000]\n",
            "loss: 0.158230  [ 5184/60000]\n",
            "loss: 0.189919  [ 5824/60000]\n",
            "loss: 0.308665  [ 6464/60000]\n",
            "loss: 0.269028  [ 7104/60000]\n",
            "loss: 0.289767  [ 7744/60000]\n",
            "loss: 0.357450  [ 8384/60000]\n",
            "loss: 0.185227  [ 9024/60000]\n",
            "loss: 0.558881  [ 9664/60000]\n",
            "loss: 0.490411  [10304/60000]\n",
            "loss: 0.325099  [10944/60000]\n",
            "loss: 0.221946  [11584/60000]\n",
            "loss: 0.259087  [12224/60000]\n",
            "loss: 0.325953  [12864/60000]\n",
            "loss: 0.131820  [13504/60000]\n",
            "loss: 0.291211  [14144/60000]\n",
            "loss: 0.415777  [14784/60000]\n",
            "loss: 0.220345  [15424/60000]\n",
            "loss: 0.405356  [16064/60000]\n",
            "loss: 0.403607  [16704/60000]\n",
            "loss: 0.392412  [17344/60000]\n",
            "loss: 0.372193  [17984/60000]\n",
            "loss: 0.303624  [18624/60000]\n",
            "loss: 0.375668  [19264/60000]\n",
            "loss: 0.403576  [19904/60000]\n",
            "loss: 0.173597  [20544/60000]\n",
            "loss: 0.240782  [21184/60000]\n",
            "loss: 0.306935  [21824/60000]\n",
            "loss: 0.173468  [22464/60000]\n",
            "loss: 0.320670  [23104/60000]\n",
            "loss: 0.178830  [23744/60000]\n",
            "loss: 0.175053  [24384/60000]\n",
            "loss: 0.235524  [25024/60000]\n",
            "loss: 0.364197  [25664/60000]\n",
            "loss: 0.309680  [26304/60000]\n",
            "loss: 0.248364  [26944/60000]\n",
            "loss: 0.344194  [27584/60000]\n",
            "loss: 0.296775  [28224/60000]\n",
            "loss: 0.222214  [28864/60000]\n",
            "loss: 0.219994  [29504/60000]\n",
            "loss: 0.334313  [30144/60000]\n",
            "loss: 0.298876  [30784/60000]\n",
            "loss: 0.291085  [31424/60000]\n",
            "loss: 0.208161  [32064/60000]\n",
            "loss: 0.132878  [32704/60000]\n",
            "loss: 0.498225  [33344/60000]\n",
            "loss: 0.384109  [33984/60000]\n",
            "loss: 0.441793  [34624/60000]\n",
            "loss: 0.196711  [35264/60000]\n",
            "loss: 0.225809  [35904/60000]\n",
            "loss: 0.300549  [36544/60000]\n",
            "loss: 0.412542  [37184/60000]\n",
            "loss: 0.213687  [37824/60000]\n",
            "loss: 0.286274  [38464/60000]\n",
            "loss: 0.273642  [39104/60000]\n",
            "loss: 0.274395  [39744/60000]\n",
            "loss: 0.200650  [40384/60000]\n",
            "loss: 0.369346  [41024/60000]\n",
            "loss: 0.311035  [41664/60000]\n",
            "loss: 0.246676  [42304/60000]\n",
            "loss: 0.405315  [42944/60000]\n",
            "loss: 0.266775  [43584/60000]\n",
            "loss: 0.310816  [44224/60000]\n",
            "loss: 0.194765  [44864/60000]\n",
            "loss: 0.180945  [45504/60000]\n",
            "loss: 0.197943  [46144/60000]\n",
            "loss: 0.440120  [46784/60000]\n",
            "loss: 0.305678  [47424/60000]\n",
            "loss: 0.385257  [48064/60000]\n",
            "loss: 0.337710  [48704/60000]\n",
            "loss: 0.238070  [49344/60000]\n",
            "loss: 0.188076  [49984/60000]\n",
            "loss: 0.259990  [50624/60000]\n",
            "loss: 0.408379  [51264/60000]\n",
            "loss: 0.237034  [51904/60000]\n",
            "loss: 0.270249  [52544/60000]\n",
            "loss: 0.311885  [53184/60000]\n",
            "loss: 0.269723  [53824/60000]\n",
            "loss: 0.176923  [54464/60000]\n",
            "loss: 0.251425  [55104/60000]\n",
            "loss: 0.346805  [55744/60000]\n",
            "loss: 0.388678  [56384/60000]\n",
            "loss: 0.369318  [57024/60000]\n",
            "loss: 0.293673  [57664/60000]\n",
            "loss: 0.348058  [58304/60000]\n",
            "loss: 0.344308  [58944/60000]\n",
            "loss: 0.342506  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.385194 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.379670  [   64/60000]\n",
            "loss: 0.256933  [  704/60000]\n",
            "loss: 0.240193  [ 1344/60000]\n",
            "loss: 0.256317  [ 1984/60000]\n",
            "loss: 0.217583  [ 2624/60000]\n",
            "loss: 0.310585  [ 3264/60000]\n",
            "loss: 0.352089  [ 3904/60000]\n",
            "loss: 0.199745  [ 4544/60000]\n",
            "loss: 0.376767  [ 5184/60000]\n",
            "loss: 0.421954  [ 5824/60000]\n",
            "loss: 0.292767  [ 6464/60000]\n",
            "loss: 0.288978  [ 7104/60000]\n",
            "loss: 0.189135  [ 7744/60000]\n",
            "loss: 0.298748  [ 8384/60000]\n",
            "loss: 0.154743  [ 9024/60000]\n",
            "loss: 0.419129  [ 9664/60000]\n",
            "loss: 0.269392  [10304/60000]\n",
            "loss: 0.189220  [10944/60000]\n",
            "loss: 0.389038  [11584/60000]\n",
            "loss: 0.245389  [12224/60000]\n",
            "loss: 0.220976  [12864/60000]\n",
            "loss: 0.420565  [13504/60000]\n",
            "loss: 0.147116  [14144/60000]\n",
            "loss: 0.393830  [14784/60000]\n",
            "loss: 0.424093  [15424/60000]\n",
            "loss: 0.300303  [16064/60000]\n",
            "loss: 0.400244  [16704/60000]\n",
            "loss: 0.216131  [17344/60000]\n",
            "loss: 0.270889  [17984/60000]\n",
            "loss: 0.566974  [18624/60000]\n",
            "loss: 0.294354  [19264/60000]\n",
            "loss: 0.595167  [19904/60000]\n",
            "loss: 0.393908  [20544/60000]\n",
            "loss: 0.364959  [21184/60000]\n",
            "loss: 0.244059  [21824/60000]\n",
            "loss: 0.333309  [22464/60000]\n",
            "loss: 0.200846  [23104/60000]\n",
            "loss: 0.236393  [23744/60000]\n",
            "loss: 0.286864  [24384/60000]\n",
            "loss: 0.218174  [25024/60000]\n",
            "loss: 0.263116  [25664/60000]\n",
            "loss: 0.176217  [26304/60000]\n",
            "loss: 0.373843  [26944/60000]\n",
            "loss: 0.241259  [27584/60000]\n",
            "loss: 0.378148  [28224/60000]\n",
            "loss: 0.429304  [28864/60000]\n",
            "loss: 0.264395  [29504/60000]\n",
            "loss: 0.172572  [30144/60000]\n",
            "loss: 0.257638  [30784/60000]\n",
            "loss: 0.355938  [31424/60000]\n",
            "loss: 0.368640  [32064/60000]\n",
            "loss: 0.188536  [32704/60000]\n",
            "loss: 0.166042  [33344/60000]\n",
            "loss: 0.246279  [33984/60000]\n",
            "loss: 0.271449  [34624/60000]\n",
            "loss: 0.228930  [35264/60000]\n",
            "loss: 0.366814  [35904/60000]\n",
            "loss: 0.326609  [36544/60000]\n",
            "loss: 0.219889  [37184/60000]\n",
            "loss: 0.156649  [37824/60000]\n",
            "loss: 0.440402  [38464/60000]\n",
            "loss: 0.275941  [39104/60000]\n",
            "loss: 0.414719  [39744/60000]\n",
            "loss: 0.282168  [40384/60000]\n",
            "loss: 0.503900  [41024/60000]\n",
            "loss: 0.390934  [41664/60000]\n",
            "loss: 0.161023  [42304/60000]\n",
            "loss: 0.194585  [42944/60000]\n",
            "loss: 0.302628  [43584/60000]\n",
            "loss: 0.249071  [44224/60000]\n",
            "loss: 0.239007  [44864/60000]\n",
            "loss: 0.280616  [45504/60000]\n",
            "loss: 0.256312  [46144/60000]\n",
            "loss: 0.249910  [46784/60000]\n",
            "loss: 0.224527  [47424/60000]\n",
            "loss: 0.383509  [48064/60000]\n",
            "loss: 0.394342  [48704/60000]\n",
            "loss: 0.288280  [49344/60000]\n",
            "loss: 0.319343  [49984/60000]\n",
            "loss: 0.426424  [50624/60000]\n",
            "loss: 0.195649  [51264/60000]\n",
            "loss: 0.319143  [51904/60000]\n",
            "loss: 0.245086  [52544/60000]\n",
            "loss: 0.228638  [53184/60000]\n",
            "loss: 0.246225  [53824/60000]\n",
            "loss: 0.215729  [54464/60000]\n",
            "loss: 0.217923  [55104/60000]\n",
            "loss: 0.181134  [55744/60000]\n",
            "loss: 0.407377  [56384/60000]\n",
            "loss: 0.180451  [57024/60000]\n",
            "loss: 0.444543  [57664/60000]\n",
            "loss: 0.545894  [58304/60000]\n",
            "loss: 0.410400  [58944/60000]\n",
            "loss: 0.281649  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.387239 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.355340  [   64/60000]\n",
            "loss: 0.356043  [  704/60000]\n",
            "loss: 0.345893  [ 1344/60000]\n",
            "loss: 0.398814  [ 1984/60000]\n",
            "loss: 0.285677  [ 2624/60000]\n",
            "loss: 0.469616  [ 3264/60000]\n",
            "loss: 0.413134  [ 3904/60000]\n",
            "loss: 0.178799  [ 4544/60000]\n",
            "loss: 0.139020  [ 5184/60000]\n",
            "loss: 0.402266  [ 5824/60000]\n",
            "loss: 0.363200  [ 6464/60000]\n",
            "loss: 0.380665  [ 7104/60000]\n",
            "loss: 0.270211  [ 7744/60000]\n",
            "loss: 0.124564  [ 8384/60000]\n",
            "loss: 0.212113  [ 9024/60000]\n",
            "loss: 0.212404  [ 9664/60000]\n",
            "loss: 0.276568  [10304/60000]\n",
            "loss: 0.209786  [10944/60000]\n",
            "loss: 0.320821  [11584/60000]\n",
            "loss: 0.230452  [12224/60000]\n",
            "loss: 0.179929  [12864/60000]\n",
            "loss: 0.211186  [13504/60000]\n",
            "loss: 0.223614  [14144/60000]\n",
            "loss: 0.336678  [14784/60000]\n",
            "loss: 0.422475  [15424/60000]\n",
            "loss: 0.365141  [16064/60000]\n",
            "loss: 0.375595  [16704/60000]\n",
            "loss: 0.184816  [17344/60000]\n",
            "loss: 0.312814  [17984/60000]\n",
            "loss: 0.560858  [18624/60000]\n",
            "loss: 0.126991  [19264/60000]\n",
            "loss: 0.488117  [19904/60000]\n",
            "loss: 0.139442  [20544/60000]\n",
            "loss: 0.172623  [21184/60000]\n",
            "loss: 0.435731  [21824/60000]\n",
            "loss: 0.291594  [22464/60000]\n",
            "loss: 0.262527  [23104/60000]\n",
            "loss: 0.197047  [23744/60000]\n",
            "loss: 0.321806  [24384/60000]\n",
            "loss: 0.308431  [25024/60000]\n",
            "loss: 0.371951  [25664/60000]\n",
            "loss: 0.128541  [26304/60000]\n",
            "loss: 0.171773  [26944/60000]\n",
            "loss: 0.310131  [27584/60000]\n",
            "loss: 0.218138  [28224/60000]\n",
            "loss: 0.278418  [28864/60000]\n",
            "loss: 0.268670  [29504/60000]\n",
            "loss: 0.305865  [30144/60000]\n",
            "loss: 0.316552  [30784/60000]\n",
            "loss: 0.261905  [31424/60000]\n",
            "loss: 0.188839  [32064/60000]\n",
            "loss: 0.172342  [32704/60000]\n",
            "loss: 0.301315  [33344/60000]\n",
            "loss: 0.225665  [33984/60000]\n",
            "loss: 0.315852  [34624/60000]\n",
            "loss: 0.233876  [35264/60000]\n",
            "loss: 0.250644  [35904/60000]\n",
            "loss: 0.282607  [36544/60000]\n",
            "loss: 0.414011  [37184/60000]\n",
            "loss: 0.231600  [37824/60000]\n",
            "loss: 0.348117  [38464/60000]\n",
            "loss: 0.385593  [39104/60000]\n",
            "loss: 0.321699  [39744/60000]\n",
            "loss: 0.104812  [40384/60000]\n",
            "loss: 0.237170  [41024/60000]\n",
            "loss: 0.148371  [41664/60000]\n",
            "loss: 0.353078  [42304/60000]\n",
            "loss: 0.341738  [42944/60000]\n",
            "loss: 0.298334  [43584/60000]\n",
            "loss: 0.264866  [44224/60000]\n",
            "loss: 0.265521  [44864/60000]\n",
            "loss: 0.101616  [45504/60000]\n",
            "loss: 0.238467  [46144/60000]\n",
            "loss: 0.403523  [46784/60000]\n",
            "loss: 0.163493  [47424/60000]\n",
            "loss: 0.292938  [48064/60000]\n",
            "loss: 0.129124  [48704/60000]\n",
            "loss: 0.141477  [49344/60000]\n",
            "loss: 0.402938  [49984/60000]\n",
            "loss: 0.235356  [50624/60000]\n",
            "loss: 0.182501  [51264/60000]\n",
            "loss: 0.298446  [51904/60000]\n",
            "loss: 0.240595  [52544/60000]\n",
            "loss: 0.348810  [53184/60000]\n",
            "loss: 0.284238  [53824/60000]\n",
            "loss: 0.164261  [54464/60000]\n",
            "loss: 0.512163  [55104/60000]\n",
            "loss: 0.308723  [55744/60000]\n",
            "loss: 0.186050  [56384/60000]\n",
            "loss: 0.245318  [57024/60000]\n",
            "loss: 0.350378  [57664/60000]\n",
            "loss: 0.357969  [58304/60000]\n",
            "loss: 0.337478  [58944/60000]\n",
            "loss: 0.318929  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.381265 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.320040  [   64/60000]\n",
            "loss: 0.413786  [  704/60000]\n",
            "loss: 0.268183  [ 1344/60000]\n",
            "loss: 0.293089  [ 1984/60000]\n",
            "loss: 0.378192  [ 2624/60000]\n",
            "loss: 0.270086  [ 3264/60000]\n",
            "loss: 0.497606  [ 3904/60000]\n",
            "loss: 0.330575  [ 4544/60000]\n",
            "loss: 0.378483  [ 5184/60000]\n",
            "loss: 0.346906  [ 5824/60000]\n",
            "loss: 0.278870  [ 6464/60000]\n",
            "loss: 0.201817  [ 7104/60000]\n",
            "loss: 0.259530  [ 7744/60000]\n",
            "loss: 0.235387  [ 8384/60000]\n",
            "loss: 0.345197  [ 9024/60000]\n",
            "loss: 0.285212  [ 9664/60000]\n",
            "loss: 0.320848  [10304/60000]\n",
            "loss: 0.195755  [10944/60000]\n",
            "loss: 0.240507  [11584/60000]\n",
            "loss: 0.273479  [12224/60000]\n",
            "loss: 0.403939  [12864/60000]\n",
            "loss: 0.238407  [13504/60000]\n",
            "loss: 0.301848  [14144/60000]\n",
            "loss: 0.415017  [14784/60000]\n",
            "loss: 0.300395  [15424/60000]\n",
            "loss: 0.225420  [16064/60000]\n",
            "loss: 0.254591  [16704/60000]\n",
            "loss: 0.419028  [17344/60000]\n",
            "loss: 0.282746  [17984/60000]\n",
            "loss: 0.369178  [18624/60000]\n",
            "loss: 0.209028  [19264/60000]\n",
            "loss: 0.260480  [19904/60000]\n",
            "loss: 0.151316  [20544/60000]\n",
            "loss: 0.343565  [21184/60000]\n",
            "loss: 0.101021  [21824/60000]\n",
            "loss: 0.215165  [22464/60000]\n",
            "loss: 0.202336  [23104/60000]\n",
            "loss: 0.141264  [23744/60000]\n",
            "loss: 0.398086  [24384/60000]\n",
            "loss: 0.334266  [25024/60000]\n",
            "loss: 0.270383  [25664/60000]\n",
            "loss: 0.438441  [26304/60000]\n",
            "loss: 0.200246  [26944/60000]\n",
            "loss: 0.202155  [27584/60000]\n",
            "loss: 0.249139  [28224/60000]\n",
            "loss: 0.273225  [28864/60000]\n",
            "loss: 0.170301  [29504/60000]\n",
            "loss: 0.096884  [30144/60000]\n",
            "loss: 0.347815  [30784/60000]\n",
            "loss: 0.259061  [31424/60000]\n",
            "loss: 0.169766  [32064/60000]\n",
            "loss: 0.411425  [32704/60000]\n",
            "loss: 0.215556  [33344/60000]\n",
            "loss: 0.189616  [33984/60000]\n",
            "loss: 0.265580  [34624/60000]\n",
            "loss: 0.224918  [35264/60000]\n",
            "loss: 0.248646  [35904/60000]\n",
            "loss: 0.190501  [36544/60000]\n",
            "loss: 0.451259  [37184/60000]\n",
            "loss: 0.416699  [37824/60000]\n",
            "loss: 0.317090  [38464/60000]\n",
            "loss: 0.221399  [39104/60000]\n",
            "loss: 0.301416  [39744/60000]\n",
            "loss: 0.295431  [40384/60000]\n",
            "loss: 0.246154  [41024/60000]\n",
            "loss: 0.269373  [41664/60000]\n",
            "loss: 0.332091  [42304/60000]\n",
            "loss: 0.270964  [42944/60000]\n",
            "loss: 0.520616  [43584/60000]\n",
            "loss: 0.255502  [44224/60000]\n",
            "loss: 0.143696  [44864/60000]\n",
            "loss: 0.234596  [45504/60000]\n",
            "loss: 0.234776  [46144/60000]\n",
            "loss: 0.193107  [46784/60000]\n",
            "loss: 0.455349  [47424/60000]\n",
            "loss: 0.235425  [48064/60000]\n",
            "loss: 0.390030  [48704/60000]\n",
            "loss: 0.088442  [49344/60000]\n",
            "loss: 0.191109  [49984/60000]\n",
            "loss: 0.145899  [50624/60000]\n",
            "loss: 0.309249  [51264/60000]\n",
            "loss: 0.246283  [51904/60000]\n",
            "loss: 0.151860  [52544/60000]\n",
            "loss: 0.369173  [53184/60000]\n",
            "loss: 0.255617  [53824/60000]\n",
            "loss: 0.355391  [54464/60000]\n",
            "loss: 0.318146  [55104/60000]\n",
            "loss: 0.461325  [55744/60000]\n",
            "loss: 0.261401  [56384/60000]\n",
            "loss: 0.174556  [57024/60000]\n",
            "loss: 0.278648  [57664/60000]\n",
            "loss: 0.281924  [58304/60000]\n",
            "loss: 0.261134  [58944/60000]\n",
            "loss: 0.315282  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.391689 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.201397  [   64/60000]\n",
            "loss: 0.488740  [  704/60000]\n",
            "loss: 0.229732  [ 1344/60000]\n",
            "loss: 0.252757  [ 1984/60000]\n",
            "loss: 0.318848  [ 2624/60000]\n",
            "loss: 0.219569  [ 3264/60000]\n",
            "loss: 0.296593  [ 3904/60000]\n",
            "loss: 0.156219  [ 4544/60000]\n",
            "loss: 0.414089  [ 5184/60000]\n",
            "loss: 0.335488  [ 5824/60000]\n",
            "loss: 0.320823  [ 6464/60000]\n",
            "loss: 0.225611  [ 7104/60000]\n",
            "loss: 0.246420  [ 7744/60000]\n",
            "loss: 0.373189  [ 8384/60000]\n",
            "loss: 0.234797  [ 9024/60000]\n",
            "loss: 0.357614  [ 9664/60000]\n",
            "loss: 0.233596  [10304/60000]\n",
            "loss: 0.216920  [10944/60000]\n",
            "loss: 0.439054  [11584/60000]\n",
            "loss: 0.279326  [12224/60000]\n",
            "loss: 0.199218  [12864/60000]\n",
            "loss: 0.279681  [13504/60000]\n",
            "loss: 0.299497  [14144/60000]\n",
            "loss: 0.290236  [14784/60000]\n",
            "loss: 0.365520  [15424/60000]\n",
            "loss: 0.387322  [16064/60000]\n",
            "loss: 0.273542  [16704/60000]\n",
            "loss: 0.249367  [17344/60000]\n",
            "loss: 0.588446  [17984/60000]\n",
            "loss: 0.272909  [18624/60000]\n",
            "loss: 0.259332  [19264/60000]\n",
            "loss: 0.312713  [19904/60000]\n",
            "loss: 0.320307  [20544/60000]\n",
            "loss: 0.187536  [21184/60000]\n",
            "loss: 0.147284  [21824/60000]\n",
            "loss: 0.201574  [22464/60000]\n",
            "loss: 0.503668  [23104/60000]\n",
            "loss: 0.158842  [23744/60000]\n",
            "loss: 0.260793  [24384/60000]\n",
            "loss: 0.405095  [25024/60000]\n",
            "loss: 0.265220  [25664/60000]\n",
            "loss: 0.528978  [26304/60000]\n",
            "loss: 0.353759  [26944/60000]\n",
            "loss: 0.371663  [27584/60000]\n",
            "loss: 0.234083  [28224/60000]\n",
            "loss: 0.173675  [28864/60000]\n",
            "loss: 0.403081  [29504/60000]\n",
            "loss: 0.204738  [30144/60000]\n",
            "loss: 0.187445  [30784/60000]\n",
            "loss: 0.163055  [31424/60000]\n",
            "loss: 0.209261  [32064/60000]\n",
            "loss: 0.392473  [32704/60000]\n",
            "loss: 0.263617  [33344/60000]\n",
            "loss: 0.291740  [33984/60000]\n",
            "loss: 0.107901  [34624/60000]\n",
            "loss: 0.264210  [35264/60000]\n",
            "loss: 0.406026  [35904/60000]\n",
            "loss: 0.292488  [36544/60000]\n",
            "loss: 0.285560  [37184/60000]\n",
            "loss: 0.376308  [37824/60000]\n",
            "loss: 0.256721  [38464/60000]\n",
            "loss: 0.417889  [39104/60000]\n",
            "loss: 0.199996  [39744/60000]\n",
            "loss: 0.294622  [40384/60000]\n",
            "loss: 0.161232  [41024/60000]\n",
            "loss: 0.242458  [41664/60000]\n",
            "loss: 0.358144  [42304/60000]\n",
            "loss: 0.355235  [42944/60000]\n",
            "loss: 0.233569  [43584/60000]\n",
            "loss: 0.232295  [44224/60000]\n",
            "loss: 0.292792  [44864/60000]\n",
            "loss: 0.259190  [45504/60000]\n",
            "loss: 0.212723  [46144/60000]\n",
            "loss: 0.238903  [46784/60000]\n",
            "loss: 0.180684  [47424/60000]\n",
            "loss: 0.164368  [48064/60000]\n",
            "loss: 0.404165  [48704/60000]\n",
            "loss: 0.129181  [49344/60000]\n",
            "loss: 0.136331  [49984/60000]\n",
            "loss: 0.372074  [50624/60000]\n",
            "loss: 0.296900  [51264/60000]\n",
            "loss: 0.281464  [51904/60000]\n",
            "loss: 0.190903  [52544/60000]\n",
            "loss: 0.346433  [53184/60000]\n",
            "loss: 0.185868  [53824/60000]\n",
            "loss: 0.249153  [54464/60000]\n",
            "loss: 0.255219  [55104/60000]\n",
            "loss: 0.176985  [55744/60000]\n",
            "loss: 0.292150  [56384/60000]\n",
            "loss: 0.466058  [57024/60000]\n",
            "loss: 0.278122  [57664/60000]\n",
            "loss: 0.296456  [58304/60000]\n",
            "loss: 0.261816  [58944/60000]\n",
            "loss: 0.233573  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.449210 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.344564  [   64/60000]\n",
            "loss: 0.443668  [  704/60000]\n",
            "loss: 0.221125  [ 1344/60000]\n",
            "loss: 0.358183  [ 1984/60000]\n",
            "loss: 0.224305  [ 2624/60000]\n",
            "loss: 0.229663  [ 3264/60000]\n",
            "loss: 0.180035  [ 3904/60000]\n",
            "loss: 0.218577  [ 4544/60000]\n",
            "loss: 0.365662  [ 5184/60000]\n",
            "loss: 0.376917  [ 5824/60000]\n",
            "loss: 0.636588  [ 6464/60000]\n",
            "loss: 0.103179  [ 7104/60000]\n",
            "loss: 0.237851  [ 7744/60000]\n",
            "loss: 0.270811  [ 8384/60000]\n",
            "loss: 0.225480  [ 9024/60000]\n",
            "loss: 0.246225  [ 9664/60000]\n",
            "loss: 0.339768  [10304/60000]\n",
            "loss: 0.348944  [10944/60000]\n",
            "loss: 0.243872  [11584/60000]\n",
            "loss: 0.236991  [12224/60000]\n",
            "loss: 0.431656  [12864/60000]\n",
            "loss: 0.226529  [13504/60000]\n",
            "loss: 0.301369  [14144/60000]\n",
            "loss: 0.276737  [14784/60000]\n",
            "loss: 0.175869  [15424/60000]\n",
            "loss: 0.372024  [16064/60000]\n",
            "loss: 0.194008  [16704/60000]\n",
            "loss: 0.216093  [17344/60000]\n",
            "loss: 0.269959  [17984/60000]\n",
            "loss: 0.256014  [18624/60000]\n",
            "loss: 0.149100  [19264/60000]\n",
            "loss: 0.253987  [19904/60000]\n",
            "loss: 0.380991  [20544/60000]\n",
            "loss: 0.326509  [21184/60000]\n",
            "loss: 0.375140  [21824/60000]\n",
            "loss: 0.368374  [22464/60000]\n",
            "loss: 0.357205  [23104/60000]\n",
            "loss: 0.190265  [23744/60000]\n",
            "loss: 0.327126  [24384/60000]\n",
            "loss: 0.153624  [25024/60000]\n",
            "loss: 0.376822  [25664/60000]\n",
            "loss: 0.180929  [26304/60000]\n",
            "loss: 0.299836  [26944/60000]\n",
            "loss: 0.246074  [27584/60000]\n",
            "loss: 0.210479  [28224/60000]\n",
            "loss: 0.265589  [28864/60000]\n",
            "loss: 0.221505  [29504/60000]\n",
            "loss: 0.206261  [30144/60000]\n",
            "loss: 0.249366  [30784/60000]\n",
            "loss: 0.202161  [31424/60000]\n",
            "loss: 0.186932  [32064/60000]\n",
            "loss: 0.340526  [32704/60000]\n",
            "loss: 0.294229  [33344/60000]\n",
            "loss: 0.389501  [33984/60000]\n",
            "loss: 0.397955  [34624/60000]\n",
            "loss: 0.385972  [35264/60000]\n",
            "loss: 0.304360  [35904/60000]\n",
            "loss: 0.290694  [36544/60000]\n",
            "loss: 0.185318  [37184/60000]\n",
            "loss: 0.229298  [37824/60000]\n",
            "loss: 0.175800  [38464/60000]\n",
            "loss: 0.190943  [39104/60000]\n",
            "loss: 0.329180  [39744/60000]\n",
            "loss: 0.238173  [40384/60000]\n",
            "loss: 0.312679  [41024/60000]\n",
            "loss: 0.240559  [41664/60000]\n",
            "loss: 0.486008  [42304/60000]\n",
            "loss: 0.186497  [42944/60000]\n",
            "loss: 0.202713  [43584/60000]\n",
            "loss: 0.270615  [44224/60000]\n",
            "loss: 0.162869  [44864/60000]\n",
            "loss: 0.214786  [45504/60000]\n",
            "loss: 0.265829  [46144/60000]\n",
            "loss: 0.299928  [46784/60000]\n",
            "loss: 0.259618  [47424/60000]\n",
            "loss: 0.274997  [48064/60000]\n",
            "loss: 0.206005  [48704/60000]\n",
            "loss: 0.347141  [49344/60000]\n",
            "loss: 0.302089  [49984/60000]\n",
            "loss: 0.331951  [50624/60000]\n",
            "loss: 0.276309  [51264/60000]\n",
            "loss: 0.324758  [51904/60000]\n",
            "loss: 0.231537  [52544/60000]\n",
            "loss: 0.320960  [53184/60000]\n",
            "loss: 0.428554  [53824/60000]\n",
            "loss: 0.273153  [54464/60000]\n",
            "loss: 0.218284  [55104/60000]\n",
            "loss: 0.365656  [55744/60000]\n",
            "loss: 0.466748  [56384/60000]\n",
            "loss: 0.230428  [57024/60000]\n",
            "loss: 0.307686  [57664/60000]\n",
            "loss: 0.294033  [58304/60000]\n",
            "loss: 0.390733  [58944/60000]\n",
            "loss: 0.229972  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.406651 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide if we are loading for predictions or more training\n",
        "model.eval()\n",
        "# - or -\n",
        "#model.train()\n",
        "\n",
        "# Make predictions\n",
        "pred = model(test_dataset.__getitem__(1)[0]).argmax()\n",
        "truth = test_dataset.__getitem__(1)[1]\n",
        "print(f\"This image is predicted to be a {pred}, and is labeled as {truth}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvaY3pFmkxoP",
        "outputId": "5149b512-d5ba-48e5-9d66-9fa20bbe75ed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is predicted to be a 2, and is labeled as 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the evaluation only giving me the predicted image was not enough, I had now clue how good my training data was on the testdata. So I trained it once, which gave me a poor outcome. Than I added the transformers and this improved my prediction on the test data drastically."
      ],
      "metadata": {
        "id": "1r5rMssInQl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading for predictions\n",
        "model.eval()\n",
        "\n",
        "# Initialize counters for correct predictions and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for data, target in test_dataset:\n",
        "\n",
        "    output = model(data)\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()\n",
        "    total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100. * correct / total\n",
        "\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAwn1Ke_7j3g",
        "outputId": "f5f8bb16-79f9-4e57-bab5-2cd57e0afa40"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-9ff9c6fec93d>:14: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 86.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially I had a score of 7.92%, but for my machine learning research I found someone that said that angles and especially rotations matter. This will let it 'learn' better to make better predictions for new data it has never seen before. So that was what I did, with my transformers.\n"
      ],
      "metadata": {
        "id": "oSy96CZp9rz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the model\n"
      ],
      "metadata": {
        "id": "soA39MDHmD4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For saving the model I made some chances after I did not manage to save and export my file. Ultimately I think that it was not really necessary but I am able to see more about my data than only some basic information. With the export of the additional data I was better able to tell of my import was correct."
      ],
      "metadata": {
        "id": "g1CHUpCBoMC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our model for later, so we can train more or make predictions\n",
        "EPOCH = epochs\n",
        "\n",
        "PATH = \"model_BF.pt\"\n",
        "\n",
        "state = {\n",
        "    'epoch': EPOCH,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'avarage_loss': {loss_fn},\n",
        "    'validation_accuracy': {accuracy}, #added to see at import if it imports everything\n",
        "}\n",
        "torch.save(state, PATH)\n",
        "from google.colab import files\n",
        "files.download('model_BF.pt')\n"
      ],
      "metadata": {
        "id": "mNdU54dudGZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8a0acc48-d18a-491e-f56e-5383d71a3587"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9eadc711-32de-4b5a-8fbd-0b6841af9135\", \"model_BF.pt\", 2315070)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "-GwIBdlSENZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loading the model I made use of the github repository. In my Github the model.pt is placed and in here imported. When using the code of the lecture I had now clue of what was saved. So I changed to see which epoch it uses and what it latest accuracy was. With that I was able to understand what was going on in the file and which information it contained."
      ],
      "metadata": {
        "id": "LRK21AltF7RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "\n",
        "filepath = 'https://github.com/Gilian2002/Assignment-3/raw/main/model_BF-4.pt' # Updated URL to raw file\n",
        "\n",
        "# Download the file from GitHub\n",
        "response = requests.get(filepath)\n",
        "\n",
        "# Save the downloaded file locally\n",
        "local_filepath = 'model_BF.pt'\n",
        "with open(local_filepath, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Now load the model using the local file path\n",
        "state = torch.load(local_filepath, weights_only=False)\n",
        "\n",
        "# Load model, optimizer, and epoch information\n",
        "model.load_state_dict(state['state_dict'])\n",
        "optimizer.load_state_dict(state['optimizer'])\n",
        "start_epoch = state['epoch']\n",
        "last_val_accuracy = state.get('validation_accuracy', None)\n",
        "last_avg_loss = state.get('avarage_loss', None)\n",
        "\n",
        "print(f\"Resuming from epoch {start_epoch}.\")\n",
        "print(f\"Last saved epoch validation accuracy: {last_val_accuracy}\")\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-7x1C-djAa",
        "outputId": "e1ff7298-0571-4f5c-e14f-4d9396d95cc2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 20.\n",
            "Last saved epoch validation accuracy: {86.51}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FirstNet(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_model): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=16, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the imported model\n",
        "With the code below, which is the same as a little higher. A test is performed on how the model accuracy is on the testdata.\n",
        "\n",
        "I have done this, because I had troubles on my testdata and kept losing my model somehow. So instead of only seeing what my image does I wanted to now what the % was on the test data. That was the reason why I left the code in so that you could see how my model performs on the test data."
      ],
      "metadata": {
        "id": "wWsBlrXlAL_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# These are the counters for correct predictions and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for data, target in test_dataset: #load the dataset in an earlier prompt\n",
        "    output = model(data)\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(torch.tensor(target).unsqueeze(0)).sum().item()\n",
        "    total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100. * correct / total\n",
        "\n",
        "print(f\"Accuracy on test data: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjQny1m0EqUM",
        "outputId": "7c9973ff-3c6b-40a8-ec35-fc052323863c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-de877b8240ef>:11: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 86.42%\n"
          ]
        }
      ]
    }
  ]
}